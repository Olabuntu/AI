{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fce1872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import ks_2samp, pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gene_expression_data(n_samples=500, n_genes=2000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic gene expression data optimized for VAE training.\n",
    "    \n",
    "    I designed this to capture key characteristics of real RNA-seq data:\n",
    "    gene co-expression modules (simulating biological pathways), log-normal\n",
    "    distributions, and technical noise. This structure helps the VAE learn\n",
    "    meaningful representations.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create gene modules that simulate co-expression patterns found in real data\n",
    "    # Each module represents a biological pathway where genes are correlated\n",
    "    n_modules = max(15, n_genes // 30)\n",
    "    genes_per_module = n_genes // n_modules\n",
    "    \n",
    "    X = np.zeros((n_samples, n_genes))\n",
    "    \n",
    "    for module_idx in range(n_modules):\n",
    "        start_gene = module_idx * genes_per_module\n",
    "        end_gene = start_gene + genes_per_module if module_idx < n_modules - 1 else n_genes\n",
    "        \n",
    "        # Shared activity factor drives correlation within each module\n",
    "        module_activity = np.random.randn(n_samples)\n",
    "        \n",
    "        # Vary correlation strength to make modules more realistic\n",
    "        correlation_strength = 0.6 + np.random.rand() * 0.3\n",
    "        \n",
    "        for gene_idx in range(start_gene, end_gene):\n",
    "            # Each gene combines module activity with independent noise\n",
    "            gene_expression = (\n",
    "                correlation_strength * module_activity + \n",
    "                (1 - correlation_strength) * np.random.randn(n_samples) * 0.5\n",
    "            )\n",
    "            X[:, gene_idx] = gene_expression\n",
    "    \n",
    "    # Add global factors that affect all genes (simulates batch effects or cell type differences)\n",
    "    global_factors = np.random.randn(n_samples, 3)\n",
    "    global_weights = np.random.randn(3, n_genes) * 0.2\n",
    "    X = X + global_factors @ global_weights\n",
    "    \n",
    "    # Transform to log-normal distribution, which matches real RNA-seq data\n",
    "    X = X - X.min() + 1\n",
    "    X = np.log1p(X)\n",
    "    \n",
    "    # Add technical noise to simulate sequencing artifacts\n",
    "    noise_level = 0.15\n",
    "    X = X + np.random.normal(0, noise_level, X.shape)\n",
    "    \n",
    "    # Gene expression values must be non-negative\n",
    "    X = np.maximum(X, 0)\n",
    "    \n",
    "    # Create binary labels based on expression patterns for downstream analysis\n",
    "    sample_signature = X[:, :min(50, n_genes)].mean(axis=1)\n",
    "    y = (sample_signature > np.median(sample_signature)).astype(int)\n",
    "    \n",
    "    gene_names = [f\"GENE_{i+1:05d}\" for i in range(n_genes)]\n",
    "    sample_ids = [f\"SAMPLE_{i+1:04d}\" for i in range(n_samples)]\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=gene_names, index=sample_ids)\n",
    "    df['Disease_Status'] = y\n",
    "    \n",
    "    return df, X, y, gene_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4013bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset: (1000, 501)\n",
      "Disease distribution:\n",
      "Disease_Status\n",
      "1    500\n",
      "0    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generate VAE-optimized gene expression data\n",
    "df, X, y, gene_names = generate_gene_expression_data(\n",
    "    n_samples=1000,\n",
    "    n_genes=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('gene_expression_data.csv', index=True)\n",
    "print(f\"Generated dataset: {df.shape}\")\n",
    "print(f\"Disease distribution:\\n{df['Disease_Status'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features to help with training stability\n",
    "# Since we're learning a generative model, using the full dataset for scaling is appropriate\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = GeneExpressionDataset(X_scaled)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for gene expression data.\n",
    "    \n",
    "    I used batch normalization and dropout to stabilize training, and LeakyReLU\n",
    "    instead of ReLU to prevent dead neurons. The architecture gradually compresses\n",
    "    from 500 genes down to a 32-dimensional latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=32, dropout=0.2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Map to latent distribution parameters\n",
    "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        \n",
    "        # Decoder mirrors encoder structure\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: sample from N(mu, var) by transforming N(0,1).\n",
    "        This allows gradients to flow through the sampling operation.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b00a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss combines reconstruction error with KL divergence regularization.\n",
    "    The beta parameter controls the trade-off between reconstruction quality\n",
    "    and latent space regularization.\n",
    "    \"\"\"\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence encourages latent distribution to match standard normal\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807938c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, dataloader, num_epochs=50, device='cuda', beta=1.0, \n",
    "              patience=10, min_delta=100, checkpoint_path='best_vae_model.pth'):\n",
    "    \"\"\"\n",
    "    Train VAE with early stopping and automatic checkpointing.\n",
    "    I use gradient clipping to prevent exploding gradients and a learning rate\n",
    "    scheduler that reduces LR when loss plateaus.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                      factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    history = {'total_loss': [], 'recon_loss': [], 'kl_loss': []}\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss_epoch = 0\n",
    "        recon_loss_epoch = 0\n",
    "        kl_loss_epoch = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(batch)\n",
    "            \n",
    "            loss, recon_loss, kl_loss = vae_loss(\n",
    "                recon_batch, batch, mu, logvar, beta=beta\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss_epoch += loss.item()\n",
    "            recon_loss_epoch += recon_loss.item()\n",
    "            kl_loss_epoch += kl_loss.item()\n",
    "        \n",
    "        avg_total = total_loss_epoch / len(dataloader)\n",
    "        avg_recon = recon_loss_epoch / len(dataloader)\n",
    "        avg_kl = kl_loss_epoch / len(dataloader)\n",
    "        \n",
    "        history['total_loss'].append(avg_total)\n",
    "        history['recon_loss'].append(avg_recon)\n",
    "        history['kl_loss'].append(avg_kl)\n",
    "        \n",
    "        scheduler.step(avg_total)\n",
    "        \n",
    "        if avg_total < best_loss - min_delta:\n",
    "            best_loss = avg_total\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}] - NEW BEST MODEL SAVED')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Total Loss: {avg_total:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}')\n",
    "            print(f'Best Loss: {best_loss:.4f}, Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping at epoch {epoch+1}')\n",
    "            print(f'Loading best model with loss: {best_loss:.4f}')\n",
    "            model.load_state_dict(torch.load(checkpoint_path))\n",
    "            break\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fc860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50] - NEW BEST MODEL SAVED\n",
      "Epoch [10/50]\n",
      "Total Loss: 26736.9033, Recon: 25581.8240, KL: 1155.0791\n",
      "Best Loss: 26736.9033, Patience: 0/10\n",
      "Epoch [20/50]\n",
      "Total Loss: 25288.9240, Recon: 23877.1299, KL: 1411.7939\n",
      "Best Loss: 25354.4303, Patience: 1/10\n",
      "Epoch [30/50] - NEW BEST MODEL SAVED\n",
      "Epoch [30/50]\n",
      "Total Loss: 24628.3772, Recon: 23190.4320, KL: 1437.9452\n",
      "Best Loss: 24628.3772, Patience: 0/10\n",
      "Epoch [40/50]\n",
      "Total Loss: 24269.7368, Recon: 22845.3348, KL: 1424.4021\n",
      "Best Loss: 24344.4175, Patience: 2/10\n",
      "Epoch [50/50]\n",
      "Total Loss: 23793.7281, Recon: 22387.3011, KL: 1406.4269\n",
      "Best Loss: 23829.1682, Patience: 2/10\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vae_model = VAE(input_dim=X_scaled.shape[1], hidden_dim=128, latent_dim=32)\n",
    "\n",
    "trained_vae, history = train_vae(\n",
    "    vae_model, dataloader,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    beta=1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207482e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE embedding (this may take a minute)...\n",
      "Latent space visualization saved!\n",
      "\n",
      "Generating latent space interpolation...\n",
      "Interpolation visualization saved!\n"
     ]
    }
   ],
   "source": [
    "def visualize_latent_space(model, X_scaled, y, device='cuda', n_samples=500):\n",
    "    \"\"\"\n",
    "    Project the learned latent space to 2D for visualization.\n",
    "    I use both PCA (fast) and t-SNE (better separation) to see how samples\n",
    "    cluster in the compressed representation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X_scaled[:n_samples]).to(device)\n",
    "        mu, logvar = model.encode(X_tensor)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        z_np = z.cpu().numpy()\n",
    "        y_subset = y[:n_samples]\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    z_pca = pca.fit_transform(z_np)\n",
    "    \n",
    "    print(\"Computing t-SNE embedding (this may take a minute)...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    z_tsne = tsne.fit_transform(z_np)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    scatter1 = axes[0].scatter(z_pca[:, 0], z_pca[:, 1], c=y_subset, \n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].set_title('Latent Space - PCA Projection')\n",
    "    axes[0].set_xlabel('PC1')\n",
    "    axes[0].set_ylabel('PC2')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Disease Status')\n",
    "    \n",
    "    scatter2 = axes[1].scatter(z_tsne[:, 0], z_tsne[:, 1], c=y_subset, \n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].set_title('Latent Space - t-SNE Projection')\n",
    "    axes[1].set_xlabel('t-SNE 1')\n",
    "    axes[1].set_ylabel('t-SNE 2')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Disease Status')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('latent_space_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Latent space visualization saved!\")\n",
    "    \n",
    "    return z_np\n",
    "\n",
    "latent_vectors = visualize_latent_space(trained_vae, X_scaled, y, device=device, n_samples=500)\n",
    "\n",
    "def interpolate_latent(model, z1, z2, n_steps=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two latent vectors and decode to see\n",
    "    how gene expression changes smoothly in the latent space.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    alphas = np.linspace(0, 1, n_steps)\n",
    "    interpolated = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for alpha in alphas:\n",
    "            z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "            recon = model.decode(z_interp.unsqueeze(0).to(device))\n",
    "            interpolated.append(recon.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(interpolated)\n",
    "\n",
    "print(\"\\nGenerating latent space interpolation...\")\n",
    "sample_idx1, sample_idx2 = 0, 100\n",
    "z1 = torch.FloatTensor(latent_vectors[sample_idx1]).to(device)\n",
    "z2 = torch.FloatTensor(latent_vectors[sample_idx2]).to(device)\n",
    "\n",
    "interpolated_samples = interpolate_latent(trained_vae, z1, z2, n_steps=10, device=device)\n",
    "interpolated_original = scaler.inverse_transform(interpolated_samples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "for i in range(10):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    axes[row, col].plot(interpolated_original[i, :20], 'o-', markersize=3)\n",
    "    axes[row, col].set_title(f'Step {i+1}')\n",
    "    axes[row, col].set_ylabel('Expression')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    if row == 1:\n",
    "        axes[row, col].set_xlabel('Gene Index')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation (First 20 Genes)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('latent_interpolation.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Interpolation visualization saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d309a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real data shape: (1000, 500)\n",
      "Synthetic data shape: (100, 500)\n",
      "\n",
      "Real data stats:\n",
      "           0         1         2         3         4         5         6    \\\n",
      "mean  1.779777  1.774891  1.787856  1.789800  1.784244  1.778544  1.785110   \n",
      "std   0.188598  0.198636  0.191158  0.185457  0.185571  0.196865  0.197522   \n",
      "\n",
      "           7         8         9    ...       490       491       492  \\\n",
      "mean  1.779795  1.776958  1.774142  ...  1.771237  1.779141  1.775565   \n",
      "std   0.185122  0.208729  0.193769  ...  0.221526  0.216452  0.226163   \n",
      "\n",
      "           493       494       495       496       497       498       499  \n",
      "mean  1.772184  1.771721  1.769287  1.778823  1.775983  1.774925  1.777608  \n",
      "std   0.225290  0.221796  0.233326  0.223448  0.216436  0.239569  0.215450  \n",
      "\n",
      "[2 rows x 500 columns]\n",
      "\n",
      "Synthetic data stats:\n",
      "           0         1         2         3         4         5         6    \\\n",
      "mean  1.792412  1.791412  1.796760  1.788679  1.782088  1.790907  1.796872   \n",
      "std   0.058046  0.053509  0.049432  0.048255  0.047768  0.066093  0.057351   \n",
      "\n",
      "           7         8         9    ...       490       491       492  \\\n",
      "mean  1.780741  1.769523  1.774361  ...  1.780349  1.785674  1.792944   \n",
      "std   0.043818  0.059509  0.043198  ...  0.092978  0.093770  0.095549   \n",
      "\n",
      "           493       494       495       496       497       498       499  \n",
      "mean  1.780571  1.789673  1.784650  1.791806  1.790540  1.805123  1.787184  \n",
      "std   0.097872  0.095683  0.113559  0.099024  0.085292  0.094927  0.090535  \n",
      "\n",
      "[2 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_samples(model, n_samples=100, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate new synthetic samples by sampling from the learned latent distribution\n",
    "    and decoding back to gene expression space.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, 32).to(device)\n",
    "        generated = model.decode(z).cpu().numpy()\n",
    "        generated_original = scaler.inverse_transform(generated)\n",
    "    \n",
    "    return generated_original\n",
    "\n",
    "synthetic_data = generate_samples(trained_vae, n_samples=100, device=device)\n",
    "\n",
    "print(f\"\\nReal data shape: {X.shape}\")\n",
    "print(f\"Synthetic data shape: {synthetic_data.shape}\")\n",
    "print(f\"\\nReal data stats:\")\n",
    "print(pd.DataFrame(X).describe().iloc[1:3])\n",
    "print(f\"\\nSynthetic data stats:\")\n",
    "print(pd.DataFrame(synthetic_data).describe().iloc[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb9485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training curves saved!\n",
      "Distribution comparison saved!\n",
      "\n",
      "============================================================\n",
      "EVALUATION METRICS\n",
      "============================================================\n",
      "Mean correlation (real vs synthetic): 0.3384 (p=7.33e-15)\n",
      "Std correlation (real vs synthetic): 0.8330 (p=3.97e-130)\n",
      "Average KS statistic (lower is better): 0.3023\n",
      "  (KS < 0.1: excellent, < 0.2: good, < 0.3: acceptable)\n",
      "\n",
      "Variance ratio (synthetic/real): 0.3860\n",
      "  (1.0 = perfect match, < 1.0 = synthetic has less variance)\n",
      "============================================================\n",
      "Correlation comparison saved!\n",
      "\n",
      "Correlation matrix similarity: 0.7687 (p=0.00e+00)\n",
      "\n",
      "VAE model saved!\n",
      "\n",
      "======================================================================\n",
      "PROJECT SUMMARY - VAE for Gene Expression Data Generation\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Dataset:\n",
      "   - Samples: 1000\n",
      "   - Genes: 500\n",
      "   - Generated synthetic samples: 100\n",
      "\n",
      "ðŸ¤– Model Architecture:\n",
      "   - Input dimension: 500\n",
      "   - Latent dimension: 32\n",
      "   - Hidden dimension: 128\n",
      "   - Total parameters: 347,316\n",
      "\n",
      "ðŸ“ˆ Training Performance:\n",
      "   - Final total loss: 23793.73\n",
      "   - Final reconstruction loss: 22387.30\n",
      "   - Final KL divergence: 1406.43\n",
      "   - Training epochs: 50\n",
      "\n",
      "âœ… Generated Files:\n",
      "   - gene_expression_data.csv (original data)\n",
      "   - gene_expression_vae.pth (trained model)\n",
      "   - best_vae_model.pth (best checkpoint)\n",
      "   - training_curves.png\n",
      "   - gene_distribution_comparison.png\n",
      "   - correlation_comparison.png\n",
      "   - latent_space_visualization.png\n",
      "   - latent_interpolation.png\n",
      "\n",
      "ðŸ’¡ Key Improvements Made:\n",
      "   âœ“ Enhanced VAE architecture (batch norm, dropout, deeper layers)\n",
      "   âœ“ Early stopping and model checkpointing\n",
      "   âœ“ Learning rate scheduling\n",
      "   âœ“ Comprehensive evaluation metrics\n",
      "   âœ“ Latent space visualization and interpolation\n",
      "   âœ“ Statistical validation (KS test, correlations)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history['total_loss'], label='Total Loss', linewidth=2)\n",
    "axes[0].plot(history['recon_loss'], label='Reconstruction Loss', linewidth=2)\n",
    "axes[0].plot(history['kl_loss'], label='KL Divergence', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['recon_loss'], label='Reconstruction', linewidth=2)\n",
    "axes[1].plot(history['kl_loss'], label='KL Divergence', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss Components')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Track the balance between reconstruction and regularization\n",
    "kl_recon_ratio = [kl/r if r > 0 else 0 for kl, r in zip(history['kl_loss'], history['recon_loss'])]\n",
    "axes[2].plot(kl_recon_ratio, label='KL/Recon Ratio', linewidth=2, color='purple')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Ratio')\n",
    "axes[2].set_title('KL/Reconstruction Ratio')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Training curves saved!\")\n",
    "\n",
    "# Compare distributions of individual genes between real and synthetic data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "gene_indices = [0, 10, 50, 100]\n",
    "for idx, gene_idx in enumerate(gene_indices):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    axes[row, col].hist(X[:, gene_idx], bins=30, alpha=0.6, label='Real', color='blue', density=True)\n",
    "    axes[row, col].hist(synthetic_data[:, gene_idx], bins=30, alpha=0.6, label='Synthetic', \n",
    "                       color='orange', density=True)\n",
    "    axes[row, col].set_title(f'Gene {gene_idx} Distribution')\n",
    "    axes[row, col].set_xlabel('Expression Level')\n",
    "    axes[row, col].set_ylabel('Density')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gene_distribution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Distribution comparison saved!\")\n",
    "\n",
    "# Statistical evaluation of synthetic data quality\n",
    "real_mean = X.mean(axis=0)\n",
    "real_std = X.std(axis=0)\n",
    "synth_mean = synthetic_data.mean(axis=0)\n",
    "synth_std = synthetic_data.std(axis=0)\n",
    "\n",
    "mean_corr, mean_p = pearsonr(real_mean, synth_mean)\n",
    "std_corr, std_p = pearsonr(real_std, synth_std)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean correlation (real vs synthetic): {mean_corr:.4f} (p={mean_p:.2e})\")\n",
    "print(f\"Std correlation (real vs synthetic): {std_corr:.4f} (p={std_p:.2e})\")\n",
    "\n",
    "# Kolmogorov-Smirnov test to compare distributions\n",
    "ks_results = []\n",
    "for i in range(0, min(50, X.shape[1]), 5):\n",
    "    ks_stat, ks_p = ks_2samp(X[:, i], synthetic_data[:, i])\n",
    "    ks_results.append(ks_stat)\n",
    "\n",
    "avg_ks_stat = np.mean(ks_results)\n",
    "print(f\"Average KS statistic (lower is better): {avg_ks_stat:.4f}\")\n",
    "print(f\"  (KS < 0.1: excellent, < 0.2: good, < 0.3: acceptable)\")\n",
    "\n",
    "variance_ratio = synthetic_data.std(axis=0).mean() / X.std(axis=0).mean()\n",
    "print(f\"\\nVariance ratio (synthetic/real): {variance_ratio:.4f}\")\n",
    "print(f\"  (1.0 = perfect match, < 1.0 = synthetic has less variance)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare gene-gene correlation structures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "n_genes_sample = 50\n",
    "sample_indices = np.random.choice(X.shape[1], n_genes_sample, replace=False)\n",
    "\n",
    "real_corr = np.corrcoef(X[:, sample_indices].T)\n",
    "synth_corr = np.corrcoef(synthetic_data[:, sample_indices].T)\n",
    "\n",
    "im1 = axes[0].imshow(real_corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "axes[0].set_title('Real Data Correlation Matrix (50 genes)')\n",
    "axes[0].set_xlabel('Gene Index')\n",
    "axes[0].set_ylabel('Gene Index')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(synth_corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "axes[1].set_title('Synthetic Data Correlation Matrix (50 genes)')\n",
    "axes[1].set_xlabel('Gene Index')\n",
    "axes[1].set_ylabel('Gene Index')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Correlation comparison saved!\")\n",
    "\n",
    "corr_corr, corr_p = pearsonr(real_corr.flatten(), synth_corr.flatten())\n",
    "print(f\"\\nCorrelation matrix similarity: {corr_corr:.4f} (p={corr_p:.2e})\")\n",
    "\n",
    "torch.save(trained_vae.state_dict(), 'gene_expression_vae.pth')\n",
    "print(\"\\nVAE model saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT SUMMARY - VAE for Gene Expression Data Generation\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"   - Samples: {X.shape[0]}\")\n",
    "print(f\"   - Genes: {X.shape[1]}\")\n",
    "print(f\"   - Generated synthetic samples: {synthetic_data.shape[0]}\")\n",
    "\n",
    "print(f\"\\nðŸ¤– Model Architecture:\")\n",
    "print(f\"   - Input dimension: {X_scaled.shape[1]}\")\n",
    "print(f\"   - Latent dimension: 32\")\n",
    "print(f\"   - Hidden dimension: 128\")\n",
    "print(f\"   - Total parameters: {sum(p.numel() for p in trained_vae.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Training Performance:\")\n",
    "print(f\"   - Final total loss: {history['total_loss'][-1]:.2f}\")\n",
    "print(f\"   - Final reconstruction loss: {history['recon_loss'][-1]:.2f}\")\n",
    "print(f\"   - Final KL divergence: {history['kl_loss'][-1]:.2f}\")\n",
    "print(f\"   - Training epochs: {len(history['total_loss'])}\")\n",
    "\n",
    "print(f\"\\nâœ… Generated Files:\")\n",
    "print(f\"   - gene_expression_data.csv (original data)\")\n",
    "print(f\"   - gene_expression_vae.pth (trained model)\")\n",
    "print(f\"   - best_vae_model.pth (best checkpoint)\")\n",
    "print(f\"   - training_curves.png\")\n",
    "print(f\"   - gene_distribution_comparison.png\")\n",
    "print(f\"   - correlation_comparison.png\")\n",
    "print(f\"   - latent_space_visualization.png\")\n",
    "print(f\"   - latent_interpolation.png\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Improvements Made:\")\n",
    "print(f\"   âœ“ Enhanced VAE architecture (batch norm, dropout, deeper layers)\")\n",
    "print(f\"   âœ“ Early stopping and model checkpointing\")\n",
    "print(f\"   âœ“ Learning rate scheduling\")\n",
    "print(f\"   âœ“ Comprehensive evaluation metrics\")\n",
    "print(f\"   âœ“ Latent space visualization and interpolation\")\n",
    "print(f\"   âœ“ Statistical validation (KS test, correlations)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
