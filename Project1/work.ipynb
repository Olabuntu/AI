{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_data shape: (286, 22285), target_data shape: (286, 2)\n",
      "df shape: (286, 22285)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-M_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "      <th>bone_relapse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM36777</td>\n",
       "      <td>3848.1</td>\n",
       "      <td>228.9</td>\n",
       "      <td>213.1</td>\n",
       "      <td>1009.4</td>\n",
       "      <td>31.8</td>\n",
       "      <td>551.5</td>\n",
       "      <td>176.7</td>\n",
       "      <td>11.9</td>\n",
       "      <td>309.3</td>\n",
       "      <td>...</td>\n",
       "      <td>661.5</td>\n",
       "      <td>33168.9</td>\n",
       "      <td>25644.4</td>\n",
       "      <td>11.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM36778</td>\n",
       "      <td>6520.9</td>\n",
       "      <td>112.5</td>\n",
       "      <td>189.8</td>\n",
       "      <td>2083.3</td>\n",
       "      <td>145.8</td>\n",
       "      <td>802.8</td>\n",
       "      <td>278.4</td>\n",
       "      <td>28.3</td>\n",
       "      <td>449.0</td>\n",
       "      <td>...</td>\n",
       "      <td>716.1</td>\n",
       "      <td>54401.4</td>\n",
       "      <td>40720.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>21.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM36779</td>\n",
       "      <td>5285.7</td>\n",
       "      <td>178.4</td>\n",
       "      <td>269.7</td>\n",
       "      <td>1203.4</td>\n",
       "      <td>42.5</td>\n",
       "      <td>557.5</td>\n",
       "      <td>183.3</td>\n",
       "      <td>56.4</td>\n",
       "      <td>101.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1189.3</td>\n",
       "      <td>61244.1</td>\n",
       "      <td>50878.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM36780</td>\n",
       "      <td>4043.7</td>\n",
       "      <td>398.7</td>\n",
       "      <td>312.4</td>\n",
       "      <td>1104.4</td>\n",
       "      <td>108.2</td>\n",
       "      <td>568.5</td>\n",
       "      <td>187.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>899.1</td>\n",
       "      <td>...</td>\n",
       "      <td>801.1</td>\n",
       "      <td>62292.1</td>\n",
       "      <td>46870.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>10.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM36781</td>\n",
       "      <td>4263.6</td>\n",
       "      <td>417.7</td>\n",
       "      <td>327.1</td>\n",
       "      <td>1043.3</td>\n",
       "      <td>69.2</td>\n",
       "      <td>653.2</td>\n",
       "      <td>185.8</td>\n",
       "      <td>21.8</td>\n",
       "      <td>3629.3</td>\n",
       "      <td>...</td>\n",
       "      <td>191.1</td>\n",
       "      <td>57295.1</td>\n",
       "      <td>40847.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>30.5</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id  1007_s_at  1053_at  117_at  121_at  1255_g_at  1294_at  1316_at  \\\n",
       "0  GSM36777     3848.1    228.9   213.1  1009.4       31.8    551.5    176.7   \n",
       "1  GSM36778     6520.9    112.5   189.8  2083.3      145.8    802.8    278.4   \n",
       "2  GSM36779     5285.7    178.4   269.7  1203.4       42.5    557.5    183.3   \n",
       "3  GSM36780     4043.7    398.7   312.4  1104.4      108.2    568.5    187.7   \n",
       "4  GSM36781     4263.6    417.7   327.1  1043.3       69.2    653.2    185.8   \n",
       "\n",
       "   1320_at  1405_i_at  ...  AFFX-r2-Hs28SrRNA-M_at  AFFX-r2-P1-cre-3_at  \\\n",
       "0     11.9      309.3  ...                   661.5              33168.9   \n",
       "1     28.3      449.0  ...                   716.1              54401.4   \n",
       "2     56.4      101.9  ...                  1189.3              61244.1   \n",
       "3     42.1      899.1  ...                   801.1              62292.1   \n",
       "4     21.8     3629.3  ...                   191.1              57295.1   \n",
       "\n",
       "   AFFX-r2-P1-cre-5_at  AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  \\\n",
       "0              25644.4            11.1             9.2            25.0   \n",
       "1              40720.0            17.1            62.0            21.4   \n",
       "2              50878.7             6.4             8.4             7.4   \n",
       "3              46870.8            13.5            10.8            16.0   \n",
       "4              40847.1            19.8            30.5            22.1   \n",
       "\n",
       "   AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  bone_relapse  \n",
       "0              4.2              7.0             10.0             0  \n",
       "1              9.3             11.2             15.7             0  \n",
       "2              7.1             64.0              4.1             0  \n",
       "3              4.6             12.2              7.3             0  \n",
       "4              7.3              5.5             11.6             0  \n",
       "\n",
       "[5 rows x 22285 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('Data Files', exist_ok=True)\n",
    "\n",
    "# Load gene expression data and labels\n",
    "feature_data = pd.read_csv('Data Files/gene_expression.csv')\n",
    "target_data = pd.read_csv('Data Files/labels.csv')\n",
    "\n",
    "feature_data.rename(columns={'Unnamed: 0': 'sample_id'}, inplace=True)\n",
    "print(f'feature_data shape: {feature_data.shape}, target_data shape: {target_data.shape}')\n",
    "\n",
    "# Merge features and labels by sample ID\n",
    "df = pd.merge(feature_data, target_data, on='sample_id', how='inner')\n",
    "\n",
    "# Remove metadata column that was accidentally included\n",
    "df.drop(columns=['!series_matrix_table_end'], inplace=True)\n",
    "\n",
    "print(f'df shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA OVERVIEW\n",
      "============================================================\n",
      "Shape: (286, 22285)\n",
      "Features (genes): 22283\n",
      "Samples: 286\n",
      "\n",
      "Bone_relapse distribution:\n",
      "bone_relapse\n",
      "0    217\n",
      "1     69\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "Data types:\n",
      "float64    22283\n",
      "object         1\n",
      "int64          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract gene names (exclude sample_id and target column)\n",
    "gene_names = df.columns[1:-1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Features (genes): {len(gene_names)}\")\n",
    "print(f\"Samples: {len(df)}\")\n",
    "print(f\"\\nBone_relapse distribution:\")\n",
    "print(df['bone_relapse'].value_counts())\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHqCAYAAACZcdjsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9C0lEQVR4nO3deVyU5f7/8feAgCgOiAposrkvuR01JSslUdCkLJf0WGG5VeKSpzp6ytQ2j6VHyzXrBHXSY2m7pYa4pWG5ZGqaR8ulFNBcQMhA4fr90df5OeEGNzIgr+fjcT/yvu5rrvtzz8yD5j33fd1jM8YYAQAAAIAFbq4uAAAAAEDZR7AAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAIByzGazaeLEia4uo8QNHDhQYWFhJbKvsLAwDRw40LGemJgom82mzZs3l8j+O3XqpE6dOpXIvgCUbwQLAOXK+Q91Fy4BAQGKjIzUsmXLXF0eimDixIlOr2elSpUUEhKi2NhYJSQkKCcnp1j2s2vXLk2cOFEHDhwolvGKU2muDUD5UcHVBQCAKzz77LMKDw+XMUbp6elKTExU9+7d9emnn6pHjx6uLg9FMHfuXPn4+CgnJ0eHDx/WihUr9NBDD2nGjBlaunSpgoODHX1ff/115efnF2r8Xbt2adKkSerUqVOhznbs2bNHbm7X9nu8y9X2xRdfXNN9A8B5BAsA5VK3bt3Upk0bx/qgQYMUGBio//73v2U2WBhj9Pvvv8vb29vVpbhE7969Vb16dcf6M888owULFuiBBx5Qnz59tHHjRsc2Dw+Pa1rLha+Fl5fXNd3XlXh6erp0/wDKDy6FAgBJfn5+8vb2VoUKzt+3ZGdn629/+5uCg4Pl5eWlhg0baurUqTLGOPWz2WyKj4/XRx99pBtvvFFeXl5q2rSpli9fXmBfhw8f1kMPPaTAwEBHvzfffLPQNYeFhalHjx5asWKF2rRpI29vb7322muSpFOnTmn06NGOuuvVq6cpU6Zc8Vv6gwcP6tFHH1XDhg3l7e2tatWqqU+fPgUusTl/Sdm6des0bNgwVatWTXa7XQ888IBOnjzp1Hfz5s2Kjo5W9erV5e3trfDwcD300ENOffLz8zVjxgw1bdpUFStWVGBgoIYNG1ZgrMIaMGCABg8erK+//lpJSUmO9ovNsVi0aJFat26tKlWqyG63q1mzZnrllVccx9unTx9JUmRkpOOyqzVr1ki6/Gvx5zkW5/32229XfO4uNQfmwjGvVNvF5lgcPXrUEaYrVqyoFi1a6K233nLqc+DAAdlsNk2dOlXz589X3bp15eXlpbZt22rTpk0Xfb4BlG+csQBQLmVkZOjXX3+VMUZHjx7VzJkzlZWVpfvuu8/RxxijO++8U6tXr9agQYPUsmVLrVixQk888YQOHz6s6dOnO425fv16ffDBB3r00UdVpUoVvfrqq+rVq5cOHTqkatWqSZLS09PVvn17RxCpUaOGli1bpkGDBikzM1OjR48u1HHs2bNH/fv317BhwzRkyBA1bNhQv/32mzp27KjDhw9r2LBhCgkJ0VdffaVx48YpNTVVM2bMuOR4mzZt0ldffaV+/fqpdu3aOnDggObOnatOnTpp165dqlSpklP/+Ph4+fn5aeLEidqzZ4/mzp2rgwcPas2aNbLZbDp69Ki6du2qGjVqaOzYsfLz89OBAwf0wQcfOI0zbNgwJSYm6sEHH9TIkSO1f/9+zZo1S99++602bNhg6QzD/fffr/nz5+uLL75Qly5dLtonKSlJ/fv3V+fOnTVlyhRJ0u7du7VhwwaNGjVKt912m0aOHKlXX31V//jHP9S4cWNJcvxXuvhrcTlXeu6u1tXUdqEzZ86oU6dO2rdvn+Lj4xUeHq7Fixdr4MCBOnXqlEaNGuXUf+HChTp9+rSGDRsmm82ml156Sffcc49++umna37mB0AZYwCgHElISDCSCixeXl4mMTHRqe9HH31kJJnnn3/eqb13797GZrOZffv2OdokGU9PT6e27777zkgyM2fOdLQNGjTI1KxZ0/z6669OY/br18/4+vqa33777aqPJTQ01Egyy5cvd2p/7rnnTOXKlc3//vc/p/axY8cad3d3c+jQIae6J0yY4Fi/2P5TUlKMJPP222872s4/j61btza5ubmO9pdeeslIMh9//LExxpgPP/zQSDKbNm265HF8+eWXRpJZsGCBU/vy5csv2v5nEyZMMJLMsWPHLrr95MmTRpK5++67HW1xcXEmNDTUsT5q1Chjt9vNuXPnLrmfxYsXG0lm9erVBbZd6rU4vy0uLs6xfrXPnTEFX59LjXm52jp27Gg6duzoWJ8xY4aRZN555x1HW25uromIiDA+Pj4mMzPTGGPM/v37jSRTrVo1c+LECUffjz/+2Egyn376aYF9ASjfuBQKQLk0e/ZsJSUlKSkpSe+8844iIyM1ePBgp2/SP//8c7m7u2vkyJFOj/3b3/4mY0yBu0hFRUWpbt26jvXmzZvLbrfrp59+kvTHGZD3339fsbGxMsbo119/dSzR0dHKyMjQ1q1bC3Uc4eHhio6OdmpbvHixbr31VlWtWtVpH1FRUcrLy9O6desuOd6F8zPOnj2r48ePq169evLz87tobUOHDnX61vqRRx5RhQoV9Pnnn0v64xIzSVq6dKnOnj170X0uXrxYvr6+6tKli1O9rVu3lo+Pj1avXn3Vz8fF+Pj4SJJOnz59yT5+fn7Kzs52ulyqsC72WlzOlZ67a+Xzzz9XUFCQ+vfv72jz8PDQyJEjlZWVpbVr1zr1v/fee1W1alXH+q233ipJjvc1AJzHpVAAyqWbbrrJafJ2//791apVK8XHx6tHjx7y9PTUwYMHVatWLVWpUsXpsecvMTl48KBTe0hISIH9VK1a1XHd/LFjx3Tq1CnNnz9f8+fPv2hdR48eLdRxhIeHF2jbu3evtm/frho1ahR6H2fOnNHkyZOVkJCgw4cPO80lycjIKNC/fv36Tus+Pj6qWbOmY05Gx44d1atXL02aNEnTp09Xp06d1LNnT/31r391TGreu3evMjIyFBAQUOh6r0ZWVpYkFXgdL/Too4/qvffeU7du3XTDDTeoa9eu6tu3r2JiYq56Pxd7LS7nSs/dtXLw4EHVr1+/wJ2qrvZ9fT5kWJ3/AuD6Q7AAAElubm6KjIzUK6+8or1796pp06aFHsPd3f2i7ec/nJ+fOH3fffcpLi7uon2bN29eqH1e7A5Q+fn56tKli5588smLPqZBgwaXHG/EiBFKSEjQ6NGjFRERIV9fX9lsNvXr16/Qt2eV/ph8vGTJEm3cuFGffvqp4xaw06ZN08aNG+Xj46P8/HwFBARowYIFFx3jUgHpau3cuVOSVK9evUv2CQgI0LZt27RixQotW7ZMy5YtU0JCgh544IECk5ovpSTvxpWXl1di+7rS+xoAziNYAMD/OXfunKT//w13aGioVq5cqdOnTzt92/3DDz84thdGjRo1VKVKFeXl5SkqKqqYqi6obt26ysrKKtI+lixZori4OE2bNs3R9vvvv+vUqVMX7b93715FRkY61rOyspSamqru3bs79Wvfvr3at2+vF154QQsXLtSAAQO0aNEiDR48WHXr1tXKlSvVoUOHa/Lh/D//+Y8kXfEyJU9PT8XGxio2Nlb5+fl69NFH9dprr2n8+PGqV69eoSZUX42ree6qVq1a4LnPzc1VamqqU1thagsNDdX27duVn5/vdNaiqO9rADiPORYAoD/mE3zxxRfy9PR0XBLSvXt35eXladasWU59p0+fLpvNpm7duhVqH+7u7urVq5fef/99x7foFzp27FjRD+ACffv2VUpKilasWFFg26lTpxwB6lI1/vmb6JkzZ17yG/L58+c7zZ2YO3euzp0753huTp48WWC8li1bSpLjF7H79u2rvLw8PffccwXGP3fu3CVDzdVYuHCh3njjDUVERKhz586X7Hf8+HGndTc3N8fZo/N1Vq5cWZIs1XOhKz130h8h8c9zYubPn1/g9ShMbd27d1daWpreffddR9u5c+c0c+ZM+fj4qGPHjkU5HADgjAWA8mnZsmWOb2iPHj2qhQsXau/evRo7dqzsdrskKTY2VpGRkXrqqad04MABtWjRQl988YU+/vhjjR492mmi9tX65z//qdWrV6tdu3YaMmSImjRpohMnTmjr1q1auXKlTpw4YfnYnnjiCX3yySfq0aOHBg4cqNatWys7O1s7duzQkiVLdODAAacfkrtQjx499J///Ee+vr5q0qSJUlJStHLlSsftcv8sNzdXnTt3Vt++fbVnzx7NmTNHt9xyi+68805J0ltvvaU5c+bo7rvvVt26dXX69Gm9/vrrstvtjm/mO3bsqGHDhmny5Mnatm2bunbtKg8PD+3du1eLFy/WK6+8ot69e1/xuJcsWSIfHx/l5uY6fnl7w4YNatGihRYvXnzZxw4ePFgnTpzQ7bffrtq1a+vgwYOaOXOmWrZs6QiaLVu2lLu7u6ZMmaKMjAx5eXnp9ttvv+TckCu50nN3vq6HH35YvXr1UpcuXfTdd99pxYoVBV6/wtQ2dOhQvfbaaxo4cKC2bNmisLAwLVmyRBs2bNCMGTMuOxcFAC7LdTekAoCSd7HbzVasWNG0bNnSzJ071+Tn5zv1P336tHnsscdMrVq1jIeHh6lfv755+eWXC/STZIYPH15gf3++LagxxqSnp5vhw4eb4OBg4+HhYYKCgkznzp3N/PnzC3UsoaGh5o477rjottOnT5tx48aZevXqGU9PT1O9enVz8803m6lTpzrd4lR/up3pyZMnzYMPPmiqV69ufHx8THR0tPnhhx8uecvUtWvXmqFDh5qqVasaHx8fM2DAAHP8+HFHv61bt5r+/fubkJAQ4+XlZQICAkyPHj3M5s2bC9Q8f/5807p1a+Pt7W2qVKlimjVrZp588klz5MiRyz4P5283e+HrWbt2bdOjRw/z5ptvmt9//73AY/58u9klS5aYrl27moCAAOPp6WlCQkLMsGHDTGpqqtPjXn/9dVOnTh3j7u7udHvXy70WRX3ujDEmLy/P/P3vfzfVq1c3lSpVMtHR0Wbfvn0XfV9dqrY/327WmD/eg+dfZ09PT9OsWTOTkJDg1Of87WZffvnlAsf05/cNABhjjM0YZl8BAArn/I/Zbdq0yenuWgCA8os5FgAAAAAsY44FAJQyx44du+ztRD09PeXv71+CFQEAcGUECwAoZdq2bVvgR8ou1LFjR61Zs6bkCgIA4CowxwIASpkNGzbozJkzl9xetWpVtW7dugQrAgDgyggWAAAAACxj8jYAAAAAy5hjISk/P19HjhxRlSpVZLPZXF0OAAAAUCoYY3T69GnVqlVLbm6XPydBsJB05MgRBQcHu7oMAAAAoFT6+eefVbt27cv2IVhIqlKliqQ/njC73e7iagAAAIDSITMzU8HBwY7Py5dDsJAclz/Z7XaCBQAAAPAnVzNdgMnbAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBZAKTZ58mS1bdtWVapUUUBAgHr27Kk9e/Y49Zk/f746deoku90um82mU6dOOW1fs2aNbDbbRZdNmzaV4NEAAIDrGcECKMXWrl2r4cOHa+PGjUpKStLZs2fVtWtXZWdnO/r89ttviomJ0T/+8Y+LjnHzzTcrNTXVaRk8eLDCw8PVpk2bkjoUAABwneN2s0Aptnz5cqf1xMREBQQEaMuWLbrtttskSaNHj5b0x5mJi/H09FRQUJBj/ezZs/r44481YsQIfmkeAAAUG85YAGVIRkaGJMnf37/IY3zyySc6fvy4HnzwweIqCwAAgGABlBX5+fkaPXq0OnTooBtvvLHI4/z73/9WdHS0ateuXYzVAQCA8o5LoYAyYvjw4dq5c6fWr19f5DF++eUXrVixQu+9914xVgYAAECwAMqE+Ph4LV26VOvWrbN0piEhIUHVqlXTnXfeWYzVAQAAECyAUs0YoxEjRujDDz/UmjVrFB4ebmmshIQEPfDAA/Lw8CjGKgEAAAgWQKk2fPhwLVy4UB9//LGqVKmitLQ0SZKvr6+8vb0lSWlpaUpLS9O+ffskSTt27FCVKlUUEhLiNMl71apV2r9/vwYPHlzyBwIAAK57NmOMcXURrpaZmSlfX19lZGTIbre7uhzA4VK3g01ISNDAgQMlSRMnTtSkSZMu20eS/vrXv+rgwYPasGHDtSgVAABchwrzOZlgIYIFAAAAcDGF+ZzM7WYBAAAAWEawAAAAAGAZk7dRYsLGfubqEoBiceCfd7i6BAAASh3OWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACxzabCYPHmy2rZtqypVqiggIEA9e/bUnj17nPr8/vvvGj58uKpVqyYfHx/16tVL6enpTn0OHTqkO+64Q5UqVVJAQICeeOIJnTt3riQPBQAAACjXXBos1q5dq+HDh2vjxo1KSkrS2bNn1bVrV2VnZzv6PPbYY/r000+1ePFirV27VkeOHNE999zj2J6Xl6c77rhDubm5+uqrr/TWW28pMTFRzzzzjCsOCQAAACiXbMYY4+oizjt27JgCAgK0du1a3XbbbcrIyFCNGjW0cOFC9e7dW5L0ww8/qHHjxkpJSVH79u21bNky9ejRQ0eOHFFgYKAkad68efr73/+uY8eOydPT84r7zczMlK+vrzIyMmS326/pMZZnYWM/c3UJQLE48M87XF0CAAAlojCfk0vVHIuMjAxJkr+/vyRpy5YtOnv2rKKiohx9GjVqpJCQEKWkpEiSUlJS1KxZM0eokKTo6GhlZmbq+++/L8HqAQAAgPKrgqsLOC8/P1+jR49Whw4ddOONN0qS0tLS5OnpKT8/P6e+gYGBSktLc/S5MFSc335+28Xk5OQoJyfHsZ6ZmVlchwEAAACUS6XmjMXw4cO1c+dOLVq06Jrva/LkyfL19XUswcHB13yfAAAAwPWsVASL+Ph4LV26VKtXr1bt2rUd7UFBQcrNzdWpU6ec+qenpysoKMjR5893iTq/fr7Pn40bN04ZGRmO5eeffy7GowEAAADKH5cGC2OM4uPj9eGHH2rVqlUKDw932t66dWt5eHgoOTnZ0bZnzx4dOnRIERERkqSIiAjt2LFDR48edfRJSkqS3W5XkyZNLrpfLy8v2e12pwUAAABA0bl0jsXw4cO1cOFCffzxx6pSpYpjToSvr6+8vb3l6+urQYMGacyYMfL395fdbteIESMUERGh9u3bS5K6du2qJk2a6P7779dLL72ktLQ0Pf300xo+fLi8vLxceXgAAABAueHSYDF37lxJUqdOnZzaExISNHDgQEnS9OnT5ebmpl69eiknJ0fR0dGaM2eOo6+7u7uWLl2qRx55RBEREapcubLi4uL07LPPltRhAAAAAOVeqfodC1fhdyxKBr9jgesFv2MBACgvyuzvWAAAAAAomwgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALHNpsFi3bp1iY2NVq1Yt2Ww2ffTRR07bBw4cKJvN5rTExMQ49Tlx4oQGDBggu90uPz8/DRo0SFlZWSV4FAAAAABcGiyys7PVokULzZ49+5J9YmJilJqa6lj++9//Om0fMGCAvv/+eyUlJWnp0qVat26dhg4deq1LBwAAAHCBCq7cebdu3dStW7fL9vHy8lJQUNBFt+3evVvLly/Xpk2b1KZNG0nSzJkz1b17d02dOlW1atUq9poBAAAAFFTq51isWbNGAQEBatiwoR555BEdP37csS0lJUV+fn6OUCFJUVFRcnNz09dff33JMXNycpSZmem0AAAAACi6Uh0sYmJi9Pbbbys5OVlTpkzR2rVr1a1bN+Xl5UmS0tLSFBAQ4PSYChUqyN/fX2lpaZccd/LkyfL19XUswcHB1/Q4AAAAgOudSy+FupJ+/fo5/t2sWTM1b95cdevW1Zo1a9S5c+cijztu3DiNGTPGsZ6ZmUm4AAAAACwo1Wcs/qxOnTqqXr269u3bJ0kKCgrS0aNHnfqcO3dOJ06cuOS8DOmPeRt2u91pAQAAAFB0ZSpY/PLLLzp+/Lhq1qwpSYqIiNCpU6e0ZcsWR59Vq1YpPz9f7dq1c1WZAAAAQLnj0kuhsrKyHGcfJGn//v3atm2b/P395e/vr0mTJqlXr14KCgrSjz/+qCeffFL16tVTdHS0JKlx48aKiYnRkCFDNG/ePJ09e1bx8fHq168fd4QCAAAASpBLz1hs3rxZrVq1UqtWrSRJY8aMUatWrfTMM8/I3d1d27dv15133qkGDRpo0KBBat26tb788kt5eXk5xliwYIEaNWqkzp07q3v37rrllls0f/58Vx0SAAAAUC659IxFp06dZIy55PYVK1ZccQx/f38tXLiwOMsCAAAAUEhlao4FAAAAgNKJYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMuKFCzq1Kmj48ePF2g/deqU6tSpY7koAAAAAGVLkYLFgQMHlJeXV6A9JydHhw8ftlwUAAAAgLKlQmE6f/LJJ45/r1ixQr6+vo71vLw8JScnKywsrNiKAwAAAFA2FCpY9OzZU5Jks9kUFxfntM3Dw0NhYWGaNm1asRUHAAAAoGwoVLDIz8+XJIWHh2vTpk2qXr36NSkKAAAAQNlSqGBx3v79+4u7DgAAAABlWJGChSQlJycrOTlZR48edZzJOO/NN9+0XBgAAACAsqNIwWLSpEl69tln1aZNG9WsWVM2m6246wIAAABQhhQpWMybN0+JiYm6//77i7seAAAAAGVQkX7HIjc3VzfffHNx1wIAAACgjCpSsBg8eLAWLlxY3LUAAAAAKKOKdCnU77//rvnz52vlypVq3ry5PDw8nLb/61//KpbiAAAAAJQNRQoW27dvV8uWLSVJO3fudNrGRG4AAACg/ClSsFi9enVx1wEAAACgDCvSHAsAAAAAuFCRzlhERkZe9pKnVatWFbkgAAAAAGVPkYLF+fkV5509e1bbtm3Tzp07FRcXVxx1AQAAAChDihQspk+fftH2iRMnKisry1JBAAAAAMqeYp1jcd999+nNN98sziEBAAAAlAHFGixSUlJUsWLF4hwSAAAAQBlQpEuh7rnnHqd1Y4xSU1O1efNmjR8/vlgKAwAAAFB2FClY+Pr6Oq27ubmpYcOGevbZZ9W1a9diKQwAAABA2VGkYJGQkFDcdQAAAAAow4oULM7bsmWLdu/eLUlq2rSpWrVqVSxFAQAAAChbihQsjh49qn79+mnNmjXy8/OTJJ06dUqRkZFatGiRatSoUZw1AgAAACjlinRXqBEjRuj06dP6/vvvdeLECZ04cUI7d+5UZmamRo4cWdw1AgAAACjlinTGYvny5Vq5cqUaN27saGvSpIlmz57N5G0AAACgHCrSGYv8/Hx5eHgUaPfw8FB+fr7logAAAACULUUKFrfffrtGjRqlI0eOONoOHz6sxx57TJ07dy624gAAAACUDUUKFrNmzVJmZqbCwsJUt25d1a1bV+Hh4crMzNTMmTOLu0YAAAAApVyR5lgEBwdr69atWrlypX744QdJUuPGjRUVFVWsxQEAAAAoGwp1xmLVqlVq0qSJMjMzZbPZ1KVLF40YMUIjRoxQ27Zt1bRpU3355ZfXqlYAAAAApVShgsWMGTM0ZMgQ2e32Att8fX01bNgw/etf/yq24gAAAACUDYUKFt99951iYmIuub1r167asmWL5aIAAAAAlC2FChbp6ekXvc3seRUqVNCxY8csFwUAAACgbClUsLjhhhu0c+fOS27fvn27atasabkoAAAAAGVLoYJF9+7dNX78eP3+++8Ftp05c0YTJkxQjx49iq04AAAAAGVDoW43+/TTT+uDDz5QgwYNFB8fr4YNG0qSfvjhB82ePVt5eXl66qmnrkmhAAAAAEqvQgWLwMBAffXVV3rkkUc0btw4GWMkSTabTdHR0Zo9e7YCAwOvSaEAAAAASq9C//J2aGioPv/8c/3666/6+uuvtXHjRv3666/6/PPPFR4eXqix1q1bp9jYWNWqVUs2m00fffSR03ZjjJ555hnVrFlT3t7eioqK0t69e536nDhxQgMGDJDdbpefn58GDRqkrKyswh4WAAAAAAsKHSzOq1q1qtq2baubbrpJVatWLdIY2dnZatGihWbPnn3R7S+99JJeffVVzZs3T19//bUqV66s6OhopzkeAwYM0Pfff6+kpCQtXbpU69at09ChQ4tUDwAAAICiKdSlUMWtW7du6tat20W3GWM0Y8YMPf3007rrrrskSW+//bYCAwP10UcfqV+/ftq9e7eWL1+uTZs2qU2bNpKkmTNnqnv37po6dapq1apVYscCAAAAlGdFPmNxre3fv19paWmKiopytPn6+qpdu3ZKSUmRJKWkpMjPz88RKiQpKipKbm5u+vrrry85dk5OjjIzM50WAAAAAEVXaoNFWlqaJBWYDB4YGOjYlpaWpoCAAKftFSpUkL+/v6PPxUyePFm+vr6OJTg4uJirBwAAAMqXUhssrqVx48YpIyPDsfz888+uLgkAAAAo00ptsAgKCpIkpaenO7Wnp6c7tgUFBeno0aNO28+dO6cTJ044+lyMl5eX7Ha70wIAAACg6EptsAgPD1dQUJCSk5MdbZmZmfr6668VEREhSYqIiNCpU6e0ZcsWR59Vq1YpPz9f7dq1K/GaAQAAgPLKpXeFysrK0r59+xzr+/fv17Zt2+Tv76+QkBCNHj1azz//vOrXr6/w8HCNHz9etWrVUs+ePSVJjRs3VkxMjIYMGaJ58+bp7Nmzio+PV79+/bgjFAAAAFCCXBosNm/erMjISMf6mDFjJElxcXFKTEzUk08+qezsbA0dOlSnTp3SLbfcouXLl6tixYqOxyxYsEDx8fHq3Lmz3Nzc1KtXL7366qslfiwAAABAeWYzxhhXF+FqmZmZ8vX1VUZGBvMtrqGwsZ+5ugSgWBz45x2uLgEAgBJRmM/JpXaOBQAAAICyg2ABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAFhw+fFj33XefqlWrJm9vbzVr1kybN292bE9PT9fAgQNVq1YtVapUSTExMdq7d68LKwauDYIFAABAEZ08eVIdOnSQh4eHli1bpl27dmnatGmqWrWqJMkYo549e+qnn37Sxx9/rG+//VahoaGKiopSdna2i6sHilcFVxcAAABQVk2ZMkXBwcFKSEhwtIWHhzv+vXfvXm3cuFE7d+5U06ZNJUlz585VUFCQ/vvf/2rw4MElXjNwrXDGAgAAoIg++eQTtWnTRn369FFAQIBatWql119/3bE9JydHklSxYkVHm5ubm7y8vLR+/foSrxe4lggWAAAARfTTTz9p7ty5ql+/vlasWKFHHnlEI0eO1FtvvSVJatSokUJCQjRu3DidPHlSubm5mjJlin755Relpqa6uHqgeHEpFAAAQBHl5+erTZs2evHFFyVJrVq10s6dOzVv3jzFxcXJw8NDH3zwgQYNGiR/f3+5u7srKipK3bp1kzHGxdUDxYszFgAAAEVUs2ZNNWnSxKmtcePGOnTokGO9devW2rZtm06dOqXU1FQtX75cx48fV506dUq6XOCaIlgAAAAUUYcOHbRnzx6ntv/9738KDQ0t0NfX11c1atTQ3r17tXnzZt11110lVSZQIrgUCgAAoIgee+wx3XzzzXrxxRfVt29fffPNN5o/f77mz5/v6LN48WLVqFFDISEh2rFjh0aNGqWePXuqa9euLqwcKH4ECwAAgCJq27atPvzwQ40bN07PPvuswsPDNWPGDA0YMMDRJzU1VWPGjFF6erpq1qypBx54QOPHj3dh1cC1YTPMHFJmZqZ8fX2VkZEhu93u6nKuW2FjP3N1CUCxOPDPO1xdAgAAJaIwn5OZYwEAAADAMoIFAAAAAMuYYwEAQDnE5am4XnB5aunBGQsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGWlOlhMnDhRNpvNaWnUqJFj+++//67hw4erWrVq8vHxUa9evZSenu7CigEAAIDyqVQHC0lq2rSpUlNTHcv69esd2x577DF9+umnWrx4sdauXasjR47onnvucWG1AAAAQPlUwdUFXEmFChUUFBRUoD0jI0P//ve/tXDhQt1+++2SpISEBDVu3FgbN25U+/btS7pUAAAAoNwq9Wcs9u7dq1q1aqlOnToaMGCADh06JEnasmWLzp49q6ioKEffRo0aKSQkRCkpKa4qFwAAACiXSvUZi3bt2ikxMVENGzZUamqqJk2apFtvvVU7d+5UWlqaPD095efn5/SYwMBApaWlXXbcnJwc5eTkONYzMzOvRfkAAABAuVGqg0W3bt0c/27evLnatWun0NBQvffee/L29i7yuJMnT9akSZOKo0QAAAAAKgOXQl3Iz89PDRo00L59+xQUFKTc3FydOnXKqU96evpF52RcaNy4ccrIyHAsP//88zWsGgAAALj+lalgkZWVpR9//FE1a9ZU69at5eHhoeTkZMf2PXv26NChQ4qIiLjsOF5eXrLb7U4LAAAAgKIr1ZdCPf7444qNjVVoaKiOHDmiCRMmyN3dXf3795evr68GDRqkMWPGyN/fX3a7XSNGjFBERAR3hAIAAABKWKkOFr/88ov69++v48ePq0aNGrrlllu0ceNG1ahRQ5I0ffp0ubm5qVevXsrJyVF0dLTmzJnj4qoBAACA8qdUB4tFixZddnvFihU1e/ZszZ49u4QqAgAAAHAxZWqOBQAAAIDSiWABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAsusmWMyePVthYWGqWLGi2rVrp2+++cbVJQEAAADlxnURLN59912NGTNGEyZM0NatW9WiRQtFR0fr6NGjri4NAAAAKBeui2Dxr3/9S0OGDNGDDz6oJk2aaN68eapUqZLefPNNV5cGAAAAlAtlPljk5uZqy5YtioqKcrS5ubkpKipKKSkpLqwMAAAAKD8quLoAq3799Vfl5eUpMDDQqT0wMFA//PDDRR+Tk5OjnJwcx3pGRoYkKTMz89oVCuXn/ObqEoBiwd8KXA/4m4zrBX+Tr63zz68x5op9y3ywKIrJkydr0qRJBdqDg4NdUA2AssZ3hqsrAACcx9/kknH69Gn5+vpetk+ZDxbVq1eXu7u70tPTndrT09MVFBR00ceMGzdOY8aMcazn5+frxIkTqlatmmw22zWtF7hWMjMzFRwcrJ9//ll2u93V5QBAucbfZFwvjDE6ffq0atWqdcW+ZT5YeHp6qnXr1kpOTlbPnj0l/REUkpOTFR8ff9HHeHl5ycvLy6nNz8/vGlcKlAy73c7/xACglOBvMq4HVzpTcV6ZDxaSNGbMGMXFxalNmza66aabNGPGDGVnZ+vBBx90dWkAAABAuXBdBIt7771Xx44d0zPPPKO0tDS1bNlSy5cvLzChGwAAAMC1cV0EC0mKj4+/5KVPQHng5eWlCRMmFLjMDwBQ8vibjPLIZq7m3lEAAAAAcBll/gfyAAAAALgewQIAAACAZQQLAAAAAJYRLIDrxOzZsxUWFqaKFSuqXbt2+uabb1xdEgCUS+vWrVNsbKxq1aolm82mjz76yNUlASWCYAFcB959912NGTNGEyZM0NatW9WiRQtFR0fr6NGjri4NAMqd7OxstWjRQrNnz3Z1KUCJ4q5QwHWgXbt2atu2rWbNmiXpj1+fDw4O1ogRIzR27FgXVwcA5ZfNZtOHH36onj17uroU4JrjjAVQxuXm5mrLli2KiopytLm5uSkqKkopKSkurAwAAJQnBAugjPv111+Vl5dX4JfmAwMDlZaW5qKqAABAeUOwAAAAAGAZwQIo46pXry53d3elp6c7taenpysoKMhFVQEAgPKGYAGUcZ6enmrdurWSk5Mdbfn5+UpOTlZERIQLKwMAAOVJBVcXAMC6MWPGKC4uTm3atNFNN92kGTNmKDs7Ww8++KCrSwOAcicrK0v79u1zrO/fv1/btm2Tv7+/QkJCXFgZcG1xu1ngOjFr1iy9/PLLSktLU8uWLfXqq6+qXbt2ri4LAMqdNWvWKDIyskB7XFycEhMTS74goIQQLAAAAABYxhwLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAoJxITE+Xn5+fqMq5KcnKyGjdurLy8PFeXUu7069dP06ZNc3UZAMogggUA/J+BAwfKZrM5lmrVqikmJkbbt293dWnlzpNPPqmnn35a7u7ujrY1a9boL3/5i7y8vFSvXj0lJia6rsBCeOGFF3TzzTerUqVKZSLYPf3003rhhReUkZHh6lIAlDEECwC4QExMjFJTU5Wamqrk5GRVqFBBPXr0cHVZl5WXl6f8/HxXl1Fs1q9frx9//FG9evVytO3fv1933HGHIiMjtW3bNo0ePVqDBw/WihUrXFjp1cnNzVWfPn30yCOPuLqUq3LjjTeqbt26euedd1xdCoAyhmABABfw8vJSUFCQgoKC1LJlS40dO1Y///yzjh075uizY8cO3X777fL29la1atU0dOhQZWVlObYPHDhQPXv21NSpU1WzZk1Vq1ZNw4cP19mzZx19cnJy9Pjjj+uGG25Q5cqV1a5dO61Zs+aqajx/SdMnn3yiJk2ayMvLS4cOHSr0mD/++KPuuusuBQYGysfHR23bttXKlSud+oSFhem5555T//79VblyZd1www2aPXu2Y7sxRhMnTlRISIi8vLxUq1YtjRw50tJxLlq0SF26dFHFihUdbfPmzVN4eLimTZumxo0bKz4+Xr1799b06dOv6jn7swMHDshms+mDDz5QZGSkKlWqpBYtWiglJcWp3/vvv6+mTZvKy8tLYWFhRbpEaNKkSXrsscfUrFmzItV6oXXr1snDw0NpaWlO7aNHj9att97qWF+/fr1uvfVWeXt7Kzg4WCNHjlR2drZj+5w5c1S/fn1VrFhRgYGB6t27t9N4sbGxWrRokeV6AZQvBAsAuISsrCy98847qlevnqpVqyZJys7OVnR0tKpWrapNmzZp8eLFWrlypeLj450eu3r1av34449avXq13nrrLSUmJjpduhMfH6+UlBQtWrRI27dvV58+fRQTE6O9e/deVW2//fabpkyZojfeeEPff/+9AgICCj1mVlaWunfvruTkZH377beKiYlRbGysDh065NTv5ZdfVosWLfTtt99q7NixGjVqlJKSkiT98cF7+vTpeu2117R371599NFHTh+gi3KcX375pdq0aePUlpKSoqioKKe26OhopyDw4osvysfH57LLn4/tqaee0uOPP65t27apQYMG6t+/v86dOydJ2rJli/r27at+/fppx44dmjhxosaPH39NLsG6Ut0PP/ywJOm2225TnTp19J///Mfx2LNnz2rBggV66KGHJP0RGGNiYtSrVy9t375d7777rtavX+94j27evFkjR47Us88+qz179mj58uW67bbbnOq56aab9M033ygnJ6fYjxXAdcwAAIwxxsTFxRl3d3dTuXJlU7lyZSPJ1KxZ02zZssXRZ/78+aZq1aomKyvL0fbZZ58ZNzc3k5aW5hgnNDTUnDt3ztGnT58+5t577zXGGHPw4EHj7u5uDh8+7LT/zp07m3Hjxl2xzoSEBCPJbNu2zdF2NWMmJCQYX1/fy47dtGlTM3PmTMd6aGioiYmJcepz7733mm7duhljjJk2bZpp0KCByc3NLTBWUY/T19fXvP32205t9evXNy+++KJT22effWYkmd9++80YY8zx48fN3r17L7ucPXvWGGPM/v37jSTzxhtvOMb7/vvvjSSze/duY4wxf/3rX02XLl2c9vnEE0+YJk2aXLL2y7nc83+lutPT0x19p0yZYho3buxYf//9942Pj4/jPTlo0CAzdOhQp/G//PJL4+bmZs6cOWPef/99Y7fbTWZm5iVr/e6774wkc+DAgSIdK4DyqYIrQw0AlDaRkZGaO3euJOnkyZOaM2eOunXrpm+++UahoaHavXu3WrRoocqVKzse06FDB+Xn52vPnj0KDAyUJDVt2tRp4nHNmjW1Y8cOSX9cSpWXl6cGDRo47TsnJ8dxZuRKPD091bx5c8d6UcbMysrSxIkT9dlnnyk1NVXnzp3TmTNnCnyrHxERUWB9xowZkqQ+ffpoxowZqlOnjmJiYtS9e3fFxsaqQoUKRT7OM2fOOF0GdbX8/f3l7+9fqMdc+BzWrFlTknT06FE1atRIu3fv1l133eXUv0OHDpoxY4by8vKcXl+r6tWrd9V9Bw4cqKefflobN25U+/btlZiYqL59+zrek9999522b9+uBQsWOB5jjFF+fr7279+vLl26KDQ01PGaxcTE6O6771alSpUc/b29vSX9cWYMAK4WwQIALlC5cmWnD3lvvPGGfH199frrr+v555+/6nE8PDyc1m02m2OCdVZWltzd3bVly5YCH059fHyuanxvb2/ZbDbHelHGfPzxx5WUlKSpU6eqXr168vb2Vu/evZWbm3tVNUhScHCw9uzZo5UrVyopKUmPPvqoXn75Za1du7bIx1m9enWdPHnSqS0oKEjp6elObenp6bLb7Y4PwS+++KJefPHFy9a7a9cuhYSEONYvfJ3OP5+umAh/pdf9vvvu07x58yRJAQEBio2NVUJCgsLDw7Vs2TKneStZWVkaNmyY01yX80JCQuTp6amtW7dqzZo1+uKLL/TMM89o4sSJ2rRpk+OuVSdOnJAk1ahRo3gOEEC5QLAAgMuw2Wxyc3PTmTNnJEmNGzdWYmKisrOzHd8Qb9iwQW5ubmrYsOFVjdmqVSvl5eXp6NGjThNurSjKmBs2bNDAgQN19913S/rjA+mBAwcK9Nu4cWOB9caNGzvWvb29FRsbq9jYWA0fPlyNGjXSjh07inycrVq10q5du5zaIiIi9Pnnnzu1JSUlOZ1Nefjhh9W3b9/Ljl2rVq2rrqNx48basGGDU9uGDRvUoEGDYj1bIUnbtm277Ha73e60PnjwYPXv31+1a9dW3bp11aFDB8e2v/zlL9q1a9dlz4JUqFBBUVFRioqK0oQJE+Tn56dVq1bpnnvukSTt3LlTtWvXVvXq1Yt+UADKHYIFAFwgJyfHccedkydPatasWcrKylJsbKwkacCAAZowYYLi4uI0ceJEHTt2TCNGjND999/vuAzqSho0aKABAwbogQce0LRp09SqVSsdO3ZMycnJat68ue64445C112UMevXr68PPvhAsbGxstlsGj9+/EW/rd+wYYNeeukl9ezZU0lJSVq8eLE+++wzSX/coSovL0/t2rVTpUqV9M4778jb21uhoaGqVq1akY4zOjpab731llPbww8/rFmzZunJJ5/UQw89pFWrVum9995z1CEV7VKoy/nb3/6mtm3b6rnnntO9996rlJQUzZo1S3PmzCnUOIcOHdKJEyd06NAh5eXlOUJEvXr1HGcqCnMplPTHc2S32/X888/r2Wefddr297//Xe3bt1d8fLwGDx6sypUra9euXUpKStKsWbO0dOlS/fTTT7rttttUtWpVff7558rPz3cKxl9++aW6du1aqJoAgMnbAPB/4uLijCTHUqVKFdO2bVuzZMkSp37bt283kZGRpmLFisbf398MGTLEnD592mmcu+66y+kxo0aNMh07dnSs5+bmmmeeecaEhYUZDw8PU7NmTXP33Xeb7du3X7HOS00CvtKYf37c/v37TWRkpPH29jbBwcFm1qxZpmPHjmbUqFGOPqGhoWbSpEmmT58+plKlSiYoKMi88sorju0ffvihadeunbHb7aZy5cqmffv2ZuXKlZaO8/jx46ZixYrmhx9+cGpfvXq1admypfH09DR16tQxCQkJV3yuLuX85O1vv/3W0Xby5EkjyaxevdrRtmTJEtOkSRPj4eFhQkJCzMsvv+w0zoQJE0xoaOhl9/Xn99X55cL9FMX48eONu7u7OXLkSIFt33zzjenSpYvx8fExlStXNs2bNzcvvPCCMeaPidwdO3Y0VatWNd7e3qZ58+bm3XffdTz2zJkzxtfX16SkpFiqD0D5YzPGGFeFGgBA6RYWFqbRo0dr9OjRJbrfJ554QpmZmXrttddKdL+FFRcXJ5vN5pJfAR80aJCOHTumTz75pFjHnTt3rj788EN98cUXxTougOsfl0IBAEqdp556SnPmzFF+fr7c3ErnTy4ZY7RmzRqtX7++RPebkZGhHTt2aOHChcUeKqQ/JrTPnDmz2McFcP0rnX+tAaAc69at2yV/KO1Kdz26Xvj5+ekf//hHqQ0V0h8T+w8ePKjg4OAS3e9dd92lrl276uGHH1aXLl2KffzBgwdf9Y0IAOBCXAoFAKXM4cOHHXeh+rPinqAMAEBxIVgAAAAAsKz0nmMGAAAAUGYQLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGX/D+XUu+WhnYpHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check class distribution - important for understanding potential class imbalance\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['bone_relapse'].value_counts().plot(kind='bar')\n",
    "plt.title('Bone_relapse Distribution')\n",
    "plt.xlabel('Bone_relapse (0=no, 1=yes)')\n",
    "plt.ylabel('Count')\n",
    "for i, v in enumerate(df['bone_relapse'].value_counts()):\n",
    "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "os.makedirs('Outputs', exist_ok=True)\n",
    "plt.savefig('Outputs/class_distribution.png', dpi=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: 228 samples\n",
      "Test set: 58 samples\n",
      "spliting: test 0    44\n",
      "1    14\n",
      "Name: count, dtype: int64, train 0    173\n",
      "1     55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(['sample_id', 'bone_relapse'], axis=1).values\n",
    "y = df['bone_relapse'].values\n",
    "\n",
    "# Critical: split before any preprocessing to prevent data leakage\n",
    "# The test set must never influence training decisions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Keep a copy of original test data for inference testing later\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "test_Y = pd.Series(y_test)\n",
    "train_Y = pd.Series(y_train)\n",
    "print(f\"spliting: test {test_Y.value_counts()}, train {train_Y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values - gene expression data can have missing measurements\n",
    "# I use median imputation because it's robust to outliers common in expression data\n",
    "if np.isnan(X_train).any() or np.isnan(X_test).any():\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # Fit on training data only, then transform both sets\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    print(\"Filled missing values with median (fitted on train only)\")\n",
    "\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 22283)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286, 22283)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Correlation filter...\n",
      "  Filtered from 22283 to 13634 features (correlation > 0.05)\n",
      "  Reduction: 38.8%\n",
      "\n",
      "Stage 2: Mutual Information selection...\n",
      "  Selected 1750 final features\n",
      "  Total reduction: 22283 → 1750 features\n",
      "  Final reduction: 92.1%\n"
     ]
    }
   ],
   "source": [
    "# Two-stage feature selection for high-dimensional data\n",
    "# With 22,283 genes, I need to reduce dimensionality while preserving predictive signal\n",
    "# This approach is more robust than single-stage selection\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "# Stage 1: Quick correlation filter to remove clearly irrelevant genes\n",
    "def select_by_correlation(X, y, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Fast initial filter using correlation with target.\n",
    "    Removes genes with minimal linear relationship to relapse status.\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "    for i in range(X.shape[1]):\n",
    "        corr = np.abs(np.corrcoef(X[:, i], y)[0, 1])\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    selected_indices = np.where(correlations > threshold)[0]\n",
    "    \n",
    "    return selected_indices, correlations\n",
    "\n",
    "print(\"Stage 1: Correlation filter...\")\n",
    "corr_indices, corr_scores = select_by_correlation(X_train, y_train, threshold=0.05)\n",
    "X_train_filtered = X_train[:, corr_indices]\n",
    "X_test_filtered = X_test[:, corr_indices]\n",
    "\n",
    "print(f\"  Filtered from {X_train.shape[1]} to {len(corr_indices)} features (correlation > 0.05)\")\n",
    "print(f\"  Reduction: {(1 - len(corr_indices)/X_train.shape[1])*100:.1f}%\")\n",
    "\n",
    "# Stage 2: Mutual information to capture non-linear relationships\n",
    "print(\"\\nStage 2: Mutual Information selection...\")\n",
    "\n",
    "# I selected 1750 features as a balance between information retention and computational efficiency\n",
    "n_final_features = min(1750, X_train_filtered.shape[1])\n",
    "\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=n_final_features)\n",
    "X_train_selected = selector_mi.fit_transform(X_train_filtered, y_train)\n",
    "X_test_selected = selector_mi.transform(X_test_filtered)\n",
    "\n",
    "# Map selected indices back to original gene names for interpretability\n",
    "mi_selected_in_filtered = selector_mi.get_support(indices=True)\n",
    "final_selected_indices = corr_indices[mi_selected_in_filtered]\n",
    "selected_gene_names = [gene_names[i] for i in final_selected_indices]\n",
    "\n",
    "print(f\"  Selected {X_train_selected.shape[1]} final features\")\n",
    "print(f\"  Total reduction: {X_train.shape[1]} → {X_train_selected.shape[1]} features\")\n",
    "print(f\"  Final reduction: {(1 - X_train_selected.shape[1]/X_train.shape[1])*100:.1f}%\")\n",
    "\n",
    "X_train = X_train_selected\n",
    "X_test = X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 1750)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preprocessing complete (no data leakage!)\n"
     ]
    }
   ],
   "source": [
    "# Scale features to zero mean and unit variance\n",
    "# Essential for SVM, logistic regression, and neural networks to perform well\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData preprocessing complete (no data leakage!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 1: LOGISTIC REGRESSION\n",
      "============================================================\n",
      "Accuracy: 0.7414\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84        44\n",
      "           1       0.45      0.36      0.40        14\n",
      "\n",
      "    accuracy                           0.74        58\n",
      "   macro avg       0.63      0.61      0.62        58\n",
      "weighted avg       0.72      0.74      0.73        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Logistic Regression\n",
    "# Starting with a simple baseline model that's interpretable and fast\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    C=1.0\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "models['Logistic Regression'] = lr_model\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_lr,\n",
    "    'predictions': y_pred_lr\n",
    "}\n",
    "\n",
    "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 2: RANDOM FOREST - SKIPPED\n",
      "============================================================\n",
      "Reason: High computational requirements\n",
      "- Dataset: 228 samples × 1,750 features (after feature selection)\n",
      "- Random Forest builds multiple trees, each evaluating splits across many features\n",
      "- Computational cost scales with: n_estimators × n_features × tree_depth\n",
      "- Other models (Logistic Regression, SVM, XGBoost) are more efficient for this data\n",
      "============================================================\n",
      "\n",
      "Continuing with other models (SVM, XGBoost, Neural Network)...\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Random Forest\n",
    "# Skipping Random Forest due to computational constraints\n",
    "# With 1,750 features and 228 samples, building multiple trees becomes prohibitively expensive\n",
    "# The computational cost scales poorly for this high-dimensional setting\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 2: RANDOM FOREST - SKIPPED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Reason: High computational requirements\")\n",
    "print(\"- Dataset: 228 samples × 1,750 features (after feature selection)\")\n",
    "print(\"- Random Forest builds multiple trees, each evaluating splits across many features\")\n",
    "print(\"- Computational cost scales with: n_estimators × n_features × tree_depth\")\n",
    "print(\"- Other models (Logistic Regression, SVM, XGBoost) are more efficient for this data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Skip Random Forest training due to computational constraints\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=100,\n",
    "#     max_depth=10,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# rf_model.fit(X_train_scaled, y_train)\n",
    "# y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "# accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "# \n",
    "# models['Random Forest'] = rf_model\n",
    "# results['Random Forest'] = {\n",
    "#     'accuracy': accuracy_rf,\n",
    "#     'predictions': y_pred_rf\n",
    "# }\n",
    "# \n",
    "# print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "# print(f\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred_rf))\n",
    "# \n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Gene': selected_gene_names,\n",
    "#     'Importance': rf_model.feature_importances_\n",
    "# }).sort_values('Importance', ascending=False)\n",
    "# \n",
    "# print(f\"\\nTop 10 Most Important Genes:\")\n",
    "# print(feature_importance.head(10))\n",
    "\n",
    "print(\"\\nContinuing with other models (SVM, XGBoost, Neural Network)...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 3: SUPPORT VECTOR MACHINE (SVM)\n",
      "============================================================\n",
      "Accuracy: 0.7586\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86        44\n",
      "           1       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.76        58\n",
      "   macro avg       0.38      0.50      0.43        58\n",
      "weighted avg       0.58      0.76      0.65        58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Support Vector Machine\n",
    "# SVMs work well with high-dimensional data and can find non-linear decision boundaries\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 3: SUPPORT VECTOR MACHINE (SVM)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42,\n",
    "    probability=True\n",
    ")\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "models['SVM'] = svm_model\n",
    "results['SVM'] = {\n",
    "    'accuracy': accuracy_svm,\n",
    "    'predictions': y_pred_svm\n",
    "}\n",
    "\n",
    "print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 4: XGBOOST\n",
      "============================================================\n",
      "Accuracy: 0.7931\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88        44\n",
      "           1       0.75      0.21      0.33        14\n",
      "\n",
      "    accuracy                           0.79        58\n",
      "   macro avg       0.77      0.60      0.61        58\n",
      "weighted avg       0.79      0.79      0.75        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 4: XGBoost\n",
    "# Gradient boosting often performs well on structured data like gene expression\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 4: XGBOOST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "models['XGBoost'] = xgb_model\n",
    "results['XGBoost'] = {\n",
    "    'accuracy': accuracy_xgb,\n",
    "    'predictions': y_pred_xgb\n",
    "}\n",
    "\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 5: NEURAL NETWORK\n",
      "============================================================\n",
      "Accuracy: 0.7586\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        44\n",
      "           1       0.50      0.43      0.46        14\n",
      "\n",
      "    accuracy                           0.76        58\n",
      "   macro avg       0.66      0.65      0.65        58\n",
      "weighted avg       0.75      0.76      0.75        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 5: Neural Network\n",
    "# Multi-layer perceptron to capture complex non-linear patterns in gene expression\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL 5: NEURAL NETWORK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nn = nn_model.predict(X_test_scaled)\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "\n",
    "models['Neural Network'] = nn_model\n",
    "results['Neural Network'] = {\n",
    "    'accuracy': accuracy_nn,\n",
    "    'predictions': y_pred_nn\n",
    "}\n",
    "\n",
    "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "              Model  Accuracy\n",
      "            XGBoost  0.793103\n",
      "                SVM  0.758621\n",
      "     Neural Network  0.758621\n",
      "Logistic Regression  0.741379\n",
      "\n",
      "Best Model: XGBoost (Accuracy: 0.7931)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare all models by accuracy\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize model performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(comparison_df['Model'], comparison_df['Accuracy'])\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "plt.tight_layout()\n",
    "os.makedirs('Outputs', exist_ok=True)\n",
    "plt.savefig('Outputs/model_comparison.png', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest Model: {best_model_name} (Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f})\")\n",
    "\n",
    "# Generate confusion matrix for best model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "os.makedirs('Outputs', exist_ok=True)\n",
    "plt.savefig('Outputs/confusion_matrix.png', dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING FOR BEST MODEL\n",
      "============================================================\n",
      "Tuning hyperparameters for: XGBoost\n",
      "Original accuracy: 0.7931\n",
      "\n",
      "Performing Grid Search with cross-validation...\n",
      "Parameter grid: {'n_estimators': [100, 200, 300], 'max_depth': [3, 6, 9], 'learning_rate': [0.01, 0.1, 0.2], 'subsample': [0.8, 0.9, 1.0]}\n",
      "\n",
      "Fitting grid search (this may take a few minutes)...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "✅ Grid Search Complete!\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.8}\n",
      "Best CV Score (AUC-ROC): 0.8460\n",
      "\n",
      "============================================================\n",
      "COMPARISON: ORIGINAL vs TUNED\n",
      "============================================================\n",
      "Original Accuracy: 0.7931\n",
      "Tuned Accuracy:    0.7759\n",
      "Improvement:       -0.0172 (-2.17%)\n",
      "\n",
      "Tuned Model AUC-ROC: 0.6396\n",
      "\n",
      "Tuned Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.98      0.87        44\n",
      "           1       0.67      0.14      0.24        14\n",
      "\n",
      "    accuracy                           0.78        58\n",
      "   macro avg       0.72      0.56      0.55        58\n",
      "weighted avg       0.75      0.78      0.72        58\n",
      "\n",
      "\n",
      "⚠️ Tuned model is worse. Keeping original XGBoost\n",
      "Original Accuracy: 0.7931\n",
      "Tuned Accuracy:     0.7759\n",
      "Difference:          -0.0172 (-2.17%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING FOR BEST MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tune the best performing model to see if we can improve performance\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "original_accuracy = comparison_df.iloc[0]['Accuracy']\n",
    "print(f\"Tuning hyperparameters for: {best_model_name}\")\n",
    "print(f\"Original accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "# Define parameter grids for each model type\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    base_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    \n",
    "elif best_model_name == 'SVM':\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    base_model = SVC(random_state=42, probability=True)\n",
    "    \n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "    base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "elif best_model_name == 'Neural Network':\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(64,), (128,), (64, 32), (128, 64)],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "    base_model = MLPClassifier(random_state=42, max_iter=500, solver='adam', activation='relu')\n",
    "    \n",
    "else:\n",
    "    print(f\"Hyperparameter tuning not implemented for {best_model_name}\")\n",
    "    print(\"Using default parameters\")\n",
    "    tuned_model = best_model\n",
    "    param_grid = None\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "if param_grid is not None:\n",
    "    print(f\"\\nPerforming Grid Search with cross-validation...\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        base_model,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFitting grid search (this may take a few minutes)...\")\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"\\n✅ Grid Search Complete!\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best CV Score (AUC-ROC): {best_cv_score:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model on test set\n",
    "    y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, classification_report\n",
    "    \n",
    "    y_pred_proba_tuned = tuned_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "    \n",
    "    accuracy_original = original_accuracy\n",
    "    improvement = accuracy_tuned - accuracy_original\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARISON: ORIGINAL vs TUNED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original Accuracy: {accuracy_original:.4f}\")\n",
    "    print(f\"Tuned Accuracy:    {accuracy_tuned:.4f}\")\n",
    "    print(f\"Improvement:       {improvement:+.4f} ({improvement/accuracy_original*100:+.2f}%)\")\n",
    "    print(f\"\\nTuned Model AUC-ROC: {auc_tuned:.4f}\")\n",
    "    print(f\"\\nTuned Model Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_tuned))\n",
    "    \n",
    "    # Only update if tuned model performs better\n",
    "    if accuracy_tuned > original_accuracy:\n",
    "        best_model = tuned_model\n",
    "        models[best_model_name] = tuned_model\n",
    "        results[best_model_name]['accuracy'] = accuracy_tuned\n",
    "        results[best_model_name]['predictions'] = y_pred_tuned\n",
    "        print(f\"\\n✅ Using tuned {best_model_name} as final model\")\n",
    "    else:\n",
    "        best_model = models[best_model_name]\n",
    "        print(f\"\\n⚠️ Tuned model is worse. Keeping original {best_model_name}\")\n",
    "        print(f\"Original Accuracy: {accuracy_original:.4f}\")\n",
    "        print(f\"Tuned Accuracy:     {accuracy_tuned:.4f}\")\n",
    "        print(f\"Difference:          {improvement:+.4f} ({improvement/accuracy_original*100:+.2f}%)\")\n",
    "\n",
    "else:\n",
    "    best_model = models[best_model_name]\n",
    "    print(f\"Using default parameters for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model and Create Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to: Models/xgboost_model.pkl\n",
      "Scaler saved to: Models/scaler.pkl\n",
      "Correlation filter indices saved to: Models/corr_indices.pkl\n",
      "Mutual Information selector saved to: Models/selector_mi.pkl\n",
      "\n",
      "============================================================\n",
      "TESTING INFERENCE FUNCTION\n",
      "============================================================\n",
      "True label: 0\n",
      "Predicted label: 0\n",
      "Probabilities: [No relapse: 0.8444, Relapse: 0.1556]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Save the best model and all preprocessing components for inference\n",
    "os.makedirs('Models', exist_ok=True)\n",
    "\n",
    "model_path = f'Models/{best_model_name.lower().replace(\" \", \"_\")}_model.pkl'\n",
    "scaler_path = 'Models/scaler.pkl'\n",
    "corr_indices_path = 'Models/corr_indices.pkl'\n",
    "selector_mi_path = 'Models/selector_mi.pkl'\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "joblib.dump(corr_indices, corr_indices_path)\n",
    "joblib.dump(selector_mi, selector_mi_path)\n",
    "\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "print(f\"Correlation filter indices saved to: {corr_indices_path}\")\n",
    "print(f\"Mutual Information selector saved to: {selector_mi_path}\")\n",
    "\n",
    "# Inference function for making predictions on new samples\n",
    "def predict_relapse(gene_expression_data, model_path, scaler_path, corr_indices_path, selector_mi_path):\n",
    "    \"\"\"\n",
    "    Predict relapse status from gene expression data.\n",
    "    \n",
    "    Important: Input must have exactly 22,283 features in the same order as training data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gene_expression_data : np.array or pd.DataFrame\n",
    "        Gene expression values (n_samples × 22,283 genes)\n",
    "    model_path : str\n",
    "        Path to saved model\n",
    "    scaler_path : str\n",
    "        Path to saved scaler\n",
    "    corr_indices_path : str\n",
    "        Path to saved correlation filter indices\n",
    "    selector_mi_path : str\n",
    "        Path to saved mutual information selector\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction : int or array\n",
    "        Predicted relapse status (0 = no relapse, 1 = relapse)\n",
    "    probability : array\n",
    "        Probability of relapse [P(no relapse), P(relapse)]\n",
    "    \"\"\"\n",
    "    # Load all preprocessing components and model\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    corr_indices = joblib.load(corr_indices_path)\n",
    "    selector_mi = joblib.load(selector_mi_path)\n",
    "    \n",
    "    if isinstance(gene_expression_data, pd.DataFrame):\n",
    "        gene_expression_data = gene_expression_data.values\n",
    "    \n",
    "    # Verify input dimensions match training data\n",
    "    if gene_expression_data.shape[1] != 22283:\n",
    "        raise ValueError(f\"Input must have 22,283 features (genes), but got {gene_expression_data.shape[1]}. \"\n",
    "                        f\"Features must be in the same order as training data.\")\n",
    "    \n",
    "    # Apply same preprocessing pipeline as training\n",
    "    X_filtered = gene_expression_data[:, corr_indices]\n",
    "    X_selected = selector_mi.transform(X_filtered)\n",
    "    X_scaled = scaler.transform(X_selected)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X_scaled)\n",
    "    probability = model.predict_proba(X_scaled)\n",
    "    \n",
    "    if len(prediction) == 1:\n",
    "        return prediction[0], probability[0]\n",
    "    else:\n",
    "        return prediction, probability\n",
    "\n",
    "# Test the inference function\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING INFERENCE FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_sample = X_test_original[0:1]\n",
    "pred, prob = predict_relapse(test_sample, model_path, scaler_path, corr_indices_path, selector_mi_path)\n",
    "\n",
    "print(f\"True label: {y_test[0]}\")\n",
    "print(f\"Predicted label: {pred}\")\n",
    "print(f\"Probabilities: [No relapse: {prob[0]:.4f}, Relapse: {prob[1]:.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
