{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49785c17",
   "metadata": {
    "executionInfo": {
     "elapsed": 5366,
     "status": "ok",
     "timestamp": 1769353476442,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "49785c17"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72164651",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1769353476650,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "72164651",
    "outputId": "98bd268e-cfea-4051-9652-088e26a30847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2067 promoter sequences and 27731 non-promoter sequences.\n",
      "Final encoded dataset shape: (4134, 200), labels shape: (4134,)\n",
      "X_train: (3307, 200), X_test: (827, 200)\n",
      "Processed dataset saved to dna_dataset_processed.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# I'm using TATA box promoters as the positive class since they're well-characterized\n",
    "# and represent a clear biological signal for transcription initiation\n",
    "promoter_file = \"human_TATA_5000.fa\"\n",
    "non_promoter_file = \"human_nonprom_big.fa\"\n",
    "\n",
    "# Standardizing to 200bp ensures all models see the same input length\n",
    "# This length captures typical promoter regions while remaining computationally efficient\n",
    "seq_len = 200\n",
    "\n",
    "# Encoding nucleotides as integers: A=1, C=2, G=3, T=4, unknown=5, padding=0\n",
    "# This mapping allows the embedding layer to learn meaningful representations\n",
    "BASE_TO_ID = {\"A\": 1, \"C\": 2, \"G\": 3, \"T\": 4}\n",
    "UNK_ID = 5\n",
    "vocab_size = 6\n",
    "\n",
    "\n",
    "def read_fasta(path):\n",
    "    \"\"\"Read a FASTA file and return a list of sequence strings.\"\"\"\n",
    "    sequences = []\n",
    "    current = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if current:\n",
    "                    sequences.append(\"\".join(current))\n",
    "                    current = []\n",
    "            else:\n",
    "                current.append(line)\n",
    "    if current:\n",
    "        sequences.append(\"\".join(current))\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def center_crop_or_pad(seq, length, pad_char=\"N\"):\n",
    "    \"\"\"Standardize sequence length by center-cropping longer sequences or symmetric padding shorter ones.\"\"\"\n",
    "    if len(seq) == length:\n",
    "        return seq\n",
    "    if len(seq) > length:\n",
    "        # For longer sequences, I extract the center region to preserve the most informative part\n",
    "        mid = len(seq) // 2\n",
    "        half = length // 2\n",
    "        start = max(0, mid - half)\n",
    "        end = start + length\n",
    "        return seq[start:end]\n",
    "    # Shorter sequences get padded symmetrically with N to maintain positional context\n",
    "    pad_total = length - len(seq)\n",
    "    left = pad_total // 2\n",
    "    right = pad_total - left\n",
    "    return pad_char * left + seq + pad_char * right\n",
    "\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    \"\"\"Convert a DNA string to a list of integer token IDs.\"\"\"\n",
    "    seq = seq.upper()\n",
    "    return [BASE_TO_ID.get(base, UNK_ID) for base in seq]\n",
    "\n",
    "\n",
    "promoter_seqs = read_fasta(promoter_file)\n",
    "non_promoter_seqs = read_fasta(non_promoter_file)\n",
    "\n",
    "print(f\"Loaded {len(promoter_seqs)} promoter sequences and {len(non_promoter_seqs)} non-promoter sequences.\")\n",
    "\n",
    "# Balancing the classes prevents the model from learning a trivial majority-class predictor\n",
    "num_samples = min(len(promoter_seqs), len(non_promoter_seqs))\n",
    "promoter_seqs = promoter_seqs[:num_samples]\n",
    "non_promoter_seqs = non_promoter_seqs[:num_samples]\n",
    "\n",
    "all_seqs = []\n",
    "all_labels = []\n",
    "\n",
    "for s in promoter_seqs:\n",
    "    s_fixed = center_crop_or_pad(s, seq_len)\n",
    "    all_seqs.append(encode_sequence(s_fixed))\n",
    "    all_labels.append(1)\n",
    "\n",
    "for s in non_promoter_seqs:\n",
    "    s_fixed = center_crop_or_pad(s, seq_len)\n",
    "    all_seqs.append(encode_sequence(s_fixed))\n",
    "    all_labels.append(0)\n",
    "\n",
    "all_seqs = np.array(all_seqs, dtype=np.int64)\n",
    "all_labels = np.array(all_labels, dtype=np.int64)\n",
    "\n",
    "print(f\"Final encoded dataset shape: {all_seqs.shape}, labels shape: {all_labels.shape}\")\n",
    "\n",
    "# Using stratified split ensures both classes are represented proportionally in train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_seqs,\n",
    "    all_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_labels,\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "# Saving the processed data allows me to skip preprocessing in future runs\n",
    "processed_path = \"dna_dataset_processed.npz\"\n",
    "np.savez(\n",
    "    processed_path,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    seq_len=seq_len,\n",
    "    vocab_size=vocab_size,\n",
    ")\n",
    "\n",
    "print(f\"Processed dataset saved to {processed_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dcd989",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1769353476764,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "59dcd989",
    "outputId": "d32bf599-ccad-45d4-d8f8-089eb0ede142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "class DNASequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        # LongTensor is required for embedding layers and integer class labels\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = DNASequenceDataset(X_train, y_train)\n",
    "test_dataset = DNASequenceDataset(X_test, y_test)\n",
    "\n",
    "# Batch size of 32 provides a good balance between memory usage and gradient stability\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885215b",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1769353476771,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "5885215b"
   },
   "outputs": [],
   "source": [
    "# Step 4: Define LSTM Model\n",
    "class DNAClassifierLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based classifier for DNA promoter prediction.\n",
    "    \n",
    "    The architecture learns sequence-level patterns by processing nucleotides\n",
    "    sequentially, which is well-suited for capturing dependencies in DNA sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128,\n",
    "                 num_layers=2, num_classes=2, dropout=0.3, bidirectional=True):\n",
    "        super(DNAClassifierLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        lstm_output_size = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # For bidirectional LSTMs, I concatenate forward and backward final states\n",
    "        # to capture context from both sequence directions\n",
    "        if self.lstm.bidirectional:\n",
    "            forward_hidden = hidden[-2]\n",
    "            backward_hidden = hidden[-1]\n",
    "            final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        else:\n",
    "            final_hidden = hidden[-1]\n",
    "\n",
    "        out = self.dropout(self.relu(self.fc1(final_hidden)))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b7952",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1769353476789,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "950b7952"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=50, patience=10, device='cuda'):\n",
    "    \"\"\"Train model with early stopping to prevent overfitting.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # StepLR reduces learning rate every 5 epochs to fine-tune convergence\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping prevents exploding gradients in RNN architectures\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in test_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        # Early stopping monitors validation loss to avoid overfitting\n",
    "        if avg_val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_state_dict = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f'No improvement for {epochs_no_improve} epoch(s)')\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "        print('-' * 60)\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf13df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18161,
     "status": "ok",
     "timestamp": 1769353494953,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "d6bf13df",
    "outputId": "720cbedd-35dd-4241-c278-f9053bf8e317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Train Loss: 0.6334, Train Acc: 65.13%\n",
      "Val Loss: 0.6140, Val Acc: 67.23%\n",
      "------------------------------------------------------------\n",
      "Epoch [2/50]\n",
      "Train Loss: 0.6008, Train Acc: 68.43%\n",
      "Val Loss: 0.6058, Val Acc: 69.29%\n",
      "------------------------------------------------------------\n",
      "Epoch [3/50]\n",
      "Train Loss: 0.5798, Train Acc: 70.28%\n",
      "Val Loss: 0.5532, Val Acc: 70.62%\n",
      "------------------------------------------------------------\n",
      "Epoch [4/50]\n",
      "Train Loss: 0.5495, Train Acc: 71.48%\n",
      "Val Loss: 0.5325, Val Acc: 73.28%\n",
      "------------------------------------------------------------\n",
      "Epoch [5/50]\n",
      "Train Loss: 0.5368, Train Acc: 73.45%\n",
      "Val Loss: 0.5877, Val Acc: 64.09%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [6/50]\n",
      "Train Loss: 0.5294, Train Acc: 71.15%\n",
      "Val Loss: 0.5457, Val Acc: 70.98%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [7/50]\n",
      "Train Loss: 0.5188, Train Acc: 73.90%\n",
      "Val Loss: 0.5238, Val Acc: 73.76%\n",
      "------------------------------------------------------------\n",
      "Epoch [8/50]\n",
      "Train Loss: 0.5099, Train Acc: 74.54%\n",
      "Val Loss: 0.5259, Val Acc: 74.24%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [9/50]\n",
      "Train Loss: 0.5126, Train Acc: 74.51%\n",
      "Val Loss: 0.5227, Val Acc: 74.00%\n",
      "------------------------------------------------------------\n",
      "Epoch [10/50]\n",
      "Train Loss: 0.5072, Train Acc: 74.99%\n",
      "Val Loss: 0.5227, Val Acc: 74.00%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [11/50]\n",
      "Train Loss: 0.5053, Train Acc: 74.90%\n",
      "Val Loss: 0.5248, Val Acc: 74.24%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [12/50]\n",
      "Train Loss: 0.5003, Train Acc: 75.23%\n",
      "Val Loss: 0.5256, Val Acc: 74.37%\n",
      "No improvement for 3 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [13/50]\n",
      "Train Loss: 0.5018, Train Acc: 75.05%\n",
      "Val Loss: 0.5259, Val Acc: 74.49%\n",
      "No improvement for 4 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [14/50]\n",
      "Train Loss: 0.5004, Train Acc: 75.14%\n",
      "Val Loss: 0.5265, Val Acc: 74.49%\n",
      "No improvement for 5 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [15/50]\n",
      "Train Loss: 0.5040, Train Acc: 74.78%\n",
      "Val Loss: 0.5259, Val Acc: 74.49%\n",
      "No improvement for 6 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [16/50]\n",
      "Train Loss: 0.4995, Train Acc: 75.29%\n",
      "Val Loss: 0.5261, Val Acc: 74.37%\n",
      "No improvement for 7 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [17/50]\n",
      "Train Loss: 0.5009, Train Acc: 74.99%\n",
      "Val Loss: 0.5261, Val Acc: 74.37%\n",
      "No improvement for 8 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [18/50]\n",
      "Train Loss: 0.4995, Train Acc: 75.26%\n",
      "Val Loss: 0.5261, Val Acc: 74.24%\n",
      "No improvement for 9 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [19/50]\n",
      "Train Loss: 0.5035, Train Acc: 75.42%\n",
      "Val Loss: 0.5261, Val Acc: 74.37%\n",
      "No improvement for 10 epoch(s)\n",
      "Early stopping triggered at epoch 19\n"
     ]
    }
   ],
   "source": [
    "# Baseline unidirectional LSTM: processes sequences left-to-right only\n",
    "# This serves as a reference point to compare against bidirectional architectures\n",
    "model_lstm = DNAClassifierLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "trained_lstm, history_lstm = train_model(\n",
    "    model_lstm, train_loader, test_loader,\n",
    "    num_epochs=50,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680d363",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1769353495043,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "1680d363",
    "outputId": "dfe26eae-e2a7-4edd-8482-44df83edb74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model 1 - LSTM] Test Accuracy: 0.7437\n",
      "\n",
      "[Model 1 - LSTM] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Promoter       0.69      0.88      0.77       414\n",
      "    Promoter       0.83      0.61      0.70       413\n",
      "\n",
      "    accuracy                           0.74       827\n",
      "   macro avg       0.76      0.74      0.74       827\n",
      "weighted avg       0.76      0.74      0.74       827\n",
      "\n",
      "Model 1 saved to dna_lstm_classifier_lstm.pth\n"
     ]
    }
   ],
   "source": [
    "trained_lstm.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        outputs = trained_lstm(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy_lstm = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n[Model 1 - LSTM] Test Accuracy: {accuracy_lstm:.4f}\")\n",
    "\n",
    "print(\"\\n[Model 1 - LSTM] Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Non-Promoter', 'Promoter']))\n",
    "\n",
    "torch.save(trained_lstm.state_dict(), 'dna_lstm_classifier_lstm.pth')\n",
    "print(\"Model 1 saved to dna_lstm_classifier_lstm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f01b30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32302,
     "status": "ok",
     "timestamp": 1769353527353,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "75f01b30",
    "outputId": "35760d34-9a62-4cf1-891c-25cf6099c7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Train Loss: 0.6106, Train Acc: 65.92%\n",
      "Val Loss: 0.6508, Val Acc: 61.79%\n",
      "------------------------------------------------------------\n",
      "Epoch [2/50]\n",
      "Train Loss: 0.5884, Train Acc: 69.01%\n",
      "Val Loss: 0.5588, Val Acc: 70.74%\n",
      "------------------------------------------------------------\n",
      "Epoch [3/50]\n",
      "Train Loss: 0.5642, Train Acc: 71.73%\n",
      "Val Loss: 0.5478, Val Acc: 72.19%\n",
      "------------------------------------------------------------\n",
      "Epoch [4/50]\n",
      "Train Loss: 0.5858, Train Acc: 69.40%\n",
      "Val Loss: 0.5620, Val Acc: 71.70%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [5/50]\n",
      "Train Loss: 0.5650, Train Acc: 70.58%\n",
      "Val Loss: 0.5415, Val Acc: 72.43%\n",
      "------------------------------------------------------------\n",
      "Epoch [6/50]\n",
      "Train Loss: 0.5391, Train Acc: 72.69%\n",
      "Val Loss: 0.5693, Val Acc: 69.53%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [7/50]\n",
      "Train Loss: 0.5384, Train Acc: 72.75%\n",
      "Val Loss: 0.5416, Val Acc: 71.46%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [8/50]\n",
      "Train Loss: 0.5332, Train Acc: 73.00%\n",
      "Val Loss: 0.5630, Val Acc: 71.83%\n",
      "No improvement for 3 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [9/50]\n",
      "Train Loss: 0.5325, Train Acc: 73.39%\n",
      "Val Loss: 0.5470, Val Acc: 72.19%\n",
      "No improvement for 4 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [10/50]\n",
      "Train Loss: 0.5296, Train Acc: 73.81%\n",
      "Val Loss: 0.5442, Val Acc: 73.16%\n",
      "No improvement for 5 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [11/50]\n",
      "Train Loss: 0.5255, Train Acc: 74.06%\n",
      "Val Loss: 0.5477, Val Acc: 72.79%\n",
      "No improvement for 6 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [12/50]\n",
      "Train Loss: 0.5252, Train Acc: 74.09%\n",
      "Val Loss: 0.5452, Val Acc: 73.04%\n",
      "No improvement for 7 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [13/50]\n",
      "Train Loss: 0.5214, Train Acc: 74.30%\n",
      "Val Loss: 0.5410, Val Acc: 73.04%\n",
      "------------------------------------------------------------\n",
      "Epoch [14/50]\n",
      "Train Loss: 0.5203, Train Acc: 74.48%\n",
      "Val Loss: 0.5407, Val Acc: 73.04%\n",
      "------------------------------------------------------------\n",
      "Epoch [15/50]\n",
      "Train Loss: 0.5221, Train Acc: 74.30%\n",
      "Val Loss: 0.5443, Val Acc: 72.79%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [16/50]\n",
      "Train Loss: 0.5238, Train Acc: 74.27%\n",
      "Val Loss: 0.5411, Val Acc: 72.79%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [17/50]\n",
      "Train Loss: 0.5228, Train Acc: 74.48%\n",
      "Val Loss: 0.5408, Val Acc: 72.79%\n",
      "No improvement for 3 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [18/50]\n",
      "Train Loss: 0.5196, Train Acc: 74.69%\n",
      "Val Loss: 0.5416, Val Acc: 72.79%\n",
      "No improvement for 4 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [19/50]\n",
      "Train Loss: 0.5184, Train Acc: 74.75%\n",
      "Val Loss: 0.5422, Val Acc: 73.04%\n",
      "No improvement for 5 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [20/50]\n",
      "Train Loss: 0.5224, Train Acc: 74.36%\n",
      "Val Loss: 0.5408, Val Acc: 73.04%\n",
      "No improvement for 6 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [21/50]\n",
      "Train Loss: 0.5213, Train Acc: 74.48%\n",
      "Val Loss: 0.5418, Val Acc: 73.04%\n",
      "No improvement for 7 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [22/50]\n",
      "Train Loss: 0.5233, Train Acc: 74.33%\n",
      "Val Loss: 0.5418, Val Acc: 73.04%\n",
      "No improvement for 8 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [23/50]\n",
      "Train Loss: 0.5221, Train Acc: 74.42%\n",
      "Val Loss: 0.5418, Val Acc: 73.04%\n",
      "No improvement for 9 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [24/50]\n",
      "Train Loss: 0.5236, Train Acc: 74.21%\n",
      "Val Loss: 0.5418, Val Acc: 73.04%\n",
      "No improvement for 10 epoch(s)\n",
      "Early stopping triggered at epoch 24\n",
      "\n",
      "[Model 2 - BiLSTM] Test Accuracy: 0.7304\n",
      "\n",
      "[Model 2 - BiLSTM] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Promoter       0.67      0.90      0.77       414\n",
      "    Promoter       0.85      0.56      0.67       413\n",
      "\n",
      "    accuracy                           0.73       827\n",
      "   macro avg       0.76      0.73      0.72       827\n",
      "weighted avg       0.76      0.73      0.72       827\n",
      "\n",
      "Model 2 saved to dna_lstm_classifier_bilstm.pth\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM captures context from both sequence directions\n",
    "# This should help identify promoter motifs that depend on surrounding sequence context\n",
    "model_bilstm = DNAClassifierLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "trained_bilstm, history_bilstm = train_model(\n",
    "    model_bilstm, train_loader, test_loader,\n",
    "    num_epochs=50,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trained_bilstm.eval()\n",
    "all_preds_bi = []\n",
    "all_labels_bi = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        outputs = trained_bilstm(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds_bi.extend(predicted.cpu().numpy())\n",
    "        all_labels_bi.extend(labels.numpy())\n",
    "\n",
    "accuracy_bilstm = accuracy_score(all_labels_bi, all_preds_bi)\n",
    "print(f\"\\n[Model 2 - BiLSTM] Test Accuracy: {accuracy_bilstm:.4f}\")\n",
    "\n",
    "print(\"\\n[Model 2 - BiLSTM] Classification Report:\")\n",
    "print(classification_report(all_labels_bi, all_preds_bi, target_names=['Non-Promoter', 'Promoter']))\n",
    "\n",
    "torch.save(trained_bilstm.state_dict(), 'dna_lstm_classifier_bilstm.pth')\n",
    "print(\"Model 2 saved to dna_lstm_classifier_bilstm.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35faec28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21947,
     "status": "ok",
     "timestamp": 1769353549305,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "35faec28",
    "outputId": "258f5666-284f-47b3-b615-7464df7a0c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Train Loss: 0.6237, Train Acc: 64.38%\n",
      "Val Loss: 0.5802, Val Acc: 70.50%\n",
      "------------------------------------------------------------\n",
      "Epoch [2/50]\n",
      "Train Loss: 0.5534, Train Acc: 71.82%\n",
      "Val Loss: 0.5264, Val Acc: 72.07%\n",
      "------------------------------------------------------------\n",
      "Epoch [3/50]\n",
      "Train Loss: 0.5163, Train Acc: 74.48%\n",
      "Val Loss: 0.4706, Val Acc: 77.15%\n",
      "------------------------------------------------------------\n",
      "Epoch [4/50]\n",
      "Train Loss: 0.4776, Train Acc: 76.93%\n",
      "Val Loss: 0.3713, Val Acc: 83.31%\n",
      "------------------------------------------------------------\n",
      "Epoch [5/50]\n",
      "Train Loss: 0.3157, Train Acc: 86.12%\n",
      "Val Loss: 0.2129, Val Acc: 92.14%\n",
      "------------------------------------------------------------\n",
      "Epoch [6/50]\n",
      "Train Loss: 0.1933, Train Acc: 92.56%\n",
      "Val Loss: 0.1793, Val Acc: 93.71%\n",
      "------------------------------------------------------------\n",
      "Epoch [7/50]\n",
      "Train Loss: 0.1682, Train Acc: 93.62%\n",
      "Val Loss: 0.1723, Val Acc: 93.59%\n",
      "------------------------------------------------------------\n",
      "Epoch [8/50]\n",
      "Train Loss: 0.1585, Train Acc: 94.07%\n",
      "Val Loss: 0.1609, Val Acc: 94.20%\n",
      "------------------------------------------------------------\n",
      "Epoch [9/50]\n",
      "Train Loss: 0.1503, Train Acc: 94.25%\n",
      "Val Loss: 0.1576, Val Acc: 94.80%\n",
      "------------------------------------------------------------\n",
      "Epoch [10/50]\n",
      "Train Loss: 0.1332, Train Acc: 95.01%\n",
      "Val Loss: 0.1508, Val Acc: 94.44%\n",
      "------------------------------------------------------------\n",
      "Epoch [11/50]\n",
      "Train Loss: 0.1243, Train Acc: 95.62%\n",
      "Val Loss: 0.1497, Val Acc: 95.04%\n",
      "------------------------------------------------------------\n",
      "Epoch [12/50]\n",
      "Train Loss: 0.1243, Train Acc: 95.65%\n",
      "Val Loss: 0.1497, Val Acc: 95.28%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [13/50]\n",
      "Train Loss: 0.1189, Train Acc: 95.71%\n",
      "Val Loss: 0.1484, Val Acc: 94.80%\n",
      "------------------------------------------------------------\n",
      "Epoch [14/50]\n",
      "Train Loss: 0.1183, Train Acc: 95.74%\n",
      "Val Loss: 0.1484, Val Acc: 95.04%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [15/50]\n",
      "Train Loss: 0.1178, Train Acc: 96.01%\n",
      "Val Loss: 0.1485, Val Acc: 95.04%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [16/50]\n",
      "Train Loss: 0.1162, Train Acc: 95.89%\n",
      "Val Loss: 0.1485, Val Acc: 94.92%\n",
      "No improvement for 3 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [17/50]\n",
      "Train Loss: 0.1175, Train Acc: 96.01%\n",
      "Val Loss: 0.1484, Val Acc: 94.92%\n",
      "No improvement for 4 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [18/50]\n",
      "Train Loss: 0.1153, Train Acc: 96.04%\n",
      "Val Loss: 0.1483, Val Acc: 95.16%\n",
      "No improvement for 5 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [19/50]\n",
      "Train Loss: 0.1144, Train Acc: 96.16%\n",
      "Val Loss: 0.1483, Val Acc: 94.92%\n",
      "No improvement for 6 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [20/50]\n",
      "Train Loss: 0.1157, Train Acc: 95.77%\n",
      "Val Loss: 0.1483, Val Acc: 95.16%\n",
      "No improvement for 7 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [21/50]\n",
      "Train Loss: 0.1159, Train Acc: 95.92%\n",
      "Val Loss: 0.1483, Val Acc: 95.16%\n",
      "No improvement for 8 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [22/50]\n",
      "Train Loss: 0.1147, Train Acc: 96.16%\n",
      "Val Loss: 0.1483, Val Acc: 95.16%\n",
      "No improvement for 9 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [23/50]\n",
      "Train Loss: 0.1157, Train Acc: 95.65%\n",
      "Val Loss: 0.1483, Val Acc: 95.16%\n",
      "No improvement for 10 epoch(s)\n",
      "Early stopping triggered at epoch 23\n",
      "\n",
      "[Model 3 - GRU] Test Accuracy: 0.9516\n",
      "\n",
      "[Model 3 - GRU] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Promoter       0.94      0.97      0.95       414\n",
      "    Promoter       0.97      0.94      0.95       413\n",
      "\n",
      "    accuracy                           0.95       827\n",
      "   macro avg       0.95      0.95      0.95       827\n",
      "weighted avg       0.95      0.95      0.95       827\n",
      "\n",
      "Model 3 saved to dna_lstm_classifier_gru.pth\n"
     ]
    }
   ],
   "source": [
    "# GRU is computationally more efficient than LSTM while often achieving similar performance\n",
    "# The simplified gating mechanism may help with faster convergence\n",
    "class DNAClassifierGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128,\n",
    "                 num_layers=2, num_classes=2, dropout=0.3, bidirectional=True):\n",
    "        super(DNAClassifierGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        gru_output_size = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc1 = nn.Linear(gru_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        if self.gru.bidirectional:\n",
    "            forward_hidden = hidden[-2]\n",
    "            backward_hidden = hidden[-1]\n",
    "            final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        else:\n",
    "            final_hidden = hidden[-1]\n",
    "        out = self.dropout(self.relu(self.fc1(final_hidden)))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model_gru = DNAClassifierGRU(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "trained_gru, history_gru = train_model(\n",
    "    model_gru, train_loader, test_loader,\n",
    "    num_epochs=50,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trained_gru.eval()\n",
    "all_preds_gru = []\n",
    "all_labels_gru = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        outputs = trained_gru(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds_gru.extend(predicted.cpu().numpy())\n",
    "        all_labels_gru.extend(labels.numpy())\n",
    "\n",
    "accuracy_gru = accuracy_score(all_labels_gru, all_preds_gru)\n",
    "print(f\"\\n[Model 3 - GRU] Test Accuracy: {accuracy_gru:.4f}\")\n",
    "\n",
    "print(\"\\n[Model 3 - GRU] Classification Report:\")\n",
    "print(classification_report(all_labels_gru, all_preds_gru, target_names=['Non-Promoter', 'Promoter']))\n",
    "\n",
    "torch.save(trained_gru.state_dict(), 'dna_lstm_classifier_gru.pth')\n",
    "print(\"Model 3 saved to dna_lstm_classifier_gru.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d642b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17156,
     "status": "ok",
     "timestamp": 1769353566473,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "779d642b",
    "outputId": "8ec6fb76-898d-4880-a4a5-bbeef8e43ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Train Loss: 0.5886, Train Acc: 68.43%\n",
      "Val Loss: 0.5425, Val Acc: 71.70%\n",
      "------------------------------------------------------------\n",
      "Epoch [2/50]\n",
      "Train Loss: 0.5371, Train Acc: 73.15%\n",
      "Val Loss: 0.6007, Val Acc: 67.59%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [3/50]\n",
      "Train Loss: 0.5270, Train Acc: 73.90%\n",
      "Val Loss: 0.5607, Val Acc: 75.09%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [4/50]\n",
      "Train Loss: 0.5130, Train Acc: 74.72%\n",
      "Val Loss: 0.5011, Val Acc: 75.82%\n",
      "------------------------------------------------------------\n",
      "Epoch [5/50]\n",
      "Train Loss: 0.4723, Train Acc: 76.84%\n",
      "Val Loss: 0.4623, Val Acc: 77.99%\n",
      "------------------------------------------------------------\n",
      "Epoch [6/50]\n",
      "Train Loss: 0.4337, Train Acc: 79.38%\n",
      "Val Loss: 0.4339, Val Acc: 78.60%\n",
      "------------------------------------------------------------\n",
      "Epoch [7/50]\n",
      "Train Loss: 0.4009, Train Acc: 81.13%\n",
      "Val Loss: 0.4170, Val Acc: 79.56%\n",
      "------------------------------------------------------------\n",
      "Epoch [8/50]\n",
      "Train Loss: 0.3805, Train Acc: 81.95%\n",
      "Val Loss: 0.4071, Val Acc: 79.56%\n",
      "------------------------------------------------------------\n",
      "Epoch [9/50]\n",
      "Train Loss: 0.3613, Train Acc: 82.49%\n",
      "Val Loss: 0.3888, Val Acc: 81.86%\n",
      "------------------------------------------------------------\n",
      "Epoch [10/50]\n",
      "Train Loss: 0.3464, Train Acc: 83.88%\n",
      "Val Loss: 0.3822, Val Acc: 81.74%\n",
      "------------------------------------------------------------\n",
      "Epoch [11/50]\n",
      "Train Loss: 0.3220, Train Acc: 85.33%\n",
      "Val Loss: 0.3769, Val Acc: 81.62%\n",
      "------------------------------------------------------------\n",
      "Epoch [12/50]\n",
      "Train Loss: 0.3174, Train Acc: 85.67%\n",
      "Val Loss: 0.3799, Val Acc: 81.50%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [13/50]\n",
      "Train Loss: 0.3152, Train Acc: 86.15%\n",
      "Val Loss: 0.3753, Val Acc: 81.98%\n",
      "------------------------------------------------------------\n",
      "Epoch [14/50]\n",
      "Train Loss: 0.3092, Train Acc: 86.18%\n",
      "Val Loss: 0.3743, Val Acc: 81.50%\n",
      "------------------------------------------------------------\n",
      "Epoch [15/50]\n",
      "Train Loss: 0.3098, Train Acc: 86.00%\n",
      "Val Loss: 0.3716, Val Acc: 82.10%\n",
      "------------------------------------------------------------\n",
      "Epoch [16/50]\n",
      "Train Loss: 0.3097, Train Acc: 86.27%\n",
      "Val Loss: 0.3716, Val Acc: 82.10%\n",
      "No improvement for 1 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [17/50]\n",
      "Train Loss: 0.3071, Train Acc: 86.88%\n",
      "Val Loss: 0.3719, Val Acc: 82.10%\n",
      "No improvement for 2 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [18/50]\n",
      "Train Loss: 0.3085, Train Acc: 86.24%\n",
      "Val Loss: 0.3719, Val Acc: 82.10%\n",
      "No improvement for 3 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [19/50]\n",
      "Train Loss: 0.3106, Train Acc: 86.54%\n",
      "Val Loss: 0.3720, Val Acc: 82.10%\n",
      "No improvement for 4 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [20/50]\n",
      "Train Loss: 0.3086, Train Acc: 86.27%\n",
      "Val Loss: 0.3721, Val Acc: 82.10%\n",
      "No improvement for 5 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [21/50]\n",
      "Train Loss: 0.3085, Train Acc: 86.91%\n",
      "Val Loss: 0.3721, Val Acc: 82.10%\n",
      "No improvement for 6 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [22/50]\n",
      "Train Loss: 0.3080, Train Acc: 86.48%\n",
      "Val Loss: 0.3721, Val Acc: 82.10%\n",
      "No improvement for 7 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [23/50]\n",
      "Train Loss: 0.3072, Train Acc: 86.09%\n",
      "Val Loss: 0.3721, Val Acc: 82.10%\n",
      "No improvement for 8 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [24/50]\n",
      "Train Loss: 0.3084, Train Acc: 86.06%\n",
      "Val Loss: 0.3721, Val Acc: 82.10%\n",
      "No improvement for 9 epoch(s)\n",
      "------------------------------------------------------------\n",
      "Epoch [25/50]\n",
      "Train Loss: 0.3074, Train Acc: 86.48%\n",
      "Val Loss: 0.3721, Val Acc: 82.10%\n",
      "No improvement for 10 epoch(s)\n",
      "Early stopping triggered at epoch 25\n",
      "\n",
      "[Model 4 - CNN-LSTM] Test Accuracy: 0.8210\n",
      "\n",
      "[Model 4 - CNN-LSTM] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Promoter       0.80      0.86      0.83       414\n",
      "    Promoter       0.84      0.79      0.81       413\n",
      "\n",
      "    accuracy                           0.82       827\n",
      "   macro avg       0.82      0.82      0.82       827\n",
      "weighted avg       0.82      0.82      0.82       827\n",
      "\n",
      "Model 4 saved to dna_lstm_classifier_cnnlstm.pth\n"
     ]
    }
   ],
   "source": [
    "# Hybrid CNN-LSTM architecture: CNNs detect local motifs, then LSTM models sequence-level dependencies\n",
    "# This combines the strengths of both architectures for DNA sequence analysis\n",
    "class DNAClassifierCNNLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, num_filters=128,\n",
    "                 kernel_size=7, hidden_dim=128, num_layers=1,\n",
    "                 num_classes=2, dropout=0.3, bidirectional=True):\n",
    "        super(DNAClassifierCNNLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # Convolutional layer acts as a motif detector, scanning for short sequence patterns\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # LSTM processes the detected motifs to capture longer-range dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_filters,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        lstm_output_size = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        conv_input = embedded.permute(0, 2, 1)\n",
    "        conv_out = self.relu(self.conv1(conv_input))\n",
    "        pooled = self.pool(conv_out)\n",
    "        lstm_input = pooled.permute(0, 2, 1)\n",
    "\n",
    "        lstm_out, (hidden, cell) = self.lstm(lstm_input)\n",
    "        if self.lstm.bidirectional:\n",
    "            forward_hidden = hidden[-2]\n",
    "            backward_hidden = hidden[-1]\n",
    "            final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        else:\n",
    "            final_hidden = hidden[-1]\n",
    "\n",
    "        out = self.dropout(self.relu(self.fc1(final_hidden)))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model_cnnlstm = DNAClassifierCNNLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    num_filters=128,\n",
    "    kernel_size=7,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    num_classes=2,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "trained_cnnlstm, history_cnnlstm = train_model(\n",
    "    model_cnnlstm, train_loader, test_loader,\n",
    "    num_epochs=50,\n",
    "    patience=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trained_cnnlstm.eval()\n",
    "all_preds_cnnlstm = []\n",
    "all_labels_cnnlstm = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        outputs = trained_cnnlstm(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds_cnnlstm.extend(predicted.cpu().numpy())\n",
    "        all_labels_cnnlstm.extend(labels.numpy())\n",
    "\n",
    "accuracy_cnnlstm = accuracy_score(all_labels_cnnlstm, all_preds_cnnlstm)\n",
    "print(f\"\\n[Model 4 - CNN-LSTM] Test Accuracy: {accuracy_cnnlstm:.4f}\")\n",
    "\n",
    "print(\"\\n[Model 4 - CNN-LSTM] Classification Report:\")\n",
    "print(classification_report(all_labels_cnnlstm, all_preds_cnnlstm, target_names=['Non-Promoter', 'Promoter']))\n",
    "\n",
    "torch.save(trained_cnnlstm.state_dict(), 'dna_lstm_classifier_cnnlstm.pth')\n",
    "print(\"Model 4 saved to dna_lstm_classifier_cnnlstm.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f89793",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1769353566491,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "b5f89793",
    "outputId": "3025973f-8133-456a-a6e2-42d98ea12c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: 0.7437\n",
      "BiLSTM: 0.7304\n",
      "GRU: 0.9516\n",
      "CNN-LSTM: 0.8210\n",
      "\n",
      "Best model: GRU with accuracy 0.9516\n",
      "Best model saved to dna_best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Compare all models and identify the best performer\n",
    "required_accuracies = ['accuracy_lstm', 'accuracy_bilstm', 'accuracy_gru', 'accuracy_cnnlstm']\n",
    "missing = [acc for acc in required_accuracies if acc not in globals()]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing accuracy values. Please run evaluation cells first. Missing: {missing}\")\n",
    "\n",
    "model_accuracies = {\n",
    "    'LSTM': accuracy_lstm,\n",
    "    'BiLSTM': accuracy_bilstm,\n",
    "    'GRU': accuracy_gru,\n",
    "    'CNN-LSTM': accuracy_cnnlstm\n",
    "}\n",
    "\n",
    "for name, acc in model_accuracies.items():\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "best_model_name = max(model_accuracies, key=model_accuracies.get)\n",
    "best_accuracy = model_accuracies[best_model_name]\n",
    "print(f\"\\nBest model: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
    "\n",
    "name_to_model = {\n",
    "    'LSTM': trained_lstm,\n",
    "    'BiLSTM': trained_bilstm,\n",
    "    'GRU': trained_gru,\n",
    "    'CNN-LSTM': trained_cnnlstm\n",
    "}\n",
    "\n",
    "best_model = name_to_model[best_model_name]\n",
    "\n",
    "best_model_path = 'dna_best_model.pth'\n",
    "torch.save(best_model.state_dict(), best_model_path)\n",
    "print(f\"Best model saved to {best_model_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
