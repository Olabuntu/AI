{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE vs GAN Comparison for Synthetic Gene Expression Generation\n",
    "# Comparing generative models to determine which works better for genomics data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import wasserstein_distance\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROJECT_DIR = 'Project10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gene_expression_data(n_samples=500, n_genes=2000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic gene expression data that mimics real RNA-seq characteristics.\n",
    "    \n",
    "    I designed this to capture key biological patterns: co-expression modules (like\n",
    "    pathways), global variation (batch effects), and log-normal distributions typical\n",
    "    of sequencing data. This structure helps both VAE and GAN learn meaningful\n",
    "    representations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples (patients/cells)\n",
    "    n_genes : int\n",
    "        Number of genes (features)\n",
    "    random_state : int\n",
    "        For reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with gene expression data and labels\n",
    "    X : np.array\n",
    "        Gene expression matrix (samples Ã— genes)\n",
    "    y : np.array\n",
    "        Sample labels (based on expression patterns)\n",
    "    gene_names : list\n",
    "        Gene identifiers (e.g., \"GENE_00001\", \"GENE_00002\", ...)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create co-expression modules - genes that are expressed together, simulating\n",
    "    # biological pathways or regulatory networks\n",
    "    n_modules = max(15, n_genes // 30)\n",
    "    genes_per_module = n_genes // n_modules\n",
    "    \n",
    "    X = np.zeros((n_samples, n_genes))\n",
    "    \n",
    "    for module_idx in range(n_modules):\n",
    "        start_gene = module_idx * genes_per_module\n",
    "        end_gene = start_gene + genes_per_module if module_idx < n_modules - 1 else n_genes\n",
    "        \n",
    "        # Shared activity level for this module across samples\n",
    "        module_activity = np.random.randn(n_samples)\n",
    "        \n",
    "        # Genes within a module are correlated with the shared activity\n",
    "        # I use variable correlation strength (0.6-0.9) to make it more realistic\n",
    "        correlation_strength = 0.6 + np.random.rand() * 0.3\n",
    "        \n",
    "        for gene_idx in range(start_gene, end_gene):\n",
    "            # Each gene combines module activity with independent noise\n",
    "            gene_expression = (\n",
    "                correlation_strength * module_activity + \n",
    "                (1 - correlation_strength) * np.random.randn(n_samples) * 0.5\n",
    "            )\n",
    "            X[:, gene_idx] = gene_expression\n",
    "    \n",
    "    # Add global factors that affect many genes - this simulates batch effects,\n",
    "    # cell type differences, or other systematic variation\n",
    "    global_factors = np.random.randn(n_samples, 3)\n",
    "    global_weights = np.random.randn(3, n_genes) * 0.2\n",
    "    X = X + global_factors @ global_weights\n",
    "    \n",
    "    # Transform to log-normal distribution, which matches real RNA-seq data\n",
    "    X = X - X.min() + 1\n",
    "    X = np.log1p(X)\n",
    "    \n",
    "    # Add technical noise to simulate sequencing variability\n",
    "    noise_level = 0.15\n",
    "    X = X + np.random.normal(0, noise_level, X.shape)\n",
    "    \n",
    "    # Gene expression values must be non-negative\n",
    "    X = np.maximum(X, 0)\n",
    "    \n",
    "    # Create binary labels based on overall expression signature\n",
    "    sample_signature = X[:, :min(50, n_genes)].mean(axis=1)\n",
    "    y = (sample_signature > np.median(sample_signature)).astype(int)\n",
    "    \n",
    "    gene_names = [f\"GENE_{i+1:05d}\" for i in range(n_genes)]\n",
    "    sample_ids = [f\"SAMPLE_{i+1:04d}\" for i in range(n_samples)]\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=gene_names, index=sample_ids)\n",
    "    df['Disease_Status'] = y\n",
    "    \n",
    "    return df, X, y, gene_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaa40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset: (1000, 501)\n",
      "Disease distribution:\n",
      "Disease_Status\n",
      "1    500\n",
      "0    500\n",
      "Name: count, dtype: int64\n",
      "   âœ… Data saved: Project10/data/gene_expression_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic gene expression data\n",
    "# Using 500 genes to keep training manageable while maintaining biological realism\n",
    "df, X, y, gene_names = generate_gene_expression_data(\n",
    "    n_samples=1000,\n",
    "    n_genes=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Set up directory structure for outputs\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/data', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/models', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/figures', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/results', exist_ok=True)\n",
    "\n",
    "# Save the generated data\n",
    "df.to_csv(f'{PROJECT_DIR}/data/gene_expression_data.csv', index=True)\n",
    "print(f\"Generated dataset: {df.shape}\")\n",
    "print(f\"Disease distribution:\\n{df['Disease_Status'].value_counts()}\")\n",
    "print(f\"   âœ… Data saved: {PROJECT_DIR}/data/gene_expression_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bc853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: DATA PREPROCESSING\n",
      "======================================================================\n",
      "   âœ… Loaded 1000 samples with 500 genes\n",
      "   Data shape: (1000, 500)\n",
      "   âœ… Data scaled (mean=0, std=1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 1: DATA PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"   âœ… Loaded {len(df)} samples with {len(gene_names)} genes\")\n",
    "print(f\"   Data shape: {X.shape}\")\n",
    "\n",
    "# Standardize features to zero mean and unit variance\n",
    "# This is crucial for neural network training - it prevents any single gene\n",
    "# from dominating due to scale differences\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"   âœ… Data scaled (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: CREATING DATASET\n",
      "======================================================================\n",
      "   âœ… Dataset created: 1000 samples\n",
      "   Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: CREATING DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"Simple dataset wrapper for gene expression data.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = GeneExpressionDataset(X_scaled)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"   âœ… Dataset created: {len(dataset)} samples\")\n",
    "print(f\"   Batch size: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: DEFINING VAE MODEL\n",
      "======================================================================\n",
      "   âœ… VAE created\n",
      "   Parameters: 174,132\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: DEFINING VAE MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for gene expression data.\n",
    "    \n",
    "    The encoder maps high-dimensional gene expression to a low-dimensional\n",
    "    latent space, learning both mean and variance. The decoder reconstructs\n",
    "    expression profiles from latent codes. This architecture allows us to\n",
    "    generate new samples by sampling from the learned latent distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder: compresses gene expression to latent representation\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output mean and log-variance for the latent distribution\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: reconstructs expression from latent code\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent space parameters.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: sample z ~ N(Î¼, ÏƒÂ²) by computing\n",
    "        z = Î¼ + ÎµÂ·Ïƒ where Îµ ~ N(0,1). This makes sampling differentiable.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode from latent space to gene expression.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: encode, sample, and decode.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "input_dim = X_scaled.shape[1]\n",
    "vae = VAE(input_dim=input_dim, hidden_dim=128, latent_dim=32)\n",
    "print(f\"   âœ… VAE created\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in vae.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: DEFINING GAN MODEL\n",
      "======================================================================\n",
      "   âœ… Generator created\n",
      "   âœ… Discriminator created\n",
      "   Generator parameters: 101,748\n",
      "   Discriminator parameters: 88,961\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: DEFINING GAN MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network that transforms random noise into gene expression profiles.\n",
    "    \n",
    "    I use no activation on the final layer because the data is StandardScaled,\n",
    "    which can have negative values. This allows the generator to output the\n",
    "    full range needed to match the real data distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=32, hidden_dim=128, output_dim=200):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network that classifies samples as real or generated.\n",
    "    \n",
    "    I use LeakyReLU and dropout to prevent the discriminator from becoming\n",
    "    too strong too quickly, which helps maintain training stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=200, hidden_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "latent_dim = 32\n",
    "generator = Generator(latent_dim=latent_dim, hidden_dim=128, output_dim=input_dim)\n",
    "discriminator = Discriminator(input_dim=input_dim, hidden_dim=128)\n",
    "\n",
    "print(f\"   âœ… Generator created\")\n",
    "print(f\"   âœ… Discriminator created\")\n",
    "print(f\"   Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"   Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3759037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: TRAINING VAE\n",
      "======================================================================\n",
      "   Training VAE for 50 epochs...\n",
      "   Epoch [10/50], Loss: 1.0000, Recon: 0.9998, KL: 0.0003\n",
      "   Epoch [20/50], Loss: 0.9998, Recon: 0.9997, KL: 0.0000\n",
      "   Epoch [30/50], Loss: 0.9998, Recon: 0.9998, KL: 0.0000\n",
      "   Epoch [40/50], Loss: 0.9989, Recon: 0.9989, KL: 0.0000\n",
      "   Epoch [50/50], Loss: 1.0000, Recon: 1.0000, KL: 0.0000\n",
      "   âœ… VAE training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: TRAINING VAE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vae = vae.to(device)\n",
    "\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "vae_criterion = nn.MSELoss()\n",
    "\n",
    "def vae_loss(reconstructed, original, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss combines reconstruction error with KL divergence.\n",
    "    \n",
    "    The reconstruction term ensures the model can accurately reproduce inputs,\n",
    "    while the KL term regularizes the latent space to be close to a standard\n",
    "    normal distribution. Beta controls the trade-off - I use 1.0 for balanced\n",
    "    learning.\n",
    "    \"\"\"\n",
    "    recon_loss = vae_criterion(reconstructed, original)\n",
    "    \n",
    "    # KL divergence between learned distribution and standard normal\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    kl_loss = torch.mean(kl_loss)\n",
    "    \n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "num_epochs_vae = 50\n",
    "vae_losses = []\n",
    "vae_recon_losses = []\n",
    "vae_kl_losses = []\n",
    "\n",
    "print(f\"   Training VAE for {num_epochs_vae} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs_vae):\n",
    "    vae.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_recon = 0\n",
    "    epoch_kl = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        vae_optimizer.zero_grad()\n",
    "        reconstructed, mu, logvar = vae(batch)\n",
    "        loss, recon_loss, kl_loss = vae_loss(reconstructed, batch, mu, logvar, beta=1.0)\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_recon += recon_loss.item()\n",
    "        epoch_kl += kl_loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    avg_recon = epoch_recon / len(dataloader)\n",
    "    avg_kl = epoch_kl / len(dataloader)\n",
    "    \n",
    "    vae_losses.append(avg_loss)\n",
    "    vae_recon_losses.append(avg_recon)\n",
    "    vae_kl_losses.append(avg_kl)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"   Epoch [{epoch+1}/{num_epochs_vae}], Loss: {avg_loss:.4f}, \"\n",
    "              f\"Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\")\n",
    "\n",
    "print(\"   âœ… VAE training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: TRAINING GAN\n",
      "======================================================================\n",
      "   Training GAN for 100 epochs...\n",
      "   (GAN training can be unstable - this may take longer)\n",
      "   Epoch [20/100], G Loss: 1.9132, D Loss: 0.6867\n",
      "   Epoch [40/100], G Loss: 2.3713, D Loss: 0.6726\n",
      "   Epoch [60/100], G Loss: 2.3044, D Loss: 0.6655\n",
      "   Epoch [80/100], G Loss: 2.2898, D Loss: 0.6685\n",
      "   Epoch [100/100], G Loss: 2.2967, D Loss: 0.6648\n",
      "   âœ… GAN training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: TRAINING GAN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Using lower learning rate and different beta values for GAN training stability\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_epochs_gan = 100\n",
    "gan_g_losses = []\n",
    "gan_d_losses = []\n",
    "\n",
    "print(f\"   Training GAN for {num_epochs_gan} epochs...\")\n",
    "print(f\"   (GAN training can be unstable - this may take longer)\")\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(num_epochs_gan):\n",
    "    g_epoch_loss = 0\n",
    "    d_epoch_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.size(0)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # Real data: use label smoothing (0.9 instead of 1.0) to prevent\n",
    "        # the discriminator from becoming overconfident\n",
    "        real_labels = torch.ones(batch_size, 1).to(device) * 0.9\n",
    "        real_output = discriminator(batch)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # Fake data: generate samples and train discriminator to reject them\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data = generator(noise)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device) + 0.1\n",
    "        fake_output = discriminator(fake_data.detach())\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        # Gradient clipping prevents exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # Generator tries to fool the discriminator by making it classify\n",
    "        # fake samples as real\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data = generator(noise)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        g_epoch_loss += g_loss.item()\n",
    "        d_epoch_loss += d_loss.item()\n",
    "    \n",
    "    avg_g_loss = g_epoch_loss / len(dataloader)\n",
    "    avg_d_loss = d_epoch_loss / len(dataloader)\n",
    "    \n",
    "    gan_g_losses.append(avg_g_loss)\n",
    "    gan_d_losses.append(avg_d_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"   Epoch [{epoch+1}/{num_epochs_gan}], \"\n",
    "              f\"G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}\")\n",
    "\n",
    "print(\"   âœ… GAN training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db137fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: GENERATING SYNTHETIC SAMPLES\n",
      "======================================================================\n",
      "   âœ… Generated 100 samples from VAE\n",
      "   âœ… Generated 100 samples from GAN\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: GENERATING SYNTHETIC SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate samples from VAE by sampling from the learned latent distribution\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z_vae = torch.randn(100, 32).to(device)\n",
    "    vae_samples = vae.decode(z_vae).cpu().numpy()\n",
    "\n",
    "# Generate samples from GAN by sampling random noise\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z_gan = torch.randn(100, latent_dim).to(device)\n",
    "    gan_samples = generator(z_gan).cpu().numpy()\n",
    "\n",
    "print(f\"   âœ… Generated 100 samples from VAE\")\n",
    "print(f\"   âœ… Generated 100 samples from GAN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 8: COMPARING VAE vs GAN\n",
      "======================================================================\n",
      "\n",
      "   1. STATISTICAL SIMILARITY\n",
      "   ------------------------------------------------------------\n",
      "   Mean MSE:\n",
      "      VAE: 0.000007\n",
      "      GAN: 0.673524\n",
      "      âœ… VAE better\n",
      "\n",
      "   Std MSE:\n",
      "      VAE: 0.989515\n",
      "      GAN: 0.814004\n",
      "      âœ… GAN better\n",
      "\n",
      "   2. DISTRIBUTION SIMILARITY (Wasserstein Distance)\n",
      "   ------------------------------------------------------------\n",
      "   Wasserstein Distance (lower is better):\n",
      "      VAE: 0.801164\n",
      "      GAN: 0.142642\n",
      "      âœ… GAN better\n",
      "\n",
      "   3. TRAINING STABILITY\n",
      "   ------------------------------------------------------------\n",
      "   Loss Stability (std of last 10 epochs, lower is better):\n",
      "      VAE: 0.000344\n",
      "      GAN Generator: 0.021635\n",
      "      GAN Discriminator: 0.000635\n",
      "      âœ… VAE more stable\n",
      "\n",
      "   4. SAMPLE DIVERSITY\n",
      "   ------------------------------------------------------------\n",
      "   Average Pairwise Distance (higher = more diverse):\n",
      "      VAE: 0.163666\n",
      "      GAN: 3.174470\n",
      "      âœ… GAN more diverse\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 8: COMPARING VAE vs GAN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "real_data = X_scaled\n",
    "\n",
    "# 1. Statistical Similarity\n",
    "# Compare how well each model captures the mean and variance of real data\n",
    "print(\"\\n   1. STATISTICAL SIMILARITY\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "\n",
    "real_mean = np.mean(real_data, axis=0)\n",
    "real_std = np.std(real_data, axis=0)\n",
    "\n",
    "vae_mean = np.mean(vae_samples, axis=0)\n",
    "vae_std = np.std(vae_samples, axis=0)\n",
    "\n",
    "gan_mean = np.mean(gan_samples, axis=0)\n",
    "gan_std = np.std(gan_samples, axis=0)\n",
    "\n",
    "vae_mean_mse = mean_squared_error(real_mean, vae_mean)\n",
    "gan_mean_mse = mean_squared_error(real_mean, gan_mean)\n",
    "\n",
    "vae_std_mse = mean_squared_error(real_std, vae_std)\n",
    "gan_std_mse = mean_squared_error(real_std, gan_std)\n",
    "\n",
    "print(f\"   Mean MSE:\")\n",
    "print(f\"      VAE: {vae_mean_mse:.6f}\")\n",
    "print(f\"      GAN: {gan_mean_mse:.6f}\")\n",
    "print(f\"      {'âœ… VAE better' if vae_mean_mse < gan_mean_mse else 'âœ… GAN better'}\")\n",
    "\n",
    "print(f\"\\n   Std MSE:\")\n",
    "print(f\"      VAE: {vae_std_mse:.6f}\")\n",
    "print(f\"      GAN: {gan_std_mse:.6f}\")\n",
    "print(f\"      {'âœ… VAE better' if vae_std_mse < gan_std_mse else 'âœ… GAN better'}\")\n",
    "\n",
    "# 2. Wasserstein Distance\n",
    "# Measures how different the overall distributions are\n",
    "print(\"\\n   2. DISTRIBUTION SIMILARITY (Wasserstein Distance)\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "\n",
    "sample_size = min(100, len(real_data))\n",
    "real_sample = real_data[:sample_size].flatten()\n",
    "vae_sample = vae_samples[:sample_size].flatten()\n",
    "gan_sample = gan_samples[:sample_size].flatten()\n",
    "\n",
    "vae_wasserstein = wasserstein_distance(real_sample, vae_sample)\n",
    "gan_wasserstein = wasserstein_distance(real_sample, gan_sample)\n",
    "\n",
    "print(f\"   Wasserstein Distance (lower is better):\")\n",
    "print(f\"      VAE: {vae_wasserstein:.6f}\")\n",
    "print(f\"      GAN: {gan_wasserstein:.6f}\")\n",
    "print(f\"      {'âœ… VAE better' if vae_wasserstein < gan_wasserstein else 'âœ… GAN better'}\")\n",
    "\n",
    "# 3. Training Stability\n",
    "# Lower variance in final epochs indicates more stable training\n",
    "print(\"\\n   3. TRAINING STABILITY\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "\n",
    "vae_loss_std = np.std(vae_losses[-10:])\n",
    "gan_g_loss_std = np.std(gan_g_losses[-10:])\n",
    "gan_d_loss_std = np.std(gan_d_losses[-10:])\n",
    "\n",
    "print(f\"   Loss Stability (std of last 10 epochs, lower is better):\")\n",
    "print(f\"      VAE: {vae_loss_std:.6f}\")\n",
    "print(f\"      GAN Generator: {gan_g_loss_std:.6f}\")\n",
    "print(f\"      GAN Discriminator: {gan_d_loss_std:.6f}\")\n",
    "print(f\"      {'âœ… VAE more stable' if vae_loss_std < gan_g_loss_std else 'âœ… GAN more stable'}\")\n",
    "\n",
    "# 4. Sample Diversity\n",
    "# Higher pairwise distances indicate more diverse generated samples\n",
    "print(\"\\n   4. SAMPLE DIVERSITY\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "\n",
    "def calculate_diversity(samples):\n",
    "    \"\"\"Calculate average pairwise distance between samples.\"\"\"\n",
    "    distances = []\n",
    "    for i in range(len(samples)):\n",
    "        for j in range(i+1, len(samples)):\n",
    "            dist = np.linalg.norm(samples[i] - samples[j])\n",
    "            distances.append(dist)\n",
    "    return np.mean(distances)\n",
    "\n",
    "vae_diversity = calculate_diversity(vae_samples)\n",
    "gan_diversity = calculate_diversity(gan_samples)\n",
    "\n",
    "print(f\"   Average Pairwise Distance (higher = more diverse):\")\n",
    "print(f\"      VAE: {vae_diversity:.6f}\")\n",
    "print(f\"      GAN: {gan_diversity:.6f}\")\n",
    "print(f\"      {'âœ… VAE more diverse' if vae_diversity > gan_diversity else 'âœ… GAN more diverse'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f5d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 9: CREATING VISUALIZATIONS\n",
      "======================================================================\n",
      "   âœ… Saved: Project10/figures/training_curves.png\n",
      "   âœ… Saved: Project10/figures/pca_comparison.png\n",
      "   âœ… Saved: Project10/figures/distribution_comparison.png\n",
      "   âœ… Saved: Project10/figures/comparison_summary.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 9: CREATING VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "os.makedirs(f'{PROJECT_DIR}/figures', exist_ok=True)\n",
    "\n",
    "# 1. Training Curves\n",
    "# Visualize how loss evolves during training for both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(vae_losses, label='Total Loss', alpha=0.7)\n",
    "axes[0].plot(vae_recon_losses, label='Reconstruction Loss', alpha=0.7)\n",
    "axes[0].plot(vae_kl_losses, label='KL Divergence', alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('VAE Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(gan_g_losses, label='Generator Loss', alpha=0.7)\n",
    "axes[1].plot(gan_d_losses, label='Discriminator Loss', alpha=0.7)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('GAN Training Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/figures/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ… Saved: {PROJECT_DIR}/figures/training_curves.png\")\n",
    "\n",
    "# 2. PCA Visualization\n",
    "# Project high-dimensional data to 2D to see how well generated samples\n",
    "# match the real data distribution\n",
    "n_viz_samples = min(100, len(real_data), len(vae_samples), len(gan_samples))\n",
    "pca = PCA(n_components=2)\n",
    "real_pca = pca.fit_transform(real_data[:n_viz_samples])\n",
    "vae_pca = pca.transform(vae_samples[:n_viz_samples])\n",
    "gan_pca = pca.transform(gan_samples[:n_viz_samples])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.5, label='Real Data', s=30)\n",
    "ax.scatter(vae_pca[:, 0], vae_pca[:, 1], alpha=0.5, label='VAE Generated', s=30)\n",
    "ax.scatter(gan_pca[:, 0], gan_pca[:, 1], alpha=0.5, label='GAN Generated', s=30)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA Visualization: Real vs Generated Data')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/figures/pca_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ… Saved: {PROJECT_DIR}/figures/pca_comparison.png\")\n",
    "\n",
    "# 3. Distribution Comparison\n",
    "# Compare expression distributions for individual genes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "gene_indices = [0, 10, 50, 100]\n",
    "\n",
    "for idx, gene_idx in enumerate(gene_indices):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    axes[row, col].hist(real_data[:, gene_idx], bins=30, alpha=0.5, label='Real', density=True)\n",
    "    axes[row, col].hist(vae_samples[:, gene_idx], bins=30, alpha=0.5, label='VAE', density=True)\n",
    "    axes[row, col].hist(gan_samples[:, gene_idx], bins=30, alpha=0.5, label='GAN', density=True)\n",
    "    axes[row, col].set_xlabel(f'Gene {gene_idx} Expression')\n",
    "    axes[row, col].set_ylabel('Density')\n",
    "    axes[row, col].set_title(f'Gene {gene_idx} Distribution')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_DIR}/figures/distribution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ… Saved: {PROJECT_DIR}/figures/distribution_comparison.png\")\n",
    "\n",
    "# 4. Comparison Summary Table\n",
    "# Create a visual summary of all metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "comparison_data = [\n",
    "    ['Metric', 'VAE', 'GAN', 'Winner'],\n",
    "    ['Mean MSE', f'{vae_mean_mse:.6f}', f'{gan_mean_mse:.6f}', \n",
    "     'VAE' if vae_mean_mse < gan_mean_mse else 'GAN'],\n",
    "    ['Std MSE', f'{vae_std_mse:.6f}', f'{gan_std_mse:.6f}',\n",
    "     'VAE' if vae_std_mse < gan_std_mse else 'GAN'],\n",
    "    ['Wasserstein Distance', f'{vae_wasserstein:.6f}', f'{gan_wasserstein:.6f}',\n",
    "     'VAE' if vae_wasserstein < gan_wasserstein else 'GAN'],\n",
    "    ['Training Stability', f'{vae_loss_std:.6f}', f'{gan_g_loss_std:.6f}',\n",
    "     'VAE' if vae_loss_std < gan_g_loss_std else 'GAN'],\n",
    "    ['Sample Diversity', f'{vae_diversity:.6f}', f'{gan_diversity:.6f}',\n",
    "     'VAE' if vae_diversity > gan_diversity else 'GAN'],\n",
    "]\n",
    "\n",
    "table = ax.table(cellText=comparison_data, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.title('VAE vs GAN Comparison Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.savefig(f'{PROJECT_DIR}/figures/comparison_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ… Saved: {PROJECT_DIR}/figures/comparison_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac34afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 10: SAVING MODELS AND RESULTS\n",
      "======================================================================\n",
      "   âœ… Models saved: Project10/models/models.pth\n",
      "   âœ… Results saved: Project10/results/comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 10: SAVING MODELS AND RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "os.makedirs(f'{PROJECT_DIR}/models', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/results', exist_ok=True)\n",
    "\n",
    "# Save all model checkpoints and preprocessing scaler for reproducibility\n",
    "model_path = f'{PROJECT_DIR}/models/models.pth'\n",
    "torch.save({\n",
    "    'vae_state_dict': vae.state_dict(),\n",
    "    'generator_state_dict': generator.state_dict(),\n",
    "    'discriminator_state_dict': discriminator.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'vae_config': {'input_dim': input_dim, 'hidden_dim': 128, 'latent_dim': 32},\n",
    "    'gan_config': {'latent_dim': latent_dim, 'hidden_dim': 128, 'output_dim': input_dim},\n",
    "}, model_path)\n",
    "\n",
    "# Save quantitative comparison results\n",
    "results = {\n",
    "    'vae_mean_mse': vae_mean_mse,\n",
    "    'gan_mean_mse': gan_mean_mse,\n",
    "    'vae_std_mse': vae_std_mse,\n",
    "    'gan_std_mse': gan_std_mse,\n",
    "    'vae_wasserstein': vae_wasserstein,\n",
    "    'gan_wasserstein': gan_wasserstein,\n",
    "    'vae_diversity': vae_diversity,\n",
    "    'gan_diversity': gan_diversity,\n",
    "    'vae_loss_std': vae_loss_std,\n",
    "    'gan_g_loss_std': gan_g_loss_std,\n",
    "}\n",
    "\n",
    "results_path = f'{PROJECT_DIR}/results/comparison_results.csv'\n",
    "pd.DataFrame([results]).to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"   âœ… Models saved: {model_path}\")\n",
    "print(f\"   âœ… Results saved: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5af3630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š METRICS COMPARISON:\n",
      "   Metric                         VAE             GAN             Winner\n",
      "   ---------------------------------------------------------------------------\n",
      "   Mean MSE                       0.000007        0.673524        VAE\n",
      "   Std MSE                        0.989515        0.814004        GAN\n",
      "   Wasserstein Distance           0.801164        0.142642        GAN\n",
      "   Training Stability             0.000344        0.021635        VAE\n",
      "   Sample Diversity               0.163666        3.174470        GAN\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ“Š METRICS COMPARISON:\")\n",
    "print(f\"   {'Metric':<30} {'VAE':<15} {'GAN':<15} {'Winner'}\")\n",
    "print(\"   \" + \"-\" * 75)\n",
    "print(f\"   {'Mean MSE':<30} {vae_mean_mse:<15.6f} {gan_mean_mse:<15.6f} \"\n",
    "      f\"{'VAE' if vae_mean_mse < gan_mean_mse else 'GAN'}\")\n",
    "print(f\"   {'Std MSE':<30} {vae_std_mse:<15.6f} {gan_std_mse:<15.6f} \"\n",
    "      f\"{'VAE' if vae_std_mse < gan_std_mse else 'GAN'}\")\n",
    "print(f\"   {'Wasserstein Distance':<30} {vae_wasserstein:<15.6f} {gan_wasserstein:<15.6f} \"\n",
    "      f\"{'VAE' if vae_wasserstein < gan_wasserstein else 'GAN'}\")\n",
    "print(f\"   {'Training Stability':<30} {vae_loss_std:<15.6f} {gan_g_loss_std:<15.6f} \"\n",
    "      f\"{'VAE' if vae_loss_std < gan_g_loss_std else 'GAN'}\")\n",
    "print(f\"   {'Sample Diversity':<30} {vae_diversity:<15.6f} {gan_diversity:<15.6f} \"\n",
    "      f\"{'VAE' if vae_diversity > gan_diversity else 'GAN'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
