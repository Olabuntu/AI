{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3001e29",
   "metadata": {
    "executionInfo": {
     "elapsed": 12001,
     "status": "ok",
     "timestamp": 1768894529357,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "e3001e29"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7h8Lt6UpU-D9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768894529376,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "7h8Lt6UpU-D9",
    "outputId": "d81e6f0d-c5cc-4251-d44e-88d0891c211d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cece1",
   "metadata": {
    "id": "670cece1"
   },
   "source": [
    "# Generate synthetic DNA-protein sequence pairs using the standard genetic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63601e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1768894546948,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "eb63601e",
    "outputId": "aed09cf3-0d60-46ee-bebf-d288dc89857f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: GENERATING SYNTHETIC DNA-PROTEIN PAIRS\n",
      "======================================================================\n",
      "   Generating 20000 sequence pairs...\n",
      "   Using standard genetic code (61 coding codons)\n",
      "   Generated 1000/20000 sequences...\n",
      "   Generated 2000/20000 sequences...\n",
      "   Generated 3000/20000 sequences...\n",
      "   Generated 4000/20000 sequences...\n",
      "   Generated 5000/20000 sequences...\n",
      "   Generated 6000/20000 sequences...\n",
      "   Generated 7000/20000 sequences...\n",
      "   Generated 8000/20000 sequences...\n",
      "   Generated 9000/20000 sequences...\n",
      "   Generated 10000/20000 sequences...\n",
      "   Generated 11000/20000 sequences...\n",
      "   Generated 12000/20000 sequences...\n",
      "   Generated 13000/20000 sequences...\n",
      "   Generated 14000/20000 sequences...\n",
      "   Generated 15000/20000 sequences...\n",
      "   Generated 16000/20000 sequences...\n",
      "   Generated 17000/20000 sequences...\n",
      "   Generated 18000/20000 sequences...\n",
      "   Generated 19000/20000 sequences...\n",
      "   Generated 20000/20000 sequences...\n",
      "\n",
      "   âœ… Generated 20000 valid sequence pairs\n",
      "   DNA length range: 30 - 300 nucleotides\n",
      "   Protein length range: 10 - 100 amino acids\n",
      "\n",
      "   ðŸ“‹ Example sequences:\n",
      "      DNA: ACTATTGATTGCGCAGGAGTCACTATTGAGGGCAACGAACAATGGGCTCTAATCACCGTC...\n",
      "      Protein: TIDCAGVTIEGNEQWALITV...\n",
      "\n",
      "      DNA: TCGGTAAATCCGGAGTGTTCCGGCTGTACAGCGAGCACCTTGTCGCCCATTTACTTAAAC...\n",
      "      Protein: SVNPECSGCTASTLSPIYLN...\n",
      "\n",
      "      DNA: CAGGTAAGGCACATCATTTACACCACCTTATGGTGC...\n",
      "      Protein: QVRHIIYTTLWC...\n",
      "\n",
      "Data saved to data/dna_protein_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: SYNTHETIC DATA GENERATION (Complete Standard Genetic Code)\n",
    "# ============================================================================\n",
    "\n",
    "def get_standard_genetic_code():\n",
    "    \"\"\"\n",
    "    Returns the standard genetic code mapping (64 codons â†’ amino acids)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    genetic_code : dict\n",
    "        Mapping from DNA codon (3 nucleotides) to amino acid (1 letter code)\n",
    "        '*' represents stop codons\n",
    "    \"\"\"\n",
    "    genetic_code = {\n",
    "        # TTT, TTC -> Phenylalanine (F)\n",
    "        'TTT': 'F', 'TTC': 'F',\n",
    "        # TTA, TTG, CTT, CTC, CTA, CTG -> Leucine (L)\n",
    "        'TTA': 'L', 'TTG': 'L', 'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n",
    "        # ATT, ATC, ATA -> Isoleucine (I)\n",
    "        'ATT': 'I', 'ATC': 'I', 'ATA': 'I',\n",
    "        # ATG -> Methionine (M) - Start codon\n",
    "        'ATG': 'M',\n",
    "        # GTT, GTC, GTA, GTG -> Valine (V)\n",
    "        'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n",
    "        # TCT, TCC, TCA, TCG, AGT, AGC -> Serine (S)\n",
    "        'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'AGT': 'S', 'AGC': 'S',\n",
    "        # CCT, CCC, CCA, CCG -> Proline (P)\n",
    "        'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n",
    "        # ACT, ACC, ACA, ACG -> Threonine (T)\n",
    "        'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n",
    "        # GCT, GCC, GCA, GCG -> Alanine (A)\n",
    "        'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n",
    "        # TAT, TAC -> Tyrosine (Y)\n",
    "        'TAT': 'Y', 'TAC': 'Y',\n",
    "        # TAA, TAG, TGA -> Stop codons (*)\n",
    "        'TAA': '*', 'TAG': '*', 'TGA': '*',\n",
    "        # CAT, CAC -> Histidine (H)\n",
    "        'CAT': 'H', 'CAC': 'H',\n",
    "        # CAA, CAG -> Glutamine (Q)\n",
    "        'CAA': 'Q', 'CAG': 'Q',\n",
    "        # AAT, AAC -> Asparagine (N)\n",
    "        'AAT': 'N', 'AAC': 'N',\n",
    "        # AAA, AAG -> Lysine (K)\n",
    "        'AAA': 'K', 'AAG': 'K',\n",
    "        # GAT, GAC -> Aspartic acid (D)\n",
    "        'GAT': 'D', 'GAC': 'D',\n",
    "        # GAA, GAG -> Glutamic acid (E)\n",
    "        'GAA': 'E', 'GAG': 'E',\n",
    "        # TGT, TGC -> Cysteine (C)\n",
    "        'TGT': 'C', 'TGC': 'C',\n",
    "        # TGG -> Tryptophan (W)\n",
    "        'TGG': 'W',\n",
    "        # CGT, CGC, CGA, CGG, AGA, AGG -> Arginine (R)\n",
    "        'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'AGA': 'R', 'AGG': 'R',\n",
    "        # GGT, GGC, GGA, GGG -> Glycine (G)\n",
    "        'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n",
    "    }\n",
    "    return genetic_code\n",
    "\n",
    "def translate_dna_to_protein(dna_seq, genetic_code):\n",
    "    \"\"\"\n",
    "    Translate DNA sequence to protein using standard genetic code\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dna_seq : str\n",
    "        DNA sequence (A, T, G, C)\n",
    "    genetic_code : dict\n",
    "        Codon to amino acid mapping\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    protein_seq : str\n",
    "        Translated protein sequence (amino acids)\n",
    "    \"\"\"\n",
    "    protein_seq = ''\n",
    "    # Process in groups of 3 (codons)\n",
    "    for i in range(0, len(dna_seq) - 2, 3):\n",
    "        codon = dna_seq[i:i+3].upper()\n",
    "        if len(codon) == 3:\n",
    "            if codon in genetic_code:\n",
    "                aa = genetic_code[codon]\n",
    "                if aa == '*':  # Stop codon\n",
    "                    break\n",
    "                protein_seq += aa\n",
    "            else:\n",
    "                # Unknown codon (shouldn't happen with standard code)\n",
    "                protein_seq += 'X'  # Unknown amino acid\n",
    "    return protein_seq\n",
    "\n",
    "def generate_dna_protein_pairs(n_sequences=5000, min_codons=10, max_codons=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic DNA-protein sequence pairs using standard genetic code\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_sequences : int\n",
    "        Number of sequence pairs to generate\n",
    "    min_codons : int\n",
    "        Minimum number of codons per sequence\n",
    "    max_codons : int\n",
    "        Maximum number of codons per sequence\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: dna_sequence, protein_sequence, dna_length, protein_length\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 1: GENERATING SYNTHETIC DNA-PROTEIN PAIRS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    genetic_code = get_standard_genetic_code()\n",
    "\n",
    "    # Get all codons that code for amino acids (exclude stop codons)\n",
    "    coding_codons = [codon for codon, aa in genetic_code.items() if aa != '*']\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    print(f\"   Generating {n_sequences} sequence pairs...\")\n",
    "    print(f\"   Using standard genetic code ({len(coding_codons)} coding codons)\")\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        # Random number of codons\n",
    "        n_codons = np.random.randint(min_codons, max_codons + 1)\n",
    "\n",
    "        # Generate DNA sequence by selecting random codons\n",
    "        # This ensures valid codon structure\n",
    "        dna_seq = ''\n",
    "        for _ in range(n_codons):\n",
    "            codon = random.choice(coding_codons)\n",
    "            dna_seq += codon\n",
    "\n",
    "        # Translate to protein\n",
    "        protein_seq = translate_dna_to_protein(dna_seq, genetic_code)\n",
    "\n",
    "        # Only keep sequences with valid proteins\n",
    "        if len(protein_seq) > 0:\n",
    "            sequences.append({\n",
    "                'dna_sequence': dna_seq,\n",
    "                'protein_sequence': protein_seq,\n",
    "                'dna_length': len(dna_seq),\n",
    "                'protein_length': len(protein_seq)\n",
    "            })\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"   Generated {i + 1}/{n_sequences} sequences...\")\n",
    "\n",
    "    df = pd.DataFrame(sequences)\n",
    "    print(f\"\\n   âœ… Generated {len(df)} valid sequence pairs\")\n",
    "    print(f\"   DNA length range: {df['dna_length'].min()} - {df['dna_length'].max()} nucleotides\")\n",
    "    print(f\"   Protein length range: {df['protein_length'].min()} - {df['protein_length'].max()} amino acids\")\n",
    "\n",
    "    # Show examples\n",
    "    print(f\"\\n   ðŸ“‹ Example sequences:\")\n",
    "    for idx in range(min(3, len(df))):\n",
    "        dna_example = df.iloc[idx]['dna_sequence'][:60]\n",
    "        protein_example = df.iloc[idx]['protein_sequence'][:20]\n",
    "        print(f\"      DNA: {dna_example}...\")\n",
    "        print(f\"      Protein: {protein_example}...\")\n",
    "        print()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate data - Increased to 20,000 sequences for better learning\n",
    "df = generate_dna_protein_pairs(n_sequences=20000, min_codons=10, max_codons=100)\n",
    "\n",
    "\n",
    "# save data\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/dna_protein_pairs.csv', index=False)\n",
    "print('Data saved to data/dna_protein_pairs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f7c607",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1768894558070,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "b0f7c607",
    "outputId": "8b2e2e69-8e35-4fef-c42b-b99e5f652743"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20000,\n  \"fields\": [\n    {\n      \"column\": \"dna_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"ATAAATAGGGCTGCTTCCAGCTTCCGATCCCAGTACGACGGGACCGCCGGTTCTGACGATTGCCGAGAATTGTCTCGATCTGGGTTCAGCATAGCTATCTGGATTAGACTTGTCTACGACCTTTTAAAGCAGCGGTTTAGACGTCCCGCCGATGCTCTCATTACGCAAAAGGTATACTATATACTCTGTATGCATCGCTATCTACCAACGGGTCTGTCCCCTGGTGAGACC\",\n          \"CTTTGTTTTCCCATCCTCGAGCACCCCGCGCCTCAGGTTGCTTCGATCGGCGCGACGCTGAGCCCAGAAGAAAGGCTCACTGCCTGTACACCCATCTTGTTTCATGCCCGCCTTAACGCGATA\",\n          \"GGGCAATTTCGGTGGTTGGCGGAAGGTCTGCAGAGTATACGAAAGTATTGTCGGCTCACCACAGCGCTCATCTCGAAGACCATGACGCTTAAACCATACACACTGCTTCGATCTGTGCGAAGATGGCATCCACATTACGCGATCGGTAATGGGCGGCACCGGTTATGTACACAGGAA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"INRAASSFRSQYDGTAGSDDCRELSRSGFSIAIWIRLVYDLLKQRFRRPADALITQKVYYILCMHRYLPTGLSPGET\",\n          \"LCFPILEHPAPQVASIGATLSPEERLTACTPILFHARLNAI\",\n          \"GQFRWLAEGLQSIRKYCRLTTALISKTMTLKPYTLLRSVRRWHPHYAIGNGRHRLCTQE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dna_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 79,\n        \"min\": 30,\n        \"max\": 300,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          105,\n          231,\n          198\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26,\n        \"min\": 10,\n        \"max\": 100,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          35,\n          77,\n          66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-277c1d5e-6118-4011-9152-1ed892a4381d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dna_sequence</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>dna_length</th>\n",
       "      <th>protein_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACTATTGATTGCGCAGGAGTCACTATTGAGGGCAACGAACAATGGG...</td>\n",
       "      <td>TIDCAGVTIEGNEQWALITVNNAGIHLICASVH</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGGTAAATCCGGAGTGTTCCGGCTGTACAGCGAGCACCTTGTCGC...</td>\n",
       "      <td>SVNPECSGCTASTLSPIYLNTYSTGPRGLCQIKMRGISQWAIPYGR...</td>\n",
       "      <td>270</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAGGTAAGGCACATCATTTACACCACCTTATGGTGC</td>\n",
       "      <td>QVRHIIYTTLWC</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTCAAGGACCGACTCCTAGGTGCTAGATGTGCGACTAATCCCACAG...</td>\n",
       "      <td>FKDRLLGARCATNPTGEHWSPKVLG</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACGCGATCATCATATTCACATGGAGCACAAAA</td>\n",
       "      <td>NAIIIFTWSTK</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>AGACGCCATGTGATATCCGAGAATGTTAGACGGGAGCTATTTACAC...</td>\n",
       "      <td>RRHVISENVRRELFTHYIRVSSVSKALNPIRYASLNNFKAVHISQT...</td>\n",
       "      <td>204</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>CGCAATGCAGTACTAGTACATAATGCTGGGACCCTCGCA</td>\n",
       "      <td>RNAVLVHNAGTLA</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>CCCATTAACACCTCGCGACAGCATTGGTGTGCCAGAGCCCTATCGC...</td>\n",
       "      <td>PINTSRQHWCARALSPTSRLLRSALLMFIRVALTASYAFPPQHLIL...</td>\n",
       "      <td>297</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>CGATCAATCCTCGCTACTATTTCTCCACCTGCTTCGCGCAGACGGG...</td>\n",
       "      <td>RSILATISPPASRRREIQPMVLPDQSRHGRRDFDTHFHEPENLGGD...</td>\n",
       "      <td>237</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>ATTCGCGTCATGTCCGAGGTACACTGGCCTCCGATATGTGTCGGTC...</td>\n",
       "      <td>IRVMSEVHWPPICVGRQELRFRRRCARWYSIHVLPVRFSGAVYSNL...</td>\n",
       "      <td>162</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-277c1d5e-6118-4011-9152-1ed892a4381d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-277c1d5e-6118-4011-9152-1ed892a4381d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-277c1d5e-6118-4011-9152-1ed892a4381d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_8078ca14-258d-4982-9bd3-93f658243a16\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_8078ca14-258d-4982-9bd3-93f658243a16 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            dna_sequence  \\\n",
       "0      ACTATTGATTGCGCAGGAGTCACTATTGAGGGCAACGAACAATGGG...   \n",
       "1      TCGGTAAATCCGGAGTGTTCCGGCTGTACAGCGAGCACCTTGTCGC...   \n",
       "2                   CAGGTAAGGCACATCATTTACACCACCTTATGGTGC   \n",
       "3      TTCAAGGACCGACTCCTAGGTGCTAGATGTGCGACTAATCCCACAG...   \n",
       "4                      AACGCGATCATCATATTCACATGGAGCACAAAA   \n",
       "...                                                  ...   \n",
       "19995  AGACGCCATGTGATATCCGAGAATGTTAGACGGGAGCTATTTACAC...   \n",
       "19996            CGCAATGCAGTACTAGTACATAATGCTGGGACCCTCGCA   \n",
       "19997  CCCATTAACACCTCGCGACAGCATTGGTGTGCCAGAGCCCTATCGC...   \n",
       "19998  CGATCAATCCTCGCTACTATTTCTCCACCTGCTTCGCGCAGACGGG...   \n",
       "19999  ATTCGCGTCATGTCCGAGGTACACTGGCCTCCGATATGTGTCGGTC...   \n",
       "\n",
       "                                        protein_sequence  dna_length  \\\n",
       "0                      TIDCAGVTIEGNEQWALITVNNAGIHLICASVH          99   \n",
       "1      SVNPECSGCTASTLSPIYLNTYSTGPRGLCQIKMRGISQWAIPYGR...         270   \n",
       "2                                           QVRHIIYTTLWC          36   \n",
       "3                              FKDRLLGARCATNPTGEHWSPKVLG          75   \n",
       "4                                            NAIIIFTWSTK          33   \n",
       "...                                                  ...         ...   \n",
       "19995  RRHVISENVRRELFTHYIRVSSVSKALNPIRYASLNNFKAVHISQT...         204   \n",
       "19996                                      RNAVLVHNAGTLA          39   \n",
       "19997  PINTSRQHWCARALSPTSRLLRSALLMFIRVALTASYAFPPQHLIL...         297   \n",
       "19998  RSILATISPPASRRREIQPMVLPDQSRHGRRDFDTHFHEPENLGGD...         237   \n",
       "19999  IRVMSEVHWPPICVGRQELRFRRRCARWYSIHVLPVRFSGAVYSNL...         162   \n",
       "\n",
       "       protein_length  \n",
       "0                  33  \n",
       "1                  90  \n",
       "2                  12  \n",
       "3                  25  \n",
       "4                  11  \n",
       "...               ...  \n",
       "19995              68  \n",
       "19996              13  \n",
       "19997              99  \n",
       "19998              79  \n",
       "19999              54  \n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9eb5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1768894564371,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "a1e9eb5c",
    "outputId": "317e1210-2ea6-4f7a-f6b6-aaaa73c950a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: DATA PREPROCESSING AND ENCODING\n",
      "======================================================================\n",
      "\n",
      "   Creating vocabularies...\n",
      "   DNA vocab size: 6\n",
      "   Protein vocab size: 24\n",
      "\n",
      "   Max DNA length (95th percentile): 291\n",
      "   Max protein length (95th percentile): 96\n",
      "   Max protein length (with START/END tokens): 98\n",
      "\n",
      "   Encoding sequences...\n",
      "   âœ… Encoded 20000 sequences\n",
      "   DNA shape: (20000, 291)\n",
      "   Protein shape: (20000, 98)\n",
      "   Note: Protein sequences now include <START> and <END> tokens\n",
      "\n",
      "   Splitting data...\n",
      "   âœ… Training set: 12800 samples\n",
      "   âœ… Validation set: 3200 samples\n",
      "   âœ… Test set: 4000 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: DATA PREPROCESSING AND ENCODING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def encode_sequences(sequences, vocab, max_length, add_start_end=False):\n",
    "    \"\"\"\n",
    "    Encode sequences to integers\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequences : list\n",
    "        List of sequences (DNA or protein)\n",
    "    vocab : dict\n",
    "        Vocabulary mapping (char -> int)\n",
    "    max_length : int\n",
    "        Maximum sequence length (for padding)\n",
    "    add_start_end : bool\n",
    "        If True, add <START> at beginning and <END> at end (for protein sequences)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    encoded : np.array\n",
    "        Encoded sequences (n_samples, max_length)\n",
    "    \"\"\"\n",
    "    encoded = []\n",
    "    for seq in sequences:\n",
    "        seq_encoded = [vocab.get(char, vocab['<UNK>']) for char in seq]\n",
    "\n",
    "        if add_start_end and '<START>' in vocab and '<END>' in vocab:\n",
    "            seq_encoded = [vocab['<START>']] + seq_encoded + [vocab['<END>']]\n",
    "\n",
    "        if len(seq_encoded) < max_length:\n",
    "            seq_encoded += [vocab['<PAD>']] * (max_length - len(seq_encoded))\n",
    "        else:\n",
    "            seq_encoded = seq_encoded[:max_length]\n",
    "        encoded.append(seq_encoded)\n",
    "    return np.array(encoded)\n",
    "\n",
    "print(\"\\n   Creating vocabularies...\")\n",
    "\n",
    "dna_vocab = {'A': 0, 'T': 1, 'G': 2, 'C': 3, '<PAD>': 4, '<UNK>': 5}\n",
    "print(f\"   DNA vocab size: {len(dna_vocab)}\")\n",
    "\n",
    "amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I',\n",
    "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "protein_vocab = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "protein_vocab['<PAD>'] = len(amino_acids)\n",
    "protein_vocab['<UNK>'] = len(amino_acids) + 1\n",
    "protein_vocab['<START>'] = len(amino_acids) + 2\n",
    "protein_vocab['<END>'] = len(amino_acids) + 3\n",
    "print(f\"   Protein vocab size: {len(protein_vocab)}\")\n",
    "\n",
    "# Use 95th percentile to handle most sequences while avoiding outliers\n",
    "max_dna_length = int(df['dna_length'].quantile(0.95))\n",
    "max_protein_length = int(df['protein_length'].quantile(0.95))\n",
    "\n",
    "# Round DNA length to nearest multiple of 3 to preserve codon boundaries\n",
    "max_dna_length = ((max_dna_length // 3) + 1) * 3\n",
    "\n",
    "max_protein_length_with_tokens = max_protein_length + 2\n",
    "\n",
    "print(f\"\\n   Max DNA length (95th percentile): {max_dna_length}\")\n",
    "print(f\"   Max protein length (95th percentile): {max_protein_length}\")\n",
    "print(f\"   Max protein length (with START/END tokens): {max_protein_length_with_tokens}\")\n",
    "\n",
    "print(\"\\n   Encoding sequences...\")\n",
    "X = encode_sequences(df['dna_sequence'].tolist(), dna_vocab, max_dna_length, add_start_end=False)\n",
    "y = encode_sequences(df['protein_sequence'].tolist(), protein_vocab, max_protein_length_with_tokens, add_start_end=True)\n",
    "\n",
    "print(f\"   âœ… Encoded {len(X)} sequences\")\n",
    "print(f\"   DNA shape: {X.shape}\")\n",
    "print(f\"   Protein shape: {y.shape}\")\n",
    "\n",
    "print(\"\\n   Splitting data...\")\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"   âœ… Training set: {len(X_train)} samples\")\n",
    "print(f\"   âœ… Validation set: {len(X_val)} samples\")\n",
    "print(f\"   âœ… Test set: {len(X_test)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08aff6ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1768894571561,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "08aff6ec",
    "outputId": "4c25e811-bd85-4938-828d-0b08392c0a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING DATASET AND DATALOADER\n",
      "======================================================================\n",
      "   âœ… Created datasets\n",
      "   Training batches: 400\n",
      "   Validation batches: 100\n",
      "   Test batches: 125\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING DATASET AND DATALOADER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class DNAToProteinDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for DNA-Protein pairs\"\"\"\n",
    "    def __init__(self, dna_sequences, protein_sequences):\n",
    "        self.dna_sequences = torch.LongTensor(dna_sequences)\n",
    "        self.protein_sequences = torch.LongTensor(protein_sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dna_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dna_sequences[idx], self.protein_sequences[idx]\n",
    "\n",
    "train_dataset = DNAToProteinDataset(X_train, y_train)\n",
    "val_dataset = DNAToProteinDataset(X_val, y_val)\n",
    "test_dataset = DNAToProteinDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"   âœ… Created datasets\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7735257",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1768894576629,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "d7735257",
    "outputId": "985f3fd4-aac3-47ce-aaf2-295fb97e2e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: DEFINING ENCODER-DECODER MODEL\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: DEFINING ENCODER-DECODER MODEL\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4f54f",
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1768894579864,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "06c4f54f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: LSTM Encoder-Decoder\n",
    "# ============================================================================\n",
    "class DNAProteinTranslatorLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based Encoder-Decoder model for DNA to Protein translation\n",
    "\n",
    "    Architecture:\n",
    "    - Encoder: Bidirectional LSTM that processes DNA sequence\n",
    "    - Decoder: LSTM that generates protein sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, dna_vocab_size, protein_vocab_size, embedding_dim=64,\n",
    "                 hidden_dim=128, num_layers=2):\n",
    "        super(DNAProteinTranslatorLSTM, self).__init__()\n",
    "\n",
    "        self.protein_vocab_size = protein_vocab_size\n",
    "\n",
    "        # Encoder\n",
    "        self.dna_embedding = nn.Embedding(dna_vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.protein_embedding = nn.Embedding(protein_vocab_size, embedding_dim)\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim * 2, num_layers,  # *2 because encoder is bidirectional\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, protein_vocab_size)\n",
    "\n",
    "    def forward(self, dna_seq, protein_seq=None, max_length=100):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dna_seq : torch.Tensor\n",
    "            DNA sequence (batch, seq_len)\n",
    "        protein_seq : torch.Tensor, optional\n",
    "            Target protein sequence (for training)\n",
    "        max_length : int\n",
    "            Maximum protein length (for inference)\n",
    "        \"\"\"\n",
    "        batch_size = dna_seq.size(0)\n",
    "\n",
    "        # Encode DNA\n",
    "        dna_embedded = self.dna_embedding(dna_seq)\n",
    "        encoder_out, (hidden, cell) = self.encoder_lstm(dna_embedded)\n",
    "\n",
    "        # Prepare decoder hidden state\n",
    "        # For bidirectional LSTM: hidden shape is (num_layers * 2, batch, hidden_dim)\n",
    "        # We need to concatenate forward and backward for each layer\n",
    "        # Then reshape to (num_layers, batch, hidden_dim * 2)\n",
    "        num_layers = hidden.size(0) // 2\n",
    "        batch_size = hidden.size(1)\n",
    "        hidden_dim = hidden.size(2)\n",
    "\n",
    "        # Reshape: (num_layers * 2, batch, hidden_dim) -> (num_layers, batch, hidden_dim * 2)\n",
    "        hidden_forward = hidden[:num_layers]  # Forward direction\n",
    "        hidden_backward = hidden[num_layers:]  # Backward direction\n",
    "        hidden = torch.cat([hidden_forward, hidden_backward], dim=2)\n",
    "\n",
    "        cell_forward = cell[:num_layers]\n",
    "        cell_backward = cell[num_layers:]\n",
    "        cell = torch.cat([cell_forward, cell_backward], dim=2)\n",
    "\n",
    "        if self.training and protein_seq is not None:\n",
    "            # Teacher forcing: use actual protein sequence during training\n",
    "            protein_embedded = self.protein_embedding(protein_seq)\n",
    "            decoder_out, _ = self.decoder_lstm(protein_embedded, (hidden, cell))\n",
    "            output = self.fc(decoder_out)\n",
    "            return output\n",
    "        else:\n",
    "            # Inference: generate one token at a time\n",
    "            # START token is at index protein_vocab_size - 2, END token is at protein_vocab_size - 1\n",
    "            start_token_id = self.protein_vocab_size - 2\n",
    "            end_token_id = self.protein_vocab_size - 1\n",
    "\n",
    "            outputs = []\n",
    "            input_token = torch.full((batch_size, 1), start_token_id, dtype=torch.long).to(dna_seq.device)\n",
    "\n",
    "            for step in range(max_length):\n",
    "                protein_embedded = self.protein_embedding(input_token)\n",
    "                decoder_out, (hidden, cell) = self.decoder_lstm(\n",
    "                    protein_embedded, (hidden, cell)\n",
    "                )\n",
    "                output = self.fc(decoder_out)\n",
    "                outputs.append(output)\n",
    "\n",
    "                # Get predicted token\n",
    "                predicted = torch.argmax(output, dim=-1)\n",
    "\n",
    "                # Early stopping: check if all sequences in batch have generated END token\n",
    "                if torch.all(predicted == end_token_id):\n",
    "                    break\n",
    "\n",
    "                input_token = predicted\n",
    "\n",
    "            return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2994d1",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1768894583851,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "6a2994d1"
   },
   "outputs": [],
   "source": [
    "# MODEL 2: RNN Encoder-Decoder\n",
    "# ============================================================================\n",
    "class DNAProteinTranslatorRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based Encoder-Decoder model for DNA to Protein translation\n",
    "\n",
    "    Architecture:\n",
    "    - Encoder: Bidirectional RNN that processes DNA sequence\n",
    "    - Decoder: RNN that generates protein sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, dna_vocab_size, protein_vocab_size, embedding_dim=64,\n",
    "                 hidden_dim=128, num_layers=2):\n",
    "        super(DNAProteinTranslatorRNN, self).__init__()\n",
    "\n",
    "        self.protein_vocab_size = protein_vocab_size\n",
    "\n",
    "        # Encoder\n",
    "        self.dna_embedding = nn.Embedding(dna_vocab_size, embedding_dim)\n",
    "        self.encoder_rnn = nn.RNN(\n",
    "            embedding_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.protein_embedding = nn.Embedding(protein_vocab_size, embedding_dim)\n",
    "        self.decoder_rnn = nn.RNN(\n",
    "            embedding_dim, hidden_dim * 2, num_layers,  # *2 because encoder is bidirectional\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, protein_vocab_size)\n",
    "\n",
    "    def forward(self, dna_seq, protein_seq=None, max_length=100):\n",
    "        batch_size = dna_seq.size(0)\n",
    "\n",
    "        # Encode DNA\n",
    "        dna_embedded = self.dna_embedding(dna_seq)\n",
    "        encoder_out, hidden = self.encoder_rnn(dna_embedded)\n",
    "\n",
    "        # Prepare decoder hidden state\n",
    "        num_layers = hidden.size(0) // 2\n",
    "        hidden_forward = hidden[:num_layers]\n",
    "        hidden_backward = hidden[num_layers:]\n",
    "        hidden = torch.cat([hidden_forward, hidden_backward], dim=2)\n",
    "\n",
    "        if self.training and protein_seq is not None:\n",
    "            # Teacher forcing\n",
    "            protein_embedded = self.protein_embedding(protein_seq)\n",
    "            decoder_out, _ = self.decoder_rnn(protein_embedded, hidden)\n",
    "            output = self.fc(decoder_out)\n",
    "            return output\n",
    "        else:\n",
    "            # Inference: generate one token at a time\n",
    "            # START token is at index protein_vocab_size - 2, END token is at protein_vocab_size - 1\n",
    "            start_token_id = self.protein_vocab_size - 2\n",
    "            end_token_id = self.protein_vocab_size - 1\n",
    "\n",
    "            outputs = []\n",
    "            input_token = torch.full((batch_size, 1), start_token_id, dtype=torch.long).to(dna_seq.device)\n",
    "\n",
    "            for step in range(max_length):\n",
    "                protein_embedded = self.protein_embedding(input_token)\n",
    "                decoder_out, hidden = self.decoder_rnn(protein_embedded, hidden)\n",
    "                output = self.fc(decoder_out)\n",
    "                outputs.append(output)\n",
    "\n",
    "                predicted = torch.argmax(output, dim=-1)\n",
    "\n",
    "                # Early stopping: check if all sequences in batch have generated END token\n",
    "                if torch.all(predicted == end_token_id):\n",
    "                    break\n",
    "\n",
    "                input_token = predicted\n",
    "\n",
    "            return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb9b4e",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1768894587155,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "fbbb9b4e"
   },
   "outputs": [],
   "source": [
    "# MODEL 3: GRU Encoder-Decoder\n",
    "# ============================================================================\n",
    "class DNAProteinTranslatorGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based Encoder-Decoder model for DNA to Protein translation\n",
    "\n",
    "    Architecture:\n",
    "    - Encoder: Bidirectional GRU that processes DNA sequence\n",
    "    - Decoder: GRU that generates protein sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, dna_vocab_size, protein_vocab_size, embedding_dim=64,\n",
    "                 hidden_dim=128, num_layers=2):\n",
    "        super(DNAProteinTranslatorGRU, self).__init__()\n",
    "\n",
    "        self.protein_vocab_size = protein_vocab_size\n",
    "\n",
    "        # Encoder\n",
    "        self.dna_embedding = nn.Embedding(dna_vocab_size, embedding_dim)\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.protein_embedding = nn.Embedding(protein_vocab_size, embedding_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            embedding_dim, hidden_dim * 2, num_layers,  # *2 because encoder is bidirectional\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, protein_vocab_size)\n",
    "\n",
    "    def forward(self, dna_seq, protein_seq=None, max_length=100):\n",
    "        batch_size = dna_seq.size(0)\n",
    "\n",
    "        # Encode DNA\n",
    "        dna_embedded = self.dna_embedding(dna_seq)\n",
    "        encoder_out, hidden = self.encoder_gru(dna_embedded)\n",
    "\n",
    "        # Prepare decoder hidden state\n",
    "        num_layers = hidden.size(0) // 2\n",
    "        hidden_forward = hidden[:num_layers]\n",
    "        hidden_backward = hidden[num_layers:]\n",
    "        hidden = torch.cat([hidden_forward, hidden_backward], dim=2)\n",
    "\n",
    "        if self.training and protein_seq is not None:\n",
    "            # Teacher forcing\n",
    "            protein_embedded = self.protein_embedding(protein_seq)\n",
    "            decoder_out, _ = self.decoder_gru(protein_embedded, hidden)\n",
    "            output = self.fc(decoder_out)\n",
    "            return output\n",
    "        else:\n",
    "            # Inference: generate one token at a time\n",
    "            # START token is at index protein_vocab_size - 2, END token is at protein_vocab_size - 1\n",
    "            start_token_id = self.protein_vocab_size - 2\n",
    "            end_token_id = self.protein_vocab_size - 1\n",
    "\n",
    "            outputs = []\n",
    "            input_token = torch.full((batch_size, 1), start_token_id, dtype=torch.long).to(dna_seq.device)\n",
    "\n",
    "            for step in range(max_length):\n",
    "                protein_embedded = self.protein_embedding(input_token)\n",
    "                decoder_out, hidden = self.decoder_gru(protein_embedded, hidden)\n",
    "                output = self.fc(decoder_out)\n",
    "                outputs.append(output)\n",
    "\n",
    "                predicted = torch.argmax(output, dim=-1)\n",
    "\n",
    "                # Early stopping: check if all sequences in batch have generated END token\n",
    "                if torch.all(predicted == end_token_id):\n",
    "                    break\n",
    "\n",
    "                input_token = predicted\n",
    "\n",
    "            return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b88d0b",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1768894591300,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "c0b88d0b"
   },
   "outputs": [],
   "source": [
    "# MODEL 4: Transformer Encoder-Decoder\n",
    "# ============================================================================\n",
    "class DNAProteinTranslatorTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Encoder-Decoder model for DNA to Protein translation\n",
    "\n",
    "    Architecture:\n",
    "    - Encoder: Transformer Encoder with self-attention\n",
    "    - Decoder: Transformer Decoder with self-attention and cross-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dna_vocab_size, protein_vocab_size, embedding_dim=128,\n",
    "                 num_heads=8, num_layers=3, dim_feedforward=512, max_seq_length=300):\n",
    "        super(DNAProteinTranslatorTransformer, self).__init__()\n",
    "\n",
    "        self.protein_vocab_size = protein_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Encoder\n",
    "        self.dna_embedding = nn.Embedding(dna_vocab_size, embedding_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, max_seq_length, embedding_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Decoder\n",
    "        self.protein_embedding = nn.Embedding(protein_vocab_size, embedding_dim)\n",
    "        self.pos_decoder = nn.Parameter(torch.randn(1, max_seq_length, embedding_dim))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, protein_vocab_size)\n",
    "\n",
    "    def forward(self, dna_seq, protein_seq=None, max_length=100):\n",
    "        batch_size = dna_seq.size(0)\n",
    "        dna_len = dna_seq.size(1)\n",
    "\n",
    "        # Encode DNA\n",
    "        dna_embedded = self.dna_embedding(dna_seq)\n",
    "        dna_embedded = dna_embedded + self.pos_encoder[:, :dna_len, :]\n",
    "        encoder_out = self.encoder(dna_embedded)\n",
    "\n",
    "        if self.training and protein_seq is not None:\n",
    "            # Teacher forcing\n",
    "            protein_len = protein_seq.size(1)\n",
    "            protein_embedded = self.protein_embedding(protein_seq)\n",
    "            protein_embedded = protein_embedded + self.pos_decoder[:, :protein_len, :]\n",
    "\n",
    "            # Create causal mask for decoder (prevents looking at future tokens)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(protein_len).to(dna_seq.device)\n",
    "\n",
    "            decoder_out = self.decoder(\n",
    "                protein_embedded, encoder_out,\n",
    "                tgt_mask=tgt_mask\n",
    "            )\n",
    "            output = self.fc(decoder_out)\n",
    "            return output\n",
    "        else:\n",
    "            # Inference: generate one token at a time\n",
    "            # START token is at index protein_vocab_size - 2, END token is at protein_vocab_size - 1\n",
    "            start_token_id = self.protein_vocab_size - 2\n",
    "            end_token_id = self.protein_vocab_size - 1\n",
    "\n",
    "            outputs = []\n",
    "            input_sequence = torch.full((batch_size, 1), start_token_id, dtype=torch.long).to(dna_seq.device)\n",
    "            all_finished = torch.zeros(batch_size, dtype=torch.bool).to(dna_seq.device)\n",
    "\n",
    "            for step in range(max_length):\n",
    "                # Embed the current sequence\n",
    "                protein_embedded = self.protein_embedding(input_sequence)\n",
    "                # Add positional encoding for the current sequence length\n",
    "                seq_len = input_sequence.size(1)\n",
    "                protein_embedded = protein_embedded + self.pos_decoder[:, :seq_len, :]\n",
    "\n",
    "                # Create causal mask for current sequence length\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(dna_seq.device)\n",
    "\n",
    "                # Decode\n",
    "                decoder_out = self.decoder(\n",
    "                    protein_embedded, encoder_out,\n",
    "                    tgt_mask=tgt_mask\n",
    "                )\n",
    "                output = self.fc(decoder_out)\n",
    "                # Get prediction for the last position\n",
    "                outputs.append(output[:, -1:, :])\n",
    "\n",
    "                # Get predicted token and append to sequence\n",
    "                predicted = torch.argmax(output[:, -1:, :], dim=-1)\n",
    "\n",
    "                # Track which sequences have finished (predicted END token)\n",
    "                all_finished = all_finished | (predicted.squeeze(1) == end_token_id)\n",
    "\n",
    "                # Early stopping: if all sequences in batch have generated END token\n",
    "                if torch.all(all_finished):\n",
    "                    break\n",
    "\n",
    "                input_sequence = torch.cat([input_sequence, predicted], dim=1)\n",
    "\n",
    "            return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca1676",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768894596518,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "c2ca1676",
    "outputId": "edec9e0e-34b5-40a2-9248-8bcea0004bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\n   Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548c314",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1768895607307,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "8548c314",
    "outputId": "e7a59396-b7ac-4309-e6c4-656dd6be0ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " TRAINING MULTIPLE MODELS AND COMPARISON\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" TRAINING MULTIPLE MODELS AND COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def decode_sequence(encoded_seq, vocab):\n",
    "    \"\"\"Decode encoded sequence back to string\"\"\"\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    seq = ''.join([reverse_vocab.get(int(idx), '') for idx in encoded_seq])\n",
    "    seq = seq.replace('<PAD>', '').replace('<UNK>', '').replace('<START>', '').replace('<END>', '')\n",
    "    return seq\n",
    "\n",
    "model_configs = {\n",
    "    'RNN': {\n",
    "        'class': DNAProteinTranslatorRNN,\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 3\n",
    "    },\n",
    "    'LSTM': {\n",
    "        'class': DNAProteinTranslatorLSTM,\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 3\n",
    "    },\n",
    "    'GRU': {\n",
    "        'class': DNAProteinTranslatorGRU,\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 3\n",
    "    },\n",
    "    'Transformer': {\n",
    "        'class': DNAProteinTranslatorTransformer,\n",
    "        'embedding_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 4,\n",
    "        'dim_feedforward': 1024,\n",
    "        'max_seq_length': max_dna_length\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec885b",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1768895611785,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "a0ec885b"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=protein_vocab['<PAD>'])\n",
    "num_epochs = 50\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss doesn't improve.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    patience : int\n",
    "        Number of epochs to wait for improvement before stopping\n",
    "    min_delta : float\n",
    "        Minimum change to qualify as an improvement\n",
    "    restore_best_weights : bool\n",
    "        If True, restore model weights from best epoch\n",
    "    verbose : bool\n",
    "        If True, print messages when patience counter updates\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Check if early stopping should be triggered.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        val_loss : float\n",
    "            Current validation loss\n",
    "        model : nn.Module\n",
    "            Model to save weights from\n",
    "        \"\"\"\n",
    "        score = -val_loss  # Negative because lower loss is better\n",
    "\n",
    "        if self.best_score is None:\n",
    "            # First validation\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'   âš ï¸  EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # Improvement found\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save model weights when improvement is found.\"\"\"\n",
    "        if self.restore_best_weights:\n",
    "            self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        \"\"\"Restore model weights from best epoch.\"\"\"\n",
    "        if self.restore_best_weights and self.best_weights:\n",
    "            model.load_state_dict(self.best_weights)\n",
    "            if self.verbose:\n",
    "                print(f'   âœ… Restored best model weights (val_loss: {-self.best_score:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bc951",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6542674,
     "status": "ok",
     "timestamp": 1768902281765,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "985bc951",
    "outputId": "b5649515-e781-43ae-80f1-c049fed0d853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training RNN Model\n",
      "======================================================================\n",
      "   Total parameters: 2,381,592\n",
      "   Training for up to 50 epochs (with early stopping)...\n",
      "   Epoch [1/50], Train Loss: 2.7796, Val Loss: 2.7118, LR: 0.001000\n",
      "   Epoch [2/50], Train Loss: 2.6951, Val Loss: 2.6727, LR: 0.001000\n",
      "   Epoch [3/50], Train Loss: 2.6688, Val Loss: 2.6595, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [4/50], Train Loss: 2.6587, Val Loss: 2.6639, LR: 0.001000\n",
      "   Epoch [5/50], Train Loss: 2.6560, Val Loss: 2.6491, LR: 0.001000\n",
      "   Epoch [6/50], Train Loss: 2.6463, Val Loss: 2.6344, LR: 0.001000\n",
      "   Epoch [7/50], Train Loss: 2.6331, Val Loss: 2.6330, LR: 0.001000\n",
      "   Epoch [8/50], Train Loss: 2.6260, Val Loss: 2.6226, LR: 0.001000\n",
      "   Epoch [9/50], Train Loss: 2.6190, Val Loss: 2.6126, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [10/50], Train Loss: 2.6281, Val Loss: 2.6169, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [11/50], Train Loss: 2.6152, Val Loss: 2.6176, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [12/50], Train Loss: 2.6156, Val Loss: 2.6122, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [13/50], Train Loss: 2.6128, Val Loss: 2.6199, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 5/10\n",
      "   Epoch [14/50], Train Loss: 2.6132, Val Loss: 2.6165, LR: 0.001000\n",
      "   Epoch [15/50], Train Loss: 2.6134, Val Loss: 2.6111, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [16/50], Train Loss: 2.6086, Val Loss: 2.6114, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [17/50], Train Loss: 2.6079, Val Loss: 2.6158, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [18/50], Train Loss: 2.6097, Val Loss: 2.6253, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [19/50], Train Loss: 2.6122, Val Loss: 2.6183, LR: 0.000500\n",
      "   Epoch [20/50], Train Loss: 2.6008, Val Loss: 2.6008, LR: 0.000500\n",
      "   Epoch [21/50], Train Loss: 2.5986, Val Loss: 2.5998, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [22/50], Train Loss: 2.5971, Val Loss: 2.6001, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [23/50], Train Loss: 2.5944, Val Loss: 2.6016, LR: 0.000500\n",
      "   Epoch [24/50], Train Loss: 2.5919, Val Loss: 2.5980, LR: 0.000500\n",
      "   Epoch [25/50], Train Loss: 2.5902, Val Loss: 2.5938, LR: 0.000500\n",
      "   Epoch [26/50], Train Loss: 2.5854, Val Loss: 2.5918, LR: 0.000500\n",
      "   Epoch [27/50], Train Loss: 2.5828, Val Loss: 2.5886, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [28/50], Train Loss: 2.5812, Val Loss: 2.5890, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [29/50], Train Loss: 2.5779, Val Loss: 2.5877, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [30/50], Train Loss: 2.5764, Val Loss: 2.5896, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [31/50], Train Loss: 2.5718, Val Loss: 2.5892, LR: 0.000500\n",
      "   Epoch [32/50], Train Loss: 2.5690, Val Loss: 2.5827, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [33/50], Train Loss: 2.5657, Val Loss: 2.5833, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [34/50], Train Loss: 2.5634, Val Loss: 2.5820, LR: 0.000500\n",
      "   Epoch [35/50], Train Loss: 2.5627, Val Loss: 2.5807, LR: 0.000500\n",
      "   Epoch [36/50], Train Loss: 2.5601, Val Loss: 2.5788, LR: 0.000500\n",
      "   Epoch [37/50], Train Loss: 2.5587, Val Loss: 2.5758, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [38/50], Train Loss: 2.5555, Val Loss: 2.5770, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [39/50], Train Loss: 2.5542, Val Loss: 2.5760, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [40/50], Train Loss: 2.5521, Val Loss: 2.5760, LR: 0.000500\n",
      "   Epoch [41/50], Train Loss: 2.5509, Val Loss: 2.5704, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [42/50], Train Loss: 2.5499, Val Loss: 2.5722, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [43/50], Train Loss: 2.5478, Val Loss: 2.5815, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [44/50], Train Loss: 2.5510, Val Loss: 2.5824, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [45/50], Train Loss: 2.5524, Val Loss: 2.5712, LR: 0.000250\n",
      "   Epoch [46/50], Train Loss: 2.5414, Val Loss: 2.5694, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [47/50], Train Loss: 2.5353, Val Loss: 2.5706, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [48/50], Train Loss: 2.5313, Val Loss: 2.5743, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [49/50], Train Loss: 2.5281, Val Loss: 2.5746, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [50/50], Train Loss: 2.5276, Val Loss: 2.5781, LR: 0.000125\n",
      "   âœ… Restored best model weights (val_loss: 2.5694)\n",
      "\n",
      "   Evaluating RNN on test set...\n",
      "   âœ… RNN training complete!\n",
      "   Test Loss: 2.5753\n",
      "   Accuracy: 20.16% (45258/224480 tokens)\n",
      "\n",
      "======================================================================\n",
      "Training LSTM Model\n",
      "======================================================================\n",
      "   Total parameters: 9,477,912\n",
      "   Training for up to 50 epochs (with early stopping)...\n",
      "   Epoch [1/50], Train Loss: 2.8021, Val Loss: 2.7319, LR: 0.001000\n",
      "   Epoch [2/50], Train Loss: 2.6545, Val Loss: 2.6372, LR: 0.001000\n",
      "   Epoch [3/50], Train Loss: 2.5474, Val Loss: 2.5762, LR: 0.001000\n",
      "   Epoch [4/50], Train Loss: 2.4478, Val Loss: 2.5111, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [5/50], Train Loss: 2.3731, Val Loss: 2.5238, LR: 0.001000\n",
      "   Epoch [6/50], Train Loss: 2.2986, Val Loss: 2.3965, LR: 0.001000\n",
      "   Epoch [7/50], Train Loss: 2.2157, Val Loss: 2.3655, LR: 0.001000\n",
      "   Epoch [8/50], Train Loss: 1.9290, Val Loss: 2.1633, LR: 0.001000\n",
      "   Epoch [9/50], Train Loss: 1.6015, Val Loss: 2.0236, LR: 0.001000\n",
      "   Epoch [10/50], Train Loss: 1.3838, Val Loss: 1.9479, LR: 0.001000\n",
      "   Epoch [11/50], Train Loss: 1.2419, Val Loss: 1.9208, LR: 0.001000\n",
      "   Epoch [12/50], Train Loss: 1.1370, Val Loss: 1.7527, LR: 0.001000\n",
      "   Epoch [13/50], Train Loss: 1.0200, Val Loss: 1.7312, LR: 0.001000\n",
      "   Epoch [14/50], Train Loss: 0.9576, Val Loss: 1.5984, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [15/50], Train Loss: 0.8605, Val Loss: 1.7248, LR: 0.001000\n",
      "   Epoch [16/50], Train Loss: 0.8009, Val Loss: 1.5174, LR: 0.001000\n",
      "   Epoch [17/50], Train Loss: 0.7553, Val Loss: 1.4635, LR: 0.001000\n",
      "   Epoch [18/50], Train Loss: 0.6988, Val Loss: 1.4128, LR: 0.001000\n",
      "   Epoch [19/50], Train Loss: 0.6301, Val Loss: 1.3185, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [20/50], Train Loss: 0.5905, Val Loss: 1.3210, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [21/50], Train Loss: 0.5774, Val Loss: 1.3803, LR: 0.001000\n",
      "   Epoch [22/50], Train Loss: 0.5399, Val Loss: 1.2840, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [23/50], Train Loss: 0.5228, Val Loss: 1.3605, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [24/50], Train Loss: 0.5187, Val Loss: 1.3026, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [25/50], Train Loss: 0.4685, Val Loss: 1.3320, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [26/50], Train Loss: 0.4401, Val Loss: 1.3084, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 5/10\n",
      "   Epoch [27/50], Train Loss: 0.3266, Val Loss: 1.3124, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 6/10\n",
      "   Epoch [28/50], Train Loss: 0.2651, Val Loss: 1.3778, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 7/10\n",
      "   Epoch [29/50], Train Loss: 0.2398, Val Loss: 1.4563, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 8/10\n",
      "   Epoch [30/50], Train Loss: 0.2869, Val Loss: 1.4480, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 9/10\n",
      "   Epoch [31/50], Train Loss: 0.1738, Val Loss: 1.4951, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 10/10\n",
      "   Epoch [32/50], Train Loss: 0.1519, Val Loss: 1.6327, LR: 0.000250\n",
      "   â›” Early stopping triggered at epoch 32\n",
      "   âœ… Restored best model weights (val_loss: 1.2840)\n",
      "\n",
      "   Evaluating LSTM on test set...\n",
      "   âœ… LSTM training complete!\n",
      "   Test Loss: 1.3033\n",
      "   Accuracy: 64.43% (144626/224480 tokens)\n",
      "\n",
      "======================================================================\n",
      "Training GRU Model\n",
      "======================================================================\n",
      "   Total parameters: 7,112,472\n",
      "   Training for up to 50 epochs (with early stopping)...\n",
      "   Epoch [1/50], Train Loss: 2.7850, Val Loss: 2.7071, LR: 0.001000\n",
      "   Epoch [2/50], Train Loss: 2.3407, Val Loss: 2.4278, LR: 0.001000\n",
      "   Epoch [3/50], Train Loss: 1.7714, Val Loss: 2.4155, LR: 0.001000\n",
      "   Epoch [4/50], Train Loss: 1.4620, Val Loss: 2.2348, LR: 0.001000\n",
      "   Epoch [5/50], Train Loss: 1.2734, Val Loss: 2.1895, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [6/50], Train Loss: 1.1437, Val Loss: 2.2558, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [7/50], Train Loss: 1.0425, Val Loss: 2.3469, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [8/50], Train Loss: 0.9729, Val Loss: 2.2520, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [9/50], Train Loss: 0.9205, Val Loss: 2.2544, LR: 0.000500\n",
      "   Epoch [10/50], Train Loss: 0.7185, Val Loss: 2.1259, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [11/50], Train Loss: 0.6505, Val Loss: 2.1745, LR: 0.000500\n",
      "   Epoch [12/50], Train Loss: 0.6161, Val Loss: 2.0775, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [13/50], Train Loss: 0.5894, Val Loss: 2.1732, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [14/50], Train Loss: 0.5680, Val Loss: 2.2272, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [15/50], Train Loss: 0.5456, Val Loss: 2.2138, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [16/50], Train Loss: 0.5326, Val Loss: 2.2609, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 5/10\n",
      "   Epoch [17/50], Train Loss: 0.4203, Val Loss: 2.1824, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 6/10\n",
      "   Epoch [18/50], Train Loss: 0.3817, Val Loss: 2.2186, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 7/10\n",
      "   Epoch [19/50], Train Loss: 0.3634, Val Loss: 2.3111, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 8/10\n",
      "   Epoch [20/50], Train Loss: 0.3499, Val Loss: 2.4247, LR: 0.000125\n",
      "   âš ï¸  EarlyStopping counter: 9/10\n",
      "   Epoch [21/50], Train Loss: 0.2875, Val Loss: 2.3698, LR: 0.000125\n",
      "   âš ï¸  EarlyStopping counter: 10/10\n",
      "   Epoch [22/50], Train Loss: 0.2615, Val Loss: 2.4692, LR: 0.000125\n",
      "   â›” Early stopping triggered at epoch 22\n",
      "   âœ… Restored best model weights (val_loss: 2.0775)\n",
      "\n",
      "   Evaluating GRU on test set...\n",
      "   âœ… GRU training complete!\n",
      "   Test Loss: 2.0885\n",
      "   Accuracy: 49.30% (110677/224480 tokens)\n",
      "\n",
      "======================================================================\n",
      "Training Transformer Model\n",
      "======================================================================\n",
      "   Total parameters: 7,535,640\n",
      "   Training for up to 50 epochs (with early stopping)...\n",
      "   Epoch [1/50], Train Loss: 2.9197, Val Loss: 2.9052, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 1/10\n",
      "   Epoch [2/50], Train Loss: 2.9037, Val Loss: 2.9583, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 2/10\n",
      "   Epoch [3/50], Train Loss: 2.9039, Val Loss: 2.9341, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 3/10\n",
      "   Epoch [4/50], Train Loss: 2.9039, Val Loss: 2.9559, LR: 0.001000\n",
      "   âš ï¸  EarlyStopping counter: 4/10\n",
      "   Epoch [5/50], Train Loss: 2.8968, Val Loss: 2.9649, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 5/10\n",
      "   Epoch [6/50], Train Loss: 2.8950, Val Loss: 2.9464, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 6/10\n",
      "   Epoch [7/50], Train Loss: 2.8936, Val Loss: 2.9404, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 7/10\n",
      "   Epoch [8/50], Train Loss: 2.8933, Val Loss: 2.9454, LR: 0.000500\n",
      "   âš ï¸  EarlyStopping counter: 8/10\n",
      "   Epoch [9/50], Train Loss: 2.8930, Val Loss: 2.9273, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 9/10\n",
      "   Epoch [10/50], Train Loss: 2.8899, Val Loss: 2.9540, LR: 0.000250\n",
      "   âš ï¸  EarlyStopping counter: 10/10\n",
      "   Epoch [11/50], Train Loss: 2.8892, Val Loss: 2.9464, LR: 0.000250\n",
      "   â›” Early stopping triggered at epoch 11\n",
      "   âœ… Restored best model weights (val_loss: 2.9052)\n",
      "\n",
      "   Evaluating Transformer on test set...\n",
      "   âœ… Transformer training complete!\n",
      "   Test Loss: 2.9073\n",
      "   Accuracy: 9.65% (21655/224480 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Train each model\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name} Model\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Initialize model\n",
    "    if model_name == 'Transformer':\n",
    "        model = config['class'](\n",
    "            dna_vocab_size=len(dna_vocab),\n",
    "            protein_vocab_size=len(protein_vocab),\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dim_feedforward=config['dim_feedforward'],\n",
    "            max_seq_length=config['max_seq_length']\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = config['class'](\n",
    "            dna_vocab_size=len(dna_vocab),\n",
    "            protein_vocab_size=len(protein_vocab),\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_dim=config['hidden_dim'],\n",
    "            num_layers=config['num_layers']\n",
    "        ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001, verbose=True)\n",
    "\n",
    "    print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   Training for up to {num_epochs} epochs (with early stopping)...\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for dna_seq, protein_seq in train_loader:\n",
    "            dna_seq = dna_seq.to(device)\n",
    "            protein_seq = protein_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(dna_seq, protein_seq)\n",
    "\n",
    "            # Shift target by 1 since output[i] predicts token at position i+1\n",
    "            seq_len = min(output.size(1), protein_seq.size(1))\n",
    "            output_shifted = output[:, :seq_len-1, :].contiguous()\n",
    "            target_shifted = protein_seq[:, 1:seq_len].contiguous()\n",
    "\n",
    "            output_flat = output_shifted.view(-1, output_shifted.size(-1))\n",
    "            target_flat = target_shifted.view(-1)\n",
    "\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for dna_seq, protein_seq in val_loader:\n",
    "                dna_seq = dna_seq.to(device)\n",
    "                protein_seq = protein_seq.to(device)\n",
    "\n",
    "                output = model(dna_seq, protein_seq)\n",
    "\n",
    "                seq_len = min(output.size(1), protein_seq.size(1))\n",
    "                output_shifted = output[:, :seq_len-1, :].contiguous()\n",
    "                target_shifted = protein_seq[:, 1:seq_len].contiguous()\n",
    "\n",
    "                output_flat = output_shifted.view(-1, output_shifted.size(-1))\n",
    "                target_flat = target_shifted.view(-1)\n",
    "\n",
    "                loss = criterion(output_flat, target_flat)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        early_stopping(avg_val_loss, model)\n",
    "\n",
    "        print(f'   Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'   â›” Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    early_stopping.load_best_weights(model)\n",
    "\n",
    "    print(f\"\\n   Evaluating {model_name} on test set...\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dna_seq, protein_seq in test_loader:\n",
    "            dna_seq = dna_seq.to(device)\n",
    "            protein_seq = protein_seq.to(device)\n",
    "\n",
    "            output = model(dna_seq, max_length=max_protein_length_with_tokens)\n",
    "\n",
    "            target_shifted = protein_seq[:, 1:].contiguous()\n",
    "\n",
    "            min_len = min(output.size(1), target_shifted.size(1))\n",
    "            output = output[:, :min_len, :]\n",
    "            target_shifted = target_shifted[:, :min_len]\n",
    "\n",
    "            output_flat = output.contiguous().view(-1, output.size(-1))\n",
    "            target_flat = target_shifted.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred_tokens = torch.argmax(output_flat, dim=-1)\n",
    "            mask = (target_flat != protein_vocab['<PAD>']) & (target_flat != protein_vocab['<START>'])\n",
    "            correct_predictions += (pred_tokens[mask] == target_flat[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    models[model_name] = model\n",
    "    results[model_name] = {\n",
    "        'test_loss': avg_test_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct_predictions,\n",
    "        'total': total_tokens\n",
    "    }\n",
    "\n",
    "    print(f\"   âœ… {model_name} training complete!\")\n",
    "    print(f\"   Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"   Accuracy: {accuracy*100:.2f}% ({correct_predictions}/{total_tokens} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10959f3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1768902516487,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "10959f3d",
    "outputId": "eb5719cb-38f1-4c77-f73c-24aba0b85317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON RESULTS\n",
      "======================================================================\n",
      "\n",
      "Model           Test Loss       Accuracy        Correct/Total       \n",
      "-----------------------------------------------------------------\n",
      "RNN             2.5753          20.16          % 45258/224480         \n",
      "LSTM            1.3033          64.43          % 144626/224480         \n",
      "GRU             2.0885          49.30          % 110677/224480         \n",
      "Transformer     2.9073          9.65           % 21655/224480         \n"
     ]
    }
   ],
   "source": [
    "# Compare models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Model':<15} {'Test Loss':<15} {'Accuracy':<15} {'Correct/Total':<20}\")\n",
    "print(\"-\" * 65)\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name:<15} {result['test_loss']:<15.4f} {result['accuracy']*100:<15.2f}% {result['correct']}/{result['total']:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1118587f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1768902557711,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "1118587f",
    "outputId": "ab494b5d-4d5a-4555-ebec-e2ddbb1cbdd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† Best Model: LSTM\n",
      "   Test Loss: 1.3033\n",
      "   Accuracy: 64.43%\n"
     ]
    }
   ],
   "source": [
    "# Find best model\n",
    "best_model_name = min(results.keys(), key=lambda x: results[x]['test_loss'])\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Test Loss: {results[best_model_name]['test_loss']:.4f}\")\n",
    "print(f\"   Accuracy: {results[best_model_name]['accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1e3d14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1768902573213,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "8f1e3d14",
    "outputId": "d0a3df81-a94d-4273-bbbe-32dd7a7df595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Example predictions from LSTM:\n",
      "\n",
      "   Example 1:\n",
      "   True:  INRAASSFRSQYDGTAGSDDCRELSRSGFSIAIWIRLVYDLLKQRFRRPADALITQKVYY...\n",
      "   Pred:  INRASSAFRSQYDGTASDANCRELSRGSGSIIAWIFLMDVLLKQRFREPANALITQKVYT...\n",
      "\n",
      "   Example 2:\n",
      "   True:  LCFPILEHPAPQVASIGATLSPEERLTACTPILFHARLNAI...\n",
      "   Pred:  LCFPILEHPAPQVASIGATLSSERLETACTPILFHARANLRN...\n",
      "\n",
      "   Example 3:\n",
      "   True:  GQFRWLAEGLQSIRKYCRLTTALISKTMTLKPYTLLRSVRRWHPHYAIGNGRHRLCTQE...\n",
      "   Pred:  GQFGWLAELLQSIRKRYRTLTAILSKTMTKLPYTLLRSVRWPHRYRAIGNGRHWLCTQEQ...\n",
      "\n",
      "   âœ… All models trained and compared!\n"
     ]
    }
   ],
   "source": [
    "# Show example predictions from best model\n",
    "print(f\"\\nðŸ“‹ Example predictions from {best_model_name}:\")\n",
    "best_model = models[best_model_name]\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_indices = [0, 1, 2]\n",
    "    for idx in sample_indices:\n",
    "        test_dna = X_test[idx:idx+1]\n",
    "        test_protein = y_test[idx:idx+1]\n",
    "        test_dna_tensor = torch.LongTensor(test_dna).to(device)\n",
    "        predictions = best_model(test_dna_tensor, max_length=max_protein_length_with_tokens)\n",
    "        pred_tokens = torch.argmax(predictions, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        true_protein = decode_sequence(test_protein[0], protein_vocab)\n",
    "        pred_protein = decode_sequence(pred_tokens, protein_vocab)\n",
    "\n",
    "        print(f\"\\n   Example {idx + 1}:\")\n",
    "        print(f\"   True:  {true_protein[:60]}...\")\n",
    "        print(f\"   Pred:  {pred_protein[:60]}...\")\n",
    "\n",
    "print(\"\\n   âœ… All models trained and compared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6fbe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1768902595063,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "abb6fbe2",
    "outputId": "9dfc7a22-ff5f-4197-d362-d82b0677d990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: SAVING BEST MODEL\n",
      "======================================================================\n",
      "   âœ… Best model (LSTM) saved to: model/dna_protein_translator_lstm.pth\n",
      "   All model results saved in checkpoint\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: SAVING BEST MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "model_path = f'model/dna_protein_translator_{best_model_name.lower()}.pth'\n",
    "if best_model_name == 'Transformer':\n",
    "    model_config = {\n",
    "        'model_type': 'Transformer',\n",
    "        'embedding_dim': model_configs[best_model_name]['embedding_dim'],\n",
    "        'num_heads': model_configs[best_model_name]['num_heads'],\n",
    "        'num_layers': model_configs[best_model_name]['num_layers'],\n",
    "        'dim_feedforward': model_configs[best_model_name]['dim_feedforward'],\n",
    "        'max_seq_length': model_configs[best_model_name]['max_seq_length']\n",
    "    }\n",
    "else:\n",
    "    model_config = {\n",
    "        'model_type': best_model_name,\n",
    "        'embedding_dim': model_configs[best_model_name]['embedding_dim'],\n",
    "        'hidden_dim': model_configs[best_model_name]['hidden_dim'],\n",
    "        'num_layers': model_configs[best_model_name]['num_layers']\n",
    "    }\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'model_type': best_model_name,\n",
    "    'dna_vocab': dna_vocab,\n",
    "    'protein_vocab': protein_vocab,\n",
    "    'max_dna_length': max_dna_length,\n",
    "    'max_protein_length': max_protein_length,\n",
    "    'max_protein_length_with_tokens': max_protein_length_with_tokens,\n",
    "    'model_config': model_config,\n",
    "    'results': results\n",
    "}, model_path)\n",
    "\n",
    "print(f\"   âœ… Best model ({best_model_name}) saved to: {model_path}\")\n",
    "print(f\"   All model results saved in checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4aa27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1768902613353,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "fcf4aa27",
    "outputId": "c1c2159e-0e18-4adf-9915-06c51ab7f6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: TESTING INFERENCE\n",
      "======================================================================\n",
      "   âœ… Loaded LSTM model from checkpoint\n",
      "\n",
      "   Testing inference on sample sequences:\n",
      "\n",
      "   Example 1:\n",
      "   DNA:     ACTATTGATTGCGCAGGAGTCACTATTGAGGGCAACGAACAATGGGCTCTAATCACCGTC...\n",
      "   True:    TIDCAGVTIEGNEQWALITVNNAGIHLICASVH...\n",
      "   Pred:    TIDCAGVTIDEKGQWEAILVNTAGIHLICASVH...\n",
      "\n",
      "   Example 2:\n",
      "   DNA:     TCGGTAAATCCGGAGTGTTCCGGCTGTACAGCGAGCACCTTGTCGCCCATTTACTTAAAC...\n",
      "   True:    SVNPECSGCTASTLSPIYLNTYSTGPRGLCQIKMRGISQWAIPYGRSTQD...\n",
      "   Pred:    SVNPECSGCATSSLTPYNYYTLSGIRGLCQIKIIRGSLQMAIGGISTPQD...\n",
      "\n",
      "   Example 3:\n",
      "   DNA:     CAGGTAAGGCACATCATTTACACCACCTTATGGTGC...\n",
      "   True:    QVRHIIYTTLWC...\n",
      "   Pred:    QVRHIIYTHLWC...\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def predict_protein(dna_sequence, model, dna_vocab, protein_vocab,\n",
    "                    max_dna_length, max_protein_length_with_tokens, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict protein sequence from DNA sequence\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dna_sequence : str\n",
    "        DNA sequence (A, T, G, C)\n",
    "    model : DNAProteinTranslator\n",
    "        Trained model\n",
    "    dna_vocab : dict\n",
    "        DNA vocabulary\n",
    "    protein_vocab : dict\n",
    "        Protein vocabulary\n",
    "    max_dna_length : int\n",
    "        Maximum DNA length\n",
    "    max_protein_length_with_tokens : int\n",
    "        Maximum protein length (including START/END tokens)\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    protein_sequence : str\n",
    "        Predicted protein sequence (without START/END tokens)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    dna_encoded = [dna_vocab.get(char, dna_vocab['<UNK>']) for char in dna_sequence.upper()]\n",
    "    if len(dna_encoded) < max_dna_length:\n",
    "        dna_encoded += [dna_vocab['<PAD>']] * (max_dna_length - len(dna_encoded))\n",
    "    else:\n",
    "        dna_encoded = dna_encoded[:max_dna_length]\n",
    "\n",
    "    dna_tensor = torch.LongTensor([dna_encoded]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(dna_tensor, max_length=max_protein_length_with_tokens)\n",
    "        pred_tokens = torch.argmax(output, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    reverse_protein_vocab = {v: k for k, v in protein_vocab.items()}\n",
    "    protein_sequence = ''.join([reverse_protein_vocab.get(int(idx), '')\n",
    "                                for idx in pred_tokens])\n",
    "\n",
    "    protein_sequence = protein_sequence.replace('<PAD>', '').replace('<UNK>', '')\n",
    "    protein_sequence = protein_sequence.replace('<START>', '').replace('<END>', '')\n",
    "\n",
    "    return protein_sequence\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: TESTING INFERENCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "try:\n",
    "    checkpoint_max_dna_length = checkpoint.get('max_dna_length', max_dna_length)\n",
    "    checkpoint_max_protein_length_with_tokens = checkpoint.get('max_protein_length_with_tokens', max_protein_length_with_tokens)\n",
    "    checkpoint_dna_vocab = checkpoint.get('dna_vocab', dna_vocab)\n",
    "    checkpoint_protein_vocab = checkpoint.get('protein_vocab', protein_vocab)\n",
    "except NameError:\n",
    "    checkpoint_max_dna_length = checkpoint['max_dna_length']\n",
    "    checkpoint_max_protein_length_with_tokens = checkpoint['max_protein_length_with_tokens']\n",
    "    checkpoint_dna_vocab = checkpoint['dna_vocab']\n",
    "    checkpoint_protein_vocab = checkpoint['protein_vocab']\n",
    "model_type = checkpoint['model_type']\n",
    "model_config = checkpoint['model_config']\n",
    "\n",
    "if model_type == 'Transformer':\n",
    "    inference_model = DNAProteinTranslatorTransformer(\n",
    "        dna_vocab_size=len(checkpoint_dna_vocab),\n",
    "        protein_vocab_size=len(checkpoint_protein_vocab),\n",
    "        embedding_dim=model_config['embedding_dim'],\n",
    "        num_heads=model_config['num_heads'],\n",
    "        num_layers=model_config['num_layers'],\n",
    "        dim_feedforward=model_config['dim_feedforward'],\n",
    "        max_seq_length=model_config['max_seq_length']\n",
    "    ).to(device)\n",
    "elif model_type == 'LSTM':\n",
    "    inference_model = DNAProteinTranslatorLSTM(\n",
    "        dna_vocab_size=len(checkpoint_dna_vocab),\n",
    "        protein_vocab_size=len(checkpoint_protein_vocab),\n",
    "        embedding_dim=model_config['embedding_dim'],\n",
    "        hidden_dim=model_config['hidden_dim'],\n",
    "        num_layers=model_config['num_layers']\n",
    "    ).to(device)\n",
    "elif model_type == 'RNN':\n",
    "    inference_model = DNAProteinTranslatorRNN(\n",
    "        dna_vocab_size=len(checkpoint_dna_vocab),\n",
    "        protein_vocab_size=len(checkpoint_protein_vocab),\n",
    "        embedding_dim=model_config['embedding_dim'],\n",
    "        hidden_dim=model_config['hidden_dim'],\n",
    "        num_layers=model_config['num_layers']\n",
    "    ).to(device)\n",
    "elif model_type == 'GRU':\n",
    "    inference_model = DNAProteinTranslatorGRU(\n",
    "        dna_vocab_size=len(checkpoint_dna_vocab),\n",
    "        protein_vocab_size=len(checkpoint_protein_vocab),\n",
    "        embedding_dim=model_config['embedding_dim'],\n",
    "        hidden_dim=model_config['hidden_dim'],\n",
    "        num_layers=model_config['num_layers']\n",
    "    ).to(device)\n",
    "\n",
    "inference_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "inference_model.eval()\n",
    "\n",
    "print(f\"   âœ… Loaded {model_type} model from checkpoint\")\n",
    "\n",
    "test_indices = [0, 1, 2]\n",
    "print(\"\\n   Testing inference on sample sequences:\")\n",
    "for idx in test_indices:\n",
    "    test_dna = df.iloc[idx]['dna_sequence']\n",
    "    true_protein = df.iloc[idx]['protein_sequence']\n",
    "    pred_protein = predict_protein(\n",
    "        test_dna, inference_model, checkpoint_dna_vocab, checkpoint_protein_vocab,\n",
    "        checkpoint_max_dna_length, checkpoint_max_protein_length_with_tokens, device\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   Example {idx + 1}:\")\n",
    "    print(f\"   DNA:     {test_dna[:60]}...\")\n",
    "    print(f\"   True:    {true_protein[:50]}...\")\n",
    "    print(f\"   Pred:    {pred_protein[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0360b7ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1768902623168,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "0360b7ee",
    "outputId": "afe1ef78-1a51-4270-dda2-a1239c84cdc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… PROJECT 5 COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "   Model saved and ready for use!\n",
      "   Model path: model/dna_protein_translator_lstm.pth\n",
      "\n",
      "   To use the model for new predictions:\n",
      "   1. Load the checkpoint\n",
      "   2. Use predict_protein() function with your DNA sequence\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… PROJECT 5 COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n   Model saved and ready for use!\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(\"\\n   To use the model for new predictions:\")\n",
    "print(\"   1. Load the checkpoint\")\n",
    "print(\"   2. Use predict_protein() function with your DNA sequence\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
