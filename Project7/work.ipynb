{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922ff31",
   "metadata": {
    "executionInfo": {
     "elapsed": 22853,
     "status": "ok",
     "timestamp": 1769411397772,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "0922ff31"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# All models used here are public and free from Hugging Face Hub - no API keys needed.\n",
    "# First-time downloads may take a few minutes depending on connection speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the notebook\n",
    "with open('work.ipynb', 'r', encoding='utf-8') as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "# Remove the widgets metadata if it exists\n",
    "if 'metadata' in notebook and 'widgets' in notebook['metadata']:\n",
    "    del notebook['metadata']['widgets']\n",
    "    print(\"Removed widgets metadata\")\n",
    "\n",
    "# Write the cleaned notebook\n",
    "with open('work.ipynb', 'w', encoding='utf-8') as f:\n",
    "    json.dump(notebook, f, indent=1, ensure_ascii=False)\n",
    "\n",
    "print(\"Notebook cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8dc119",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1769411397784,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "7c8dc119"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class MedicalTextDataset:\n",
    "    \"\"\"\n",
    "    Container for medical text dataset with normalized label mappings.\n",
    "    \n",
    "    I use this to keep train/val/test splits together with their label mappings,\n",
    "    which makes it easier to pass everything to the training functions.\n",
    "    \"\"\"\n",
    "    train_df: pd.DataFrame\n",
    "    val_df: pd.DataFrame\n",
    "    test_df: pd.DataFrame\n",
    "    label_to_id: Dict[int, int]\n",
    "    id_to_label: Dict[int, int]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec294ada",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1769411397799,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "ec294ada"
   },
   "outputs": [],
   "source": [
    "def load_dat_file(file_path: str, has_labels: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load .dat files with tab-separated format. Handles both labeled and unlabeled data.\n",
    "    \n",
    "    I split only on the first tab since medical text can contain tabs internally.\n",
    "    This ensures the label (if present) is correctly separated from the text.\n",
    "    \"\"\"\n",
    "    texts: List[str] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if has_labels:\n",
    "                parts = line.split(\"\\t\", 1)\n",
    "                if len(parts) != 2:\n",
    "                    print(f\"Warning: Skipping malformed line in {file_path}: {line[:120]}...\")\n",
    "                    continue\n",
    "\n",
    "                label_str, text = parts\n",
    "                try:\n",
    "                    label_int = int(label_str)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Skipping line with non-integer label in {file_path}: {line[:120]}...\")\n",
    "                    continue\n",
    "\n",
    "                labels.append(label_int)\n",
    "                texts.append(text.strip())\n",
    "            else:\n",
    "                texts.append(line)\n",
    "                labels.append(-1)\n",
    "\n",
    "    if has_labels:\n",
    "        df = pd.DataFrame({\"text\": texts, \"raw_label\": labels})\n",
    "    else:\n",
    "        df = pd.DataFrame({\"text\": texts, \"raw_label\": None})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebf637",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1769411397826,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "ffebf637"
   },
   "outputs": [],
   "source": [
    "def load_medical_text_dataset(\n",
    "    base_directory: str = \".\",\n",
    "    data_subdirectory: str = \"data\",\n",
    ") -> MedicalTextDataset:\n",
    "    \"\"\"\n",
    "    Load and prepare the medical text dataset. Normalizes labels to 0..N-1 for model training,\n",
    "    splits training data into train/val sets, and saves processed data for reproducibility.\n",
    "    \"\"\"\n",
    "    train_file_path = os.path.join(base_directory, data_subdirectory, \"train.dat\")\n",
    "    test_file_path = os.path.join(base_directory, data_subdirectory, \"test.dat\")\n",
    "\n",
    "    assert os.path.exists(train_file_path), f\"Error: train.dat not found at {train_file_path}\"\n",
    "    assert os.path.exists(test_file_path), f\"Error: test.dat not found at {test_file_path}\"\n",
    "\n",
    "    print(f\"Loading training data from: {train_file_path}\")\n",
    "    train_df = load_dat_file(train_file_path, has_labels=True)\n",
    "    print(f\"  ✓ Loaded {len(train_df)} training examples\")\n",
    "\n",
    "    print(f\"Loading test data from: {test_file_path}\")\n",
    "    # Auto-detect if test file has labels by checking the first line\n",
    "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_line = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                first_line = line\n",
    "                break\n",
    "\n",
    "    test_has_labels = False\n",
    "    if first_line:\n",
    "        parts = first_line.split(\"\\t\", 1)\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                int(parts[0])\n",
    "                test_has_labels = True\n",
    "            except ValueError:\n",
    "                test_has_labels = False\n",
    "\n",
    "    test_df = load_dat_file(test_file_path, has_labels=test_has_labels)\n",
    "    print(f\"  ✓ Loaded {len(test_df)} test examples\")\n",
    "    if not test_has_labels:\n",
    "        print(f\"  ⚠ Note: Test file has no labels (unlabeled data for prediction)\")\n",
    "\n",
    "    # Build label mapping from training data only to ensure consistency\n",
    "    train_labels = train_df[\"raw_label\"].dropna().unique()\n",
    "    unique_labels = sorted([int(l) for l in train_labels if pd.notna(l)])\n",
    "    print(f\"\\nUnique raw labels found in training data: {unique_labels}\")\n",
    "    print(f\"Total number of classes: {len(unique_labels)}\")\n",
    "\n",
    "    # Normalize labels to 0..N-1 for model compatibility\n",
    "    # Using Python int to avoid JSON serialization issues with numpy types\n",
    "    label_to_id = {int(raw): int(idx) for idx, raw in enumerate(unique_labels)}\n",
    "    id_to_label = {int(idx): int(raw) for raw, idx in label_to_id.items()}\n",
    "\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "\n",
    "    train_df[\"label\"] = train_df[\"raw_label\"].map(label_to_id)\n",
    "\n",
    "    if test_df[\"raw_label\"].notna().any():\n",
    "        test_df[\"label\"] = test_df[\"raw_label\"].map(label_to_id)\n",
    "    else:\n",
    "        test_df[\"label\"] = None\n",
    "\n",
    "    print(\"\\nLabel mapping (raw_label -> normalized_id):\")\n",
    "    for raw, idx in label_to_id.items():\n",
    "        print(f\"  {raw} -> {idx}\")\n",
    "\n",
    "    # Stratified split ensures class distribution is maintained in both sets\n",
    "    print(f\"\\nSplitting training data into train/validation sets (80/20)...\")\n",
    "    train_split_df = train_df[[\"text\", \"label\"]].copy()\n",
    "    train_split_df = train_split_df.reset_index(drop=True)\n",
    "\n",
    "    train_final_df, val_df = train_test_split(\n",
    "        train_split_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=train_split_df[\"label\"]\n",
    "    )\n",
    "\n",
    "    train_final_df = train_final_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"  ✓ Training set: {len(train_final_df)} examples\")\n",
    "    print(f\"  ✓ Validation set: {len(val_df)} examples\")\n",
    "\n",
    "    if \"label\" in test_df.columns:\n",
    "        final_test_df = test_df[[\"text\", \"label\"]].copy()\n",
    "    else:\n",
    "        final_test_df = test_df[[\"text\"]].copy()\n",
    "        final_test_df[\"label\"] = None\n",
    "\n",
    "    final_test_df = final_test_df.reset_index(drop=True)\n",
    "\n",
    "    # Save processed data for reproducibility and later use\n",
    "    processed_data_dir = os.path.join(base_directory, \"processed_data\")\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "    train_save_path = os.path.join(processed_data_dir, \"train_processed.csv\")\n",
    "    val_save_path = os.path.join(processed_data_dir, \"val_processed.csv\")\n",
    "    test_save_path = os.path.join(processed_data_dir, \"test_processed.csv\")\n",
    "    label_mapping_path = os.path.join(processed_data_dir, \"label_mapping.json\")\n",
    "\n",
    "    print(f\"\\nSaving processed datasets...\")\n",
    "    train_final_df.to_csv(train_save_path, index=False, encoding=\"utf-8\")\n",
    "    val_df.to_csv(val_save_path, index=False, encoding=\"utf-8\")\n",
    "    final_test_df.to_csv(test_save_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"  ✓ Saved training data: {train_save_path} ({len(train_final_df)} examples)\")\n",
    "    print(f\"  ✓ Saved validation data: {val_save_path} ({len(val_df)} examples)\")\n",
    "    print(f\"  ✓ Saved test data: {test_save_path} ({len(final_test_df)} examples)\")\n",
    "\n",
    "    mapping_data = {\n",
    "        \"label_to_id\": {str(k): int(v) for k, v in label_to_id.items()},\n",
    "        \"id_to_label\": {str(k): int(v) for k, v in id_to_label.items()},\n",
    "        \"num_classes\": int(len(label_to_id))\n",
    "    }\n",
    "    with open(label_mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping_data, f, indent=2)\n",
    "    print(f\"  ✓ Saved label mappings: {label_mapping_path}\")\n",
    "\n",
    "    return MedicalTextDataset(\n",
    "        train_df=train_final_df,\n",
    "        val_df=val_df,\n",
    "        test_df=final_test_df,\n",
    "        label_to_id=label_to_id,\n",
    "        id_to_label=id_to_label,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67f18f",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1769411397839,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "af67f18f"
   },
   "outputs": [],
   "source": [
    "def create_tokenize_function(tokenizer, max_length: int):\n",
    "    \"\"\"\n",
    "    Returns a tokenization function compatible with Hugging Face Dataset.map().\n",
    "    I use max_length padding to ensure all sequences have the same length for batching.\n",
    "    \"\"\"\n",
    "    def _tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    return _tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832ee29",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1769411397906,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "1832ee29"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy from model predictions. Used by the Trainer during evaluation.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55853783",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1769411397929,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "55853783"
   },
   "outputs": [],
   "source": [
    "def predict_on_test_data(\n",
    "    model_path: str,\n",
    "    test_df: pd.DataFrame,\n",
    "    label_mapping: Dict[int, int],\n",
    "    max_length: int = 256,\n",
    "    original_model_name: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a trained model and generate predictions on unlabeled test data.\n",
    "    Handles both direct model paths and checkpoint subdirectories.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Making predictions on test data using model: {model_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"Loading trained model...\")\n",
    "\n",
    "    # Check for model files in base directory or checkpoint subdirectories\n",
    "    model_loaded = False\n",
    "    actual_model_path = model_path\n",
    "\n",
    "    if os.path.exists(os.path.join(model_path, \"pytorch_model.bin\")) or \\\n",
    "       os.path.exists(os.path.join(model_path, \"model.safetensors\")):\n",
    "        actual_model_path = model_path\n",
    "        model_loaded = True\n",
    "    else:\n",
    "        checkpoint_dirs = [d for d in os.listdir(model_path)\n",
    "                           if os.path.isdir(os.path.join(model_path, d)) and d.startswith(\"checkpoint-\")]\n",
    "        if checkpoint_dirs:\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[1]) if x.split(\"-\")[1].isdigit() else 0, reverse=True)\n",
    "            actual_model_path = os.path.join(model_path, checkpoint_dirs[0])\n",
    "            print(f\"  ⚠ Model not in base directory, found in: {checkpoint_dirs[0]}\")\n",
    "            model_loaded = True\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(actual_model_path)\n",
    "        print(\"  ✓ Tokenizer loaded\")\n",
    "    except Exception as e:\n",
    "        if original_model_name:\n",
    "            print(f\"  ⚠ Tokenizer not found, loading from original: {original_model_name}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Could not load tokenizer. Error: {e}\")\n",
    "\n",
    "    if not model_loaded:\n",
    "        raise OSError(f\"Model files not found in {model_path} or any checkpoint subdirectories\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(actual_model_path)\n",
    "    print(\"  ✓ Model loaded\")\n",
    "\n",
    "    test_hf = HFDataset.from_pandas(test_df[[\"text\"]])\n",
    "\n",
    "    print(f\"Tokenizing test data (max_length={max_length})...\")\n",
    "    tokenize_fn = create_tokenize_function(tokenizer, max_length)\n",
    "    test_hf = test_hf.map(tokenize_fn, batched=True)\n",
    "    test_hf = test_hf.remove_columns([\"text\"])\n",
    "    test_hf.set_format(\"torch\")\n",
    "    print(\"  ✓ Tokenization complete\")\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    trainer = Trainer(model=model)\n",
    "    predictions = trainer.predict(test_hf)\n",
    "    predicted_ids = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "    # Map back to original label space\n",
    "    id_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    predicted_labels = [id_to_label.get(int(pid), None) for pid in predicted_ids]\n",
    "\n",
    "    results_df = test_df.copy()\n",
    "    results_df[\"predicted_label_id\"] = predicted_ids\n",
    "    results_df[\"predicted_label\"] = predicted_labels\n",
    "\n",
    "    print(f\"  ✓ Predictions complete: {len(results_df)} examples\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5c7a1",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1769411397966,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "11a5c7a1"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model_name: str,\n",
    "    dataset: MedicalTextDataset,\n",
    "    output_directory: str,\n",
    "    num_epochs: int = 5,\n",
    "    batch_size: int = 8,\n",
    "    max_length: int = 256,\n",
    "    early_stopping_patience: int = 3,\n",
    ") -> Tuple[float, Dict]:\n",
    "    \"\"\"\n",
    "    Fine-tune a pretrained transformer model on the medical text dataset.\n",
    "    Uses early stopping and saves the best model based on validation accuracy.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"Loading tokenizer and model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(dataset.label_to_id)\n",
    "    )\n",
    "    print(f\"  ✓ Model loaded with {len(dataset.label_to_id)} output classes\")\n",
    "\n",
    "    print(\"\\nConverting DataFrames to Hugging Face Datasets...\")\n",
    "    train_hf = HFDataset.from_pandas(dataset.train_df)\n",
    "    val_hf = HFDataset.from_pandas(dataset.val_df)\n",
    "    print(f\"  ✓ Training dataset: {len(train_hf)} examples\")\n",
    "    print(f\"  ✓ Validation dataset: {len(val_hf)} examples\")\n",
    "\n",
    "    print(f\"\\nTokenizing datasets (max_length={max_length})...\")\n",
    "    tokenize_fn = create_tokenize_function(tokenizer, max_length)\n",
    "    train_hf = train_hf.map(tokenize_fn, batched=True)\n",
    "    val_hf = val_hf.map(tokenize_fn, batched=True)\n",
    "    print(\"  ✓ Tokenization complete\")\n",
    "\n",
    "    train_hf = train_hf.remove_columns([\"text\"])\n",
    "    val_hf = val_hf.remove_columns([\"text\"])\n",
    "    train_hf.set_format(\"torch\")\n",
    "    val_hf.set_format(\"torch\")\n",
    "\n",
    "    print(f\"\\nConfiguring training arguments...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_directory,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "        logging_dir=os.path.join(output_directory, \"logs\"),\n",
    "        report_to=[],\n",
    "    )\n",
    "    print(\"  ✓ Training arguments configured\")\n",
    "\n",
    "    print(\"\\nInitializing Trainer (with early stopping)...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_hf,\n",
    "        eval_dataset=val_hf,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    "    )\n",
    "    print(\"  ✓ Trainer initialized\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 80)\n",
    "    trainer.train()\n",
    "    print(\"  ✓ Training complete\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Evaluating model...\")\n",
    "    print(\"=\" * 80)\n",
    "    eval_results = trainer.evaluate()\n",
    "    accuracy = float(eval_results.get(\"eval_accuracy\", 0.0))\n",
    "    print(f\"\\nEvaluation results for {model_name}:\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Generating detailed classification report on validation set...\")\n",
    "    print(\"=\" * 80)\n",
    "    preds_output = trainer.predict(val_hf)\n",
    "    preds = np.argmax(preds_output.predictions, axis=-1)\n",
    "    true_labels = preds_output.label_ids\n",
    "    print(\"\\nClassification report (per-class metrics):\")\n",
    "    print(classification_report(true_labels, preds, digits=4))\n",
    "\n",
    "    print(f\"\\nSaving tokenizer to {output_directory}...\")\n",
    "    tokenizer.save_pretrained(output_directory)\n",
    "    print(\"  ✓ Tokenizer saved\")\n",
    "\n",
    "    return accuracy, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180125a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "971ca2065d8c43bab690f36286369711",
      "09d6a6c3a3694a4cb7d1a6375729f296",
      "e18d5b80e4634606a5b3347683b8189c",
      "d9059994371648fe90df8f2259331ffe",
      "e357e982052f41dfaaad680a1d3a1f8d",
      "1729f650e746493396940bb56edb02ac",
      "3e9850f2087944a38c4642cb6c113e10",
      "3f009c9c1b6543c3ae6036674afef3de",
      "3b91e1d631fa487e87709b09dae702a6",
      "2dac8cf3b44d4e9dac5f72b2728edf79",
      "69bd7bc839fa4da182fee2748e394c07",
      "ab515b706bff40999ac9e6cfd99aef34",
      "ced7c5f6eca74f8d9a9f2df616e68109",
      "8c83a8b9bfc946fab58c8bc15bea53f3",
      "808a0c7b437244a2bd048ce20bc0659a",
      "ab9a179781f24087bceaac7c5f66c426",
      "afd299cb45bd4436bfc6e9d66b92a3e9",
      "5c13058f60f248ce8a9a15718606e803",
      "ac3805985a3249e59bbbe118021e2f71",
      "d3459a22fa6c4b4a9884f8bfa1fb5eaa",
      "6ab468531f1040ce876e1941f090f6e2",
      "8de08b34c9b7427189c08e52b089997e",
      "f93d18394caf4b608d5e9d31120a4e5b",
      "b422155500574da3add9173698c978d4",
      "555f963aa7a1464aa8c99a49286dbd11",
      "7ec10ceebede4da4a438778f0c37ee3d",
      "ab61c08fe9b94cbb9f8c2de55dbbfa4e",
      "f79a13f110244410a01a627801c91ede",
      "8625e68cc5e84dcbab2f29497f6c82a8",
      "111920cfe8f64a3e8d8c6f67bbfe6a70",
      "fde450fceb3c468ba3073ab1fbbc0bb1",
      "9c1dc194575d49a6a93220aabd3def2f",
      "258a3aeaa9944cbdaa7b1426e9348229",
      "efa4ed2475894269a432df63c372ff19",
      "f80b7797abc344cd90a905c8163111d1",
      "a441d4b4af5845f2940c8a9ca33601c5",
      "a8be0fa41af64defa1ebddf62079d866",
      "a4ea8951a6744acaaa151d7725beae8f",
      "50f0fb44aeec4e1cb19eb9b014064d21",
      "f647b80b500f40e3ab6113d502d13772",
      "2eb28ea93bbc4f4098fcf8634482305e",
      "5a63edaf57b44d759a82304fca4a1903",
      "648f1a14a33d4e8fba574cbeb1d80e45",
      "3e43c2f1fa88412387b48e942437188c",
      "b985804cc0a74e1c87540a169476d3f3",
      "dbbc469b10b64a8fb4f5699a527ea0d0",
      "6cbb91eeecef46d69af4fbeb952709d5",
      "06d35a641e3043f1b407ea60e066c6de",
      "165b6c75d0e0430ebe2de747083a77e8",
      "d64edbdc85f3487abff0b7b57f3d114b",
      "7ba3398847c644af9b8c84578b01fbbd",
      "14e16dcb0bb64d5c93a4082da1c399cc",
      "fb5cc7a167c94ead81d77a5a0a4fd534",
      "d3c7b654a8ff4d4f9558783b90cdfade",
      "dcbfded2486d48f5bf51ccc46b7c5a34",
      "f2425596910c4675a8d7c5b222cb3ff4",
      "1740c62711d84927bb55eabc8768d5c3",
      "d7b50918308d489391b05763e7c13558",
      "ea5b9e08703d40beb8099e8c11cd411e",
      "31475bec409e433fa8ca42734a33e743",
      "07b5e66de8864fe39aeb0d688254a63a",
      "44a2273a6004435588b5a4b077345dfd",
      "59a0a36ffcaa4cc09e51b0fbb4fc658e",
      "1bcddccd6ec846eeaf9df8d6964de0fa",
      "737e2795d260486480adcbab7bd84133",
      "97ac3ad3a14544c68739abe1492257dc",
      "369f5badec574895a8699a99886a6063",
      "cbc1d57690e249228acdb5d3ff7d7f4c",
      "f6b93875d7f64c148731f5d840b2e748",
      "c7547645aec042fa934445815bc58eda",
      "c7007fd365734198becc68dbf6bccad2",
      "c90b485c51624591a427fad99f672b40",
      "b2a90458fcfa4dc58cf64e0467409164",
      "47a3bc2e00b44317930b0c1ef1e101d5",
      "d1a98ca1c5fc43cb978626a48d81810f",
      "50e11b9e1dac45fb8b83bc13d29c3282",
      "5953775eee3c499ab3748c0bad6b6450",
      "b4806683e9a646c3beb15d481ee2565c",
      "14ca66429bf14b8583cfbc246236f4b6",
      "05f11a09790d4cea9f0e84d46f50d7c0",
      "ba752b11ee754d03830cbd3042e1b94b",
      "4a75ecc1f319411faeabda81a5980176",
      "d98b226634964e0b8566f652e933e564",
      "0ab78b9dd79b42eab30b21cbcc750470",
      "e0601679088f4fb7b8ca29ef99988a3e",
      "d3d1edcae0c544079478331ed2ce5b89",
      "2619aa9406104e6895fcf3d820527d03",
      "1fea16c33bc64a8ebee8951d6610bffc",
      "a9edabeaa8464e68b34d7b9ea71e247c",
      "4cf2494ee2134429ba859a3d92c3a914",
      "477b2a4e354748c3958d5f88a4b3d844",
      "d64d264245ef4ba89074aa59ceb8b367",
      "e463e83ca0774fa9b53e22723c75b1d9",
      "62d698341d4e4b71941fcb2dea8c22d0",
      "b206d787f92b4cec926b471439c99841",
      "6ca9cb5ce9454b94b308d83ed830ec58",
      "bdbb9b4b84e845c99d4ef92e5316d8f5",
      "7f72c0df28df4d3bbab42253bcbe4e63",
      "be07fbf0f13b4fa2893548147c93dfe3",
      "21551396430a4f48bca5bdc86b8065a8",
      "04edb65b451d4ba3abfb7be6fdcdaf2b",
      "6e950d078cb2438199023b7a3218644e",
      "acee91cdb1fd4d9298623beba1f0e5b7",
      "bb22efedefb242548bb3c15778f1e9e8",
      "5fd4da93f9854d63b9372ccc80fc3b2b",
      "52ab2bfe032742d4ab6fcb7b1d633df3",
      "ab3f1e4bbe8f48c081eb52970f40a675",
      "8b7249dfd4ba4bfabaca24def92c011a",
      "0e2db6aa423d419db4166c6e3d704fb6",
      "6a15d529e44a4690a83ae3ca7ac82486",
      "6fd6300b459d407f90ea80dd983a1575",
      "dde66fada1924576a1322f024d4c9fa4",
      "0e844a92789846a29cfe02d3c60a60a7",
      "6c79838e673041489aef82c340f44041",
      "9f242fc90b144070b8c51a70283f4d32",
      "11ba71b7fd1e40c28c63530711bb6669",
      "b8f3496bf16a4b4587d1dd98b02abdd4",
      "f75d4b7ebb6845f29302bb0585902945",
      "ed03bf121dc640d486ca845ca79e8527",
      "920a489d093447bfa3972dceca2c0942",
      "bd647846098943e68c5360532b000cab",
      "661cd14a446947698fa8e2ddb5d91891",
      "a3e558a1af9d4ce39eab71ce7d003267",
      "b2f449fa33664a74a085d54e670df277",
      "5291bd550e3b4304bc7e2e30d3f622df",
      "dfdcb89c7bc447ed9d57697dff7f8c34",
      "d6e138d0e92240589a25cbb328df9f83",
      "3abb556f5de647ef8f69307850df3151",
      "80d31c606c964853aae0dcf79c542464",
      "6634b82ac3a341cbb9f7cbbac35aa454",
      "11daae196e6e48ffa0b450ba1bdc8a34",
      "10a467368fdf46e8af27357a10990209",
      "804e7a573a454de58f96c4a273132a77",
      "67e8bfee6d984452b36903937beb9ca9",
      "0e9ab6015c4846e7b7401702b5dc8c6e",
      "cb30e4a2e6354630ac6a7422c07cd645",
      "c7ef45828c2d422c98da026925207694",
      "86a04a9a069b489ab9a9dafff5350064",
      "a546cc324007493d9f22a82e9d1bac87",
      "87a68d6c74c74d30930ddeb3eff322be",
      "f890ad3355674d3ea4fa62c880bd4c87",
      "166d5fa16f144e1f9d3343c5b7298c56",
      "3e06859e478b44239f8cbd53372c72ad",
      "a301320ef1a94c37a504d31165367393",
      "c7412d21166a4f8d972c221d3d0b525b",
      "b4065baed64b456ea289d5f2e68a933e",
      "6e8a1e68f7fb42e383e46d336951c2c2",
      "073b44f80f4f4a63b309fb5d568f314a",
      "6c058bba473f4ef2b76a641b25ca15c1",
      "89b4fcd2c6d34477bd7afcc59eab11b7",
      "41acd8e652d448608a9a09c7aec7a9c2",
      "f248a5deb1e5462cb845112bed48edb1",
      "0556ef94234649b7a5c4922df24c6d28",
      "c040df71e9ee4cf0b85242ff4ad82b28",
      "ea0e5f7bcb7c48c7a51994c956adb260",
      "ddc35f4a0b954483a2feaa0b4d1d9b7d",
      "5c9632a593d143a4a6e145b48a6288e4",
      "a5bb8a88583042b487079314a8002669",
      "24a6d205125e4397bfdbc8a4e672550c",
      "2c54db079baf4399a57894e8b02c0cab",
      "73f7829462ef4ce183967e1c723faaf4",
      "fd4049f6f98848d5a38ca63d7bde5c57",
      "5f65d510e9174e6c854b5477d5cd2a89",
      "24936e881dc04a1786e55c4db5fe2b25",
      "e0596c28b70945e49dfb0b04c95480bb",
      "3cee2ef15b66486c9040e8e440dceef8",
      "0376d29f7d844e2b8c8a6e58c3aaf12d",
      "50fd5407711b411da1c57e67e921dad9",
      "07be060f3b144e2389ed9929737d9219",
      "3cf390cd4e2244b1860eeef395d113c0",
      "dc373fe286c04f62adf1fc2b119773ee",
      "b11a63643e0244b9b0d6fdecf1e3a5a2",
      "af0b4e16555e44ecbbd2dab98fff5a68",
      "4c11b1d42beb46c884c9578dfd899160",
      "f4f29fba72f540b89e065b76f25adcd7",
      "e33c33b3b4104e8b87800ef2e7b4d2bc",
      "759660352ba54d35b5d34b95d22c6f80",
      "03a2f004b68a4f45a1328cb21dbca8a9",
      "291665449c1342cb94b25e34033e1d51",
      "df398907958c4fe99029a60ad8a26d63",
      "e3181f59f59d4e3a885e0f86196f0da5",
      "d942abf606164bd3aefa7442a0ec9c2a",
      "9d12c4d22bc54986b7fbbbabb100228c",
      "9d6945364dcf41219aef526e75c37e77",
      "e198183146674cc9a6b8bb3012ba4dd2",
      "0fd07cc7669f42e89cac5767649810d0",
      "9a906145dde24593921a96339cc6abb3",
      "e9878381eb954e7fbb3c1f0cb68dee9e",
      "404f496f438f4d23a51f04344a76cd23",
      "44268f607cc14bdd88f8e290bf71ec96",
      "695e907321254deb9fd7646f086084f6",
      "095fbac3eb554eb187d631eb3326ead6",
      "6a597adb7cba4566882b2413828971f7",
      "74edc583e18c469fac2c067eff6669ae",
      "aa90717f046445369617df3f917fe464",
      "6d5d3de858c041b1b5cb7ef90b658915",
      "f81c0a04c81f480a8aa825b6856f8581",
      "06db7015cb9e4115bf42e1643faca2ec",
      "d4068133425c4a5786e8b966d5eed726",
      "0cf9988855684e5eae6a00f24d057ec1",
      "a04e7a3d94824a4bb505d61a7a492d54",
      "becbf8be8fb94bf6a374c447744ed1e6",
      "8211f17994f94cdfba2f25a2758d6cf0",
      "759c41528cb5474b90dab14fe86c79d1",
      "488a4501ed7b4e1cad4e6da9a9f6dca7",
      "415411be6cdd48da8820934ec221662a",
      "07beb7ee56b24cf4aabf055afff6ccb3",
      "b1b9d604e0d844689571d0d86732ea4e",
      "1a17e7412f67435bb44fc1247b650c51",
      "0a750110b5cc45e684a2a2d7a17ae002",
      "6611297013674954957dcf083edc6878",
      "f6c81654cb884de39e8ef260ada84483",
      "662cb6abd8084e7aa640f73b22aa941b",
      "cc19119903204108ba38d7cc98acc0ff",
      "815c5a29f2154e45a17f92811c18f535",
      "a319aea4efa843caaf5258f0e2f6c5fd",
      "f38197499b8a4dd982c953e920adf9cb",
      "0e32112a72e84f88a3a5e796522fc4c8",
      "ba39c34168d044038923c9646bc1abc9",
      "d23daa74de804c9aa4a0524f7b210bfb",
      "d20e0eeb6e98425db9352bc51c7c112d",
      "c0e43960ab664162ab48d623877cf625",
      "c02048d5451c46158dea1e9315a81192",
      "787d1e16bb88456e9b4a2b77d1a80fd4",
      "1899ca43127a4a4b80b060ea3cd3bc14",
      "767eea53ef9b4cd58d5b9251d96bfb08",
      "1c587fabd84d47619359ea332adb7b9a",
      "84223f9a50c6439f85ad44a014c2c26d",
      "245e55e941b943d7977bb6f2db4969da",
      "d2122b77437541e09244c0f74b000518",
      "d54a3548486d4b43bfd358f1e2a6f587",
      "a9b9a7deeb494d5eb9cd9785a8d63be4",
      "bb5190e607954c2c8047a79fbb8a4b6f",
      "30aaa055d454483cab66179168009035",
      "ade7f63949134e06ba419dc3d4d85ba0",
      "f8d6412f9b404a029a31da30957d7c9d",
      "687b954bf310440684e7cf264357e4a6",
      "4b0755c6cac941f1b09d777c83f29318",
      "7f4aabec32db480681d07e59674e5780",
      "fa9fefcee2354494929c51e50b0b518a",
      "6c750692615d4e369242ba155956c8b8",
      "f02222369bf44cd5a939610454f468f9",
      "df229eecb0c8492e83cc773d6800db7f",
      "1adc60795ec547328190003cecca70d1",
      "b92ae91f7e294b25bcfaf2a3c6049e1f",
      "b8cf90a302374a89bf8f9ead90e14300",
      "741a6b9ee574434c918256eadbe1fe3b",
      "8544d528b1ce43c48f430beacda89095",
      "25072ca8d3b8424b855e1b46b91d56d7",
      "196947f31660480f972c9bcfe4fab894",
      "840c10fca4484480a68720acc14cf793",
      "9038e1dc71424fce8042290b51db685b",
      "1fc3218a5029479ca7219ed6bc9cb5e0",
      "bc6db43967ba43f6b118583525f9b429",
      "e82478e824074ed6b4bea45ea4c9734b",
      "058d71a241e24ba8aca809d394a82b2c",
      "6f3702c2cb044be3867563d31eb21067",
      "674b41827cd845d6b81f6b5ce386da53",
      "74d9e871737b4eb8b9afcd31e9b529a6",
      "c9dc6856e2294858b7c83cdea1c4da70",
      "85b242fe7c3442aea5d3aff73dc3f0ad",
      "1b218db4aa2441f7bda169d2586b9961",
      "6f146d7e71fd4cab86b496ae57a42617",
      "9ce33a8c18924a02ac292c0e2c9b8183",
      "4b380df089c74956834e762603034809",
      "bae0168512054ffd8c8bfeb11c8f74e0",
      "53133021342e410792efbd430269f1d0",
      "1bcdc4ba92864f19aed2fbfe129af78e",
      "bf6ba656c122448c94949f5be35a5d8e",
      "3668e611af264721a6f5b6c07c3594ac",
      "7e1c1e88a8ae443cabb115317d69ae07",
      "e4f56d8ef7ff4e4a888a9b69107ce637",
      "18562f2c8e574edc9656d50d37870724",
      "fc7a8327e34340d189bacb09b135c74c",
      "38e84ee9e4f54930b3f32bfb41aeeed5",
      "5ae73e9fc8884c019f874f90cb293484",
      "9daae2342993472197679bebc8772a5d",
      "e952a7fcde4d4fb492503d116091d961",
      "5c06ac5ce1794da4a7c58345c354319b",
      "cede1a04f53143fa8275fa943501ee22",
      "4c69f0920e1b43dd831061c416781735",
      "f3cfec6e107e41ebba5abdd90d75d695",
      "10dac60bb9e14283ad3400a1d1bfb6ac",
      "5643e664e24040baab93081f3fd7e8a5",
      "ba674e1d303c4b709ea928496aa268da",
      "9a122c6b120b48178a88cd94fe32ce05",
      "792a82e295cf4461850b783e853f79a4",
      "fd5710d537df4bcdb129f1c05d033577",
      "d534c2da3394447690b1d8fa381353ad",
      "962b93bd6ee54030b622e4b585040d60",
      "553a42643a084ebebfb604f9a8157acb",
      "593acc2e72a747ed94d1687e9aeab928",
      "3cd15ac4e58c404fbcdecf51491be7c6",
      "a1b2e51d7bd54fee93bf582ee4d03441",
      "d31de5326f3744e6945d027c1818b13d",
      "eb8b84f532364e8fa8ef83bceff8d9d0",
      "e910103cac844b81a21f2c9a9b3b238b",
      "90b1e925d7604e08a3cc32e804b516d7",
      "df46cf3f08ea42f586b2e290842fa5b7",
      "d3eb50aa490641ac86f108b7bcdd2632",
      "9444a0e109924a88bd0b9fc7d123793f",
      "d4e98b354eb24ea89f8a8c9ef4884bc1",
      "0d8f780f4efa49f99d4a6bf40aeba052",
      "19ae2e2f2a3740178eda900234872b39",
      "be94f64ba6a145b18b58400944f4e2cd",
      "e506edd5f17b48929babead7071774b0",
      "688d35d0ba244245b7776dd568317c23",
      "8fab9d9b8b8c4fdfa9e1913ddd96f94b"
     ]
    },
    "executionInfo": {
     "elapsed": 6323461,
     "status": "ok",
     "timestamp": 1769417721431,
     "user": {
      "displayName": "Babatunde Olabuntu",
      "userId": "13837852601119920576"
     },
     "user_tz": -60
    },
    "id": "180125a4",
    "outputId": "06e0da4e-d6c2-4186-ffe1-cfb9824ae037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUDA is available. Training will run on GPU.\n",
      "================================================================================\n",
      "================================================================================\n",
      "STEP 1: Loading Medical Text Classification Dataset\n",
      "================================================================================\n",
      "Loading training data from: ./data/train.dat\n",
      "  ✓ Loaded 14438 training examples\n",
      "Loading test data from: ./data/test.dat\n",
      "  ✓ Loaded 14442 test examples\n",
      "  ⚠ Note: Test file has no labels (unlabeled data for prediction)\n",
      "\n",
      "Unique raw labels found in training data: [1, 2, 3, 4, 5]\n",
      "Total number of classes: 5\n",
      "\n",
      "Label mapping (raw_label -> normalized_id):\n",
      "  1 -> 0\n",
      "  2 -> 1\n",
      "  3 -> 2\n",
      "  4 -> 3\n",
      "  5 -> 4\n",
      "\n",
      "Splitting training data into train/validation sets (80/20)...\n",
      "  ✓ Training set: 11550 examples\n",
      "  ✓ Validation set: 2888 examples\n",
      "\n",
      "Saving processed datasets (maintaining order)...\n",
      "  ✓ Saved training data: ./processed_data/train_processed.csv (11550 examples)\n",
      "  ✓ Saved validation data: ./processed_data/val_processed.csv (2888 examples)\n",
      "  ✓ Saved test data: ./processed_data/test_processed.csv (14442 examples)\n",
      "  ✓ Saved label mappings: ./processed_data/label_mapping.json\n",
      "\n",
      "✓ Dataset loaded successfully\n",
      "  - Training examples: 11550\n",
      "  - Validation examples: 2888\n",
      "  - Test examples: 14442 (unlabeled, for prediction)\n",
      "  - Number of classes: 5\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Model Selection\n",
      "================================================================================\n",
      "Models to evaluate: 4\n",
      "  1. bert-base-uncased\n",
      "  2. distilbert-base-uncased\n",
      "  3. dmis-lab/biobert-v1.1\n",
      "  4. emilyalsentzer/Bio_ClinicalBERT\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Training and Evaluation\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training model: bert-base-uncased\n",
      "================================================================================\n",
      "Loading tokenizer and model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ca2065d8c43bab690f36286369711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab515b706bff40999ac9e6cfd99aef34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93d18394caf4b608d5e9d31120a4e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa4ed2475894269a432df63c372ff19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b985804cc0a74e1c87540a169476d3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Model loaded with 5 output classes\n",
      "\n",
      "Converting DataFrames to Hugging Face Datasets...\n",
      "  ✓ Training dataset: 11550 examples\n",
      "  ✓ Validation dataset: 2888 examples (for evaluation)\n",
      "\n",
      "Tokenizing datasets (max_length=256)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2425596910c4675a8d7c5b222cb3ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369f5badec574895a8699a99886a6063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Tokenization complete\n",
      "\n",
      "Configuring training arguments...\n",
      "  ✓ Training arguments configured\n",
      "\n",
      "Initializing Trainer (with early stopping)...\n",
      "  ✓ Trainer initialized\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2166' max='2166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2166/2166 27:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.898400</td>\n",
       "      <td>0.864698</td>\n",
       "      <td>0.635042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.850733</td>\n",
       "      <td>0.644391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.859484</td>\n",
       "      <td>0.641620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Training complete\n",
      "\n",
      "================================================================================\n",
      "Evaluating model...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for bert-base-uncased:\n",
      "  eval_loss: 0.8507\n",
      "  eval_accuracy: 0.6444\n",
      "  eval_runtime: 37.6585\n",
      "  eval_samples_per_second: 76.6890\n",
      "  eval_steps_per_second: 4.8060\n",
      "  epoch: 3.0000\n",
      "\n",
      "================================================================================\n",
      "Generating detailed classification report on validation set...\n",
      "================================================================================\n",
      "\n",
      "Classification report (per-class metrics):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7171    0.8610    0.7825       633\n",
      "           1     0.5178    0.7291    0.6056       299\n",
      "           2     0.6142    0.5169    0.5614       385\n",
      "           3     0.6741    0.7967    0.7303       610\n",
      "           4     0.6239    0.4298    0.5089       961\n",
      "\n",
      "    accuracy                         0.6444      2888\n",
      "   macro avg     0.6294    0.6667    0.6377      2888\n",
      "weighted avg     0.6426    0.6444    0.6326      2888\n",
      "\n",
      "\n",
      "Saving tokenizer to model_results/bert-base-uncased...\n",
      "  ✓ Tokenizer saved\n",
      "\n",
      "✓ Completed: bert-base-uncased - Accuracy: 0.6444\n",
      "\n",
      "================================================================================\n",
      "Training model: distilbert-base-uncased\n",
      "================================================================================\n",
      "Loading tokenizer and model: distilbert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4806683e9a646c3beb15d481ee2565c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9edabeaa8464e68b34d7b9ea71e247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21551396430a4f48bca5bdc86b8065a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd6300b459d407f90ea80dd983a1575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661cd14a446947698fa8e2ddb5d91891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Model loaded with 5 output classes\n",
      "\n",
      "Converting DataFrames to Hugging Face Datasets...\n",
      "  ✓ Training dataset: 11550 examples\n",
      "  ✓ Validation dataset: 2888 examples (for evaluation)\n",
      "\n",
      "Tokenizing datasets (max_length=256)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804e7a573a454de58f96c4a273132a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a301320ef1a94c37a504d31165367393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Tokenization complete\n",
      "\n",
      "Configuring training arguments...\n",
      "  ✓ Training arguments configured\n",
      "\n",
      "Initializing Trainer (with early stopping)...\n",
      "  ✓ Trainer initialized\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2166' max='2166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2166/2166 15:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.880819</td>\n",
       "      <td>0.633657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>0.857533</td>\n",
       "      <td>0.638158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.754100</td>\n",
       "      <td>0.866073</td>\n",
       "      <td>0.630194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Training complete\n",
      "\n",
      "================================================================================\n",
      "Evaluating model...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for distilbert-base-uncased:\n",
      "  eval_loss: 0.8575\n",
      "  eval_accuracy: 0.6382\n",
      "  eval_runtime: 18.5379\n",
      "  eval_samples_per_second: 155.7890\n",
      "  eval_steps_per_second: 9.7640\n",
      "  epoch: 3.0000\n",
      "\n",
      "================================================================================\n",
      "Generating detailed classification report on validation set...\n",
      "================================================================================\n",
      "\n",
      "Classification report (per-class metrics):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7191    0.8452    0.7771       633\n",
      "           1     0.5309    0.6890    0.5997       299\n",
      "           2     0.5945    0.5065    0.5470       385\n",
      "           3     0.6618    0.8148    0.7303       610\n",
      "           4     0.6056    0.4266    0.5006       961\n",
      "\n",
      "    accuracy                         0.6382      2888\n",
      "   macro avg     0.6224    0.6564    0.6309      2888\n",
      "weighted avg     0.6331    0.6382    0.6262      2888\n",
      "\n",
      "\n",
      "Saving tokenizer to model_results/distilbert-base-uncased...\n",
      "  ✓ Tokenizer saved\n",
      "\n",
      "✓ Completed: distilbert-base-uncased - Accuracy: 0.6382\n",
      "\n",
      "================================================================================\n",
      "Training model: dmis-lab/biobert-v1.1\n",
      "================================================================================\n",
      "Loading tokenizer and model: dmis-lab/biobert-v1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0e5f7bcb7c48c7a51994c956adb260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cee2ef15b66486c9040e8e440dceef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/462 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759660352ba54d35b5d34b95d22c6f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9878381eb954e7fbb3c1f0cb68dee9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4068133425c4a5786e8b966d5eed726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Model loaded with 5 output classes\n",
      "\n",
      "Converting DataFrames to Hugging Face Datasets...\n",
      "  ✓ Training dataset: 11550 examples\n",
      "  ✓ Validation dataset: 2888 examples (for evaluation)\n",
      "\n",
      "Tokenizing datasets (max_length=256)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a750110b5cc45e684a2a2d7a17ae002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20e0eeb6e98425db9352bc51c7c112d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b9a7deeb494d5eb9cd9785a8d63be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Tokenization complete\n",
      "\n",
      "Configuring training arguments...\n",
      "  ✓ Training arguments configured\n",
      "\n",
      "Initializing Trainer (with early stopping)...\n",
      "  ✓ Trainer initialized\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2166' max='2166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2166/2166 28:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.805191</td>\n",
       "      <td>0.653393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.749500</td>\n",
       "      <td>0.804610</td>\n",
       "      <td>0.642659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.833556</td>\n",
       "      <td>0.636773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Training complete\n",
      "\n",
      "================================================================================\n",
      "Evaluating model...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for dmis-lab/biobert-v1.1:\n",
      "  eval_loss: 0.8052\n",
      "  eval_accuracy: 0.6534\n",
      "  eval_runtime: 37.7689\n",
      "  eval_samples_per_second: 76.4650\n",
      "  eval_steps_per_second: 4.7920\n",
      "  epoch: 3.0000\n",
      "\n",
      "================================================================================\n",
      "Generating detailed classification report on validation set...\n",
      "================================================================================\n",
      "\n",
      "Classification report (per-class metrics):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7237    0.8483    0.7811       633\n",
      "           1     0.5580    0.7726    0.6480       299\n",
      "           2     0.5700    0.6130    0.5907       385\n",
      "           3     0.6719    0.8361    0.7451       610\n",
      "           4     0.6673    0.3881    0.4908       961\n",
      "\n",
      "    accuracy                         0.6534      2888\n",
      "   macro avg     0.6382    0.6916    0.6511      2888\n",
      "weighted avg     0.6563    0.6534    0.6377      2888\n",
      "\n",
      "\n",
      "Saving tokenizer to model_results/biobert-v1.1...\n",
      "  ✓ Tokenizer saved\n",
      "\n",
      "✓ Completed: dmis-lab/biobert-v1.1 - Accuracy: 0.6534\n",
      "\n",
      "================================================================================\n",
      "Training model: emilyalsentzer/Bio_ClinicalBERT\n",
      "================================================================================\n",
      "Loading tokenizer and model: emilyalsentzer/Bio_ClinicalBERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df229eecb0c8492e83cc773d6800db7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6db43967ba43f6b118583525f9b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b380df089c74956834e762603034809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae73e9fc8884c019f874f90cb293484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Model loaded with 5 output classes\n",
      "\n",
      "Converting DataFrames to Hugging Face Datasets...\n",
      "  ✓ Training dataset: 11550 examples\n",
      "  ✓ Validation dataset: 2888 examples (for evaluation)\n",
      "\n",
      "Tokenizing datasets (max_length=256)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a82e295cf4461850b783e853f79a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b1e925d7604e08a3cc32e804b516d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Tokenization complete\n",
      "\n",
      "Configuring training arguments...\n",
      "  ✓ Training arguments configured\n",
      "\n",
      "Initializing Trainer (with early stopping)...\n",
      "  ✓ Trainer initialized\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2166' max='2166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2166/2166 27:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.881700</td>\n",
       "      <td>0.821043</td>\n",
       "      <td>0.652701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.777900</td>\n",
       "      <td>0.826772</td>\n",
       "      <td>0.640928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.840457</td>\n",
       "      <td>0.637119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Training complete\n",
      "\n",
      "================================================================================\n",
      "Evaluating model...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for emilyalsentzer/Bio_ClinicalBERT:\n",
      "  eval_loss: 0.8210\n",
      "  eval_accuracy: 0.6527\n",
      "  eval_runtime: 37.9181\n",
      "  eval_samples_per_second: 76.1640\n",
      "  eval_steps_per_second: 4.7730\n",
      "  epoch: 3.0000\n",
      "\n",
      "================================================================================\n",
      "Generating detailed classification report on validation set...\n",
      "================================================================================\n",
      "\n",
      "Classification report (per-class metrics):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7266    0.8357    0.7774       633\n",
      "           1     0.5365    0.7625    0.6298       299\n",
      "           2     0.5773    0.6597    0.6158       385\n",
      "           3     0.6839    0.8230    0.7470       610\n",
      "           4     0.6631    0.3871    0.4888       961\n",
      "\n",
      "    accuracy                         0.6527      2888\n",
      "   macro avg     0.6375    0.6936    0.6518      2888\n",
      "weighted avg     0.6569    0.6527    0.6381      2888\n",
      "\n",
      "\n",
      "Saving tokenizer to model_results/Bio_ClinicalBERT...\n",
      "  ✓ Tokenizer saved\n",
      "\n",
      "✓ Completed: emilyalsentzer/Bio_ClinicalBERT - Accuracy: 0.6527\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Model Comparison Results\n",
      "================================================================================\n",
      "Model Name                                        Accuracy\n",
      "-----------------------------------------------------------\n",
      "bert-base-uncased                                   0.6444\n",
      "distilbert-base-uncased                             0.6382\n",
      "dmis-lab/biobert-v1.1                               0.6534\n",
      "emilyalsentzer/Bio_ClinicalBERT                     0.6527\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL\n",
      "================================================================================\n",
      "Model: dmis-lab/biobert-v1.1\n",
      "Validation Accuracy: 0.6534\n",
      "Model directory: model_results/biobert-v1.1\n",
      "\n",
      "Why this model is the best:\n",
      "  • Highest validation accuracy: 0.6534\n",
      "  • 0.96% better than average (0.6472)\n",
      "  • 0.0062 absolute improvement over average\n",
      "  • This model was pre-trained on biomedical/clinical text,\n",
      "    making it well-suited for medical text classification tasks\n",
      "\n",
      "All models have been saved to: model_results\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main pipeline: loads data, trains multiple transformer models, and compares their performance.\n",
    "    I compare both general-purpose and domain-specific models to see which works best for medical text.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: Loading Medical Text Classification Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    dataset = load_medical_text_dataset(\n",
    "        base_directory=\".\",\n",
    "        data_subdirectory=\"data\"\n",
    "    )\n",
    "    print(f\"\\n✓ Dataset loaded successfully\")\n",
    "    print(f\"  - Training examples: {len(dataset.train_df)}\")\n",
    "    print(f\"  - Validation examples: {len(dataset.val_df)}\")\n",
    "    print(f\"  - Test examples: {len(dataset.test_df)} (unlabeled, for prediction)\")\n",
    "    print(f\"  - Number of classes: {len(dataset.label_to_id)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: Model Selection\")\n",
    "    print(\"=\" * 80)\n",
    "    model_names = [\n",
    "        \"bert-base-uncased\",\n",
    "        \"distilbert-base-uncased\",\n",
    "        \"dmis-lab/biobert-v1.1\",\n",
    "        \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    ]\n",
    "    print(f\"Models to evaluate: {len(model_names)}\")\n",
    "    for i, name in enumerate(model_names, 1):\n",
    "        print(f\"  {i}. {name}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: Training and Evaluation\")\n",
    "    print(\"=\" * 80)\n",
    "    results_summary: List[Tuple[str, float]] = []\n",
    "    results_base_dir = \"model_results\"\n",
    "\n",
    "    for model_name in model_names:\n",
    "        short_name = model_name.split(\"/\")[-1]\n",
    "        model_output_dir = os.path.join(results_base_dir, short_name)\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "        accuracy, _ = train_and_evaluate_model(\n",
    "            model_name=model_name,\n",
    "            dataset=dataset,\n",
    "            output_directory=model_output_dir,\n",
    "            num_epochs=3,\n",
    "            batch_size=16,\n",
    "            max_length=256,\n",
    "            early_stopping_patience=2,\n",
    "        )\n",
    "\n",
    "        results_summary.append((model_name, accuracy))\n",
    "        print(f\"\\n✓ Completed: {model_name} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: Model Comparison Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Model Name':<45} {'Accuracy':>12}\")\n",
    "    print(\"-\" * 59)\n",
    "    for name, acc in results_summary:\n",
    "        print(f\"{name:<45} {acc:>12.4f}\")\n",
    "\n",
    "    best_model, best_acc = max(results_summary, key=lambda x: x[1])\n",
    "    best_model_dir = os.path.join(results_base_dir, best_model.split('/')[-1])\n",
    "\n",
    "    accuracies = [acc for _, acc in results_summary]\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    improvement = best_acc - avg_accuracy\n",
    "    improvement_pct = (improvement / avg_accuracy) * 100 if avg_accuracy > 0 else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BEST MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {best_model}\")\n",
    "    print(f\"Validation Accuracy: {best_acc:.4f}\")\n",
    "    print(f\"Model directory: {best_model_dir}\")\n",
    "    print(f\"\\nWhy this model is the best:\")\n",
    "    print(f\"  • Highest validation accuracy: {best_acc:.4f}\")\n",
    "    print(f\"  • {improvement_pct:.2f}% better than average ({avg_accuracy:.4f})\")\n",
    "    print(f\"  • {improvement:.4f} absolute improvement over average\")\n",
    "\n",
    "    if \"biobert\" in best_model.lower() or \"clinical\" in best_model.lower():\n",
    "        print(f\"  • This model was pre-trained on biomedical/clinical text,\")\n",
    "        print(f\"    making it well-suited for medical text classification tasks\")\n",
    "    elif \"distilbert\" in best_model.lower():\n",
    "        print(f\"  • This is a smaller, faster model that achieved competitive performance\")\n",
    "    else:\n",
    "        print(f\"  • This is a general-purpose BERT model that performed best on this task\")\n",
    "\n",
    "    print(f\"\\nAll models have been saved to: {results_base_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"=\" * 80)\n",
    "        print(\"WARNING: CUDA is not available.\")\n",
    "        print(\"Training will run on CPU and may be slow.\")\n",
    "        print(\"=\" * 80)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"=\" * 80)\n",
    "        print(\"CUDA is available. Training will run on GPU.\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}