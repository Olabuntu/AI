{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ab12c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPLETE PATIENT SUBTYPE DISCOVERY PROJECT =====\n",
    "\"\"\"\n",
    "PROJECT OVERVIEW:\n",
    "-----------------\n",
    "This project demonstrates unsupervised learning (clustering) to discover patient subtypes\n",
    "from medical data (Breast Cancer Wisconsin Dataset). In real-world scenarios, we don't know \n",
    "how many patient subtypes exist - we use clustering algorithms to discover them.\n",
    "\n",
    "WHY CLUSTERING?\n",
    "---------------\n",
    "- No labeled data: We don't know patient subtypes in advance\n",
    "- Discovery: Find hidden patterns and groups in the data\n",
    "- Personalized medicine: Different subtypes may need different treatments\n",
    "- Medical research: Understand disease heterogeneity and tumor characteristics\n",
    "\n",
    "APPROACH:\n",
    "---------\n",
    "1. Load real medical data (Breast Cancer Wisconsin - 569 patients, 30 features)\n",
    "2. Preprocess data (scale features)\n",
    "3. Apply multiple clustering algorithms (K-Means, DBSCAN, Hierarchical)\n",
    "4. Visualize results using dimensionality reduction (PCA, UMAP)\n",
    "5. Analyze discovered clusters and find marker features\n",
    "\"\"\"\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np  # Numerical operations and arrays\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Basic plotting\n",
    "import seaborn as sns  # Statistical visualizations\n",
    "\n",
    "# Clustering algorithms - we'll compare multiple methods\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "# K-Means: Finds k clusters by minimizing within-cluster variance\n",
    "# DBSCAN: Finds clusters of arbitrary shape, handles noise\n",
    "# AgglomerativeClustering: Hierarchical clustering (bottom-up approach)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # Normalize features (mean=0, std=1)\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis for visualization\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples  # Cluster quality metrics\n",
    "import umap  # UMAP: Advanced dimensionality reduction that preserves local structure\n",
    "from sklearn.datasets import make_classification, load_breast_cancer  # Generate synthetic data / Load real medical data\n",
    "from sklearn.neighbors import NearestNeighbors  # For DBSCAN parameter selection\n",
    "import os  # File operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62869fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REAL MEDICAL DATA LOADED: Breast Cancer Wisconsin Dataset\n",
      "======================================================================\n",
      "Data shape: (569, 30) (samples × features)\n",
      "Number of samples: 569 patients\n",
      "Number of features: 30 cell nucleus measurements\n",
      "Number of true classes: 2\n",
      "Class names: ['malignant' 'benign']\n",
      "\n",
      "Class distribution:\n",
      "  Benign (0): 212 patients (37.3%)\n",
      "  Malignant (1): 357 patients (62.7%)\n",
      "\n",
      "Feature examples (first 5):\n",
      "  1. mean radius\n",
      "  2. mean texture\n",
      "  3. mean perimeter\n",
      "  4. mean area\n",
      "  5. mean smoothness\n",
      "======================================================================\n",
      "\n",
      "✓ This dataset has NATURALLY well-separated clusters!\n",
      "✓ Clustering should work much better than synthetic data.\n",
      "✓ Results will be medically interpretable.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LOAD REAL MEDICAL DATA: BREAST CANCER WISCONSIN DATASET\n",
    "--------------------------------------------------------\n",
    "Why use real medical data?\n",
    "- Medically relevant: Real breast cancer patient data\n",
    "- Natural cluster structure: Data has inherent biological patterns\n",
    "- Validated ground truth: Known diagnosis labels (benign vs malignant)\n",
    "- More realistic: Tests clustering on real-world medical patterns\n",
    "- Better validation: Compare against established medical diagnosis\n",
    "\n",
    "DATASET DETAILS:\n",
    "---------------\n",
    "- 569 breast cancer patients\n",
    "- 30 features: Cell nucleus measurements (radius, texture, perimeter, etc.)\n",
    "- 2 classes: \n",
    "  * 0 = Benign (non-cancerous)\n",
    "  * 1 = Malignant (cancerous)\n",
    "- Well-separated clusters: Natural separation between benign and malignant\n",
    "\n",
    "MEDICAL RELEVANCE:\n",
    "-----------------\n",
    "This dataset is used in medical research to:\n",
    "- Classify breast cancer tumors\n",
    "- Understand tumor characteristics\n",
    "- Develop diagnostic tools\n",
    "- Study cancer subtypes\n",
    "\n",
    "NOTE: In real clustering, we wouldn't have the diagnosis labels!\n",
    "We use them here only to validate our clustering results.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load Breast Cancer Wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "target_names = cancer.target_names\n",
    "\n",
    "# Create DataFrame for consistency with your code\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Disease_Status'] = y  # Using same column name for consistency\n",
    "# IMPORTANT: In sklearn's Breast Cancer dataset:\n",
    "# - target_names = ['malignant', 'benign']\n",
    "# - y == 0 means MALIGNANT\n",
    "# - y == 1 means BENIGN\n",
    "\n",
    "# Create gene_names equivalent (feature names for consistency)\n",
    "gene_names = feature_names.tolist()\n",
    "\n",
    "# Save the data for reproducibility\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/breast_cancer_data.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REAL MEDICAL DATA LOADED: Breast Cancer Wisconsin Dataset\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data shape: {X.shape} (samples × features)\")\n",
    "print(f\"Number of samples: {len(X)} patients\")\n",
    "print(f\"Number of features: {len(feature_names)} cell nucleus measurements\")\n",
    "print(f\"Number of true classes: {len(np.unique(y))}\")\n",
    "print(f\"Class names: {target_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Malignant (0): {np.sum(y == 0)} patients ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "print(f\"  Benign (1): {np.sum(y == 1)} patients ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nFeature examples (first 5):\")\n",
    "for i, name in enumerate(feature_names[:5]):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ This dataset has NATURALLY well-separated clusters!\")\n",
    "print(\"✓ Clustering should work much better than synthetic data.\")\n",
    "print(\"✓ Results will be medically interpretable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b1c0779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data range: [0.00, 4254.00]\n",
      "Scaled data range: [-3.11, 12.07]\n",
      "Scaled data mean: -0.0000 (should be ~0)\n",
      "Scaled data std: 1.0000 (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DATA PREPROCESSING: FEATURE SCALING\n",
    "------------------------------------\n",
    "WHY SCALE?\n",
    "----------\n",
    "Cell nucleus measurements have very different scales:\n",
    "- Some features measure in micrometers (e.g., radius: 6-30 μm)\n",
    "- Others are ratios or counts (e.g., texture: 9-40, smoothness: 0.05-0.16)\n",
    "- Without scaling, features with larger values dominate distance calculations\n",
    "- Clustering algorithms use distances - we want all features to contribute equally\n",
    "\n",
    "WHAT IS STANDARDIZATION?\n",
    "------------------------\n",
    "StandardScaler transforms each feature to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "This ensures all features are on the same scale.\n",
    "\n",
    "IMPORTANT NOTE:\n",
    "---------------\n",
    "In supervised learning, we'd split data into train/test sets FIRST, then scale.\n",
    "But in CLUSTERING (unsupervised), we scale the ENTIRE dataset because:\n",
    "- We're not predicting on new data\n",
    "- We're exploring/discovering patterns in ALL available data\n",
    "- There's no \"test set\" - clustering is exploratory analysis\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Transform: (X - mean) / std for each feature\n",
    "\n",
    "print(f\"Original data range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"Scaled data range: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]\")\n",
    "print(f\"Scaled data mean: {X_scaled.mean():.4f} (should be ~0)\")\n",
    "print(f\"Scaled data std: {X_scaled.std():.4f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3452250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by PC1: 44.27%\n",
      "Variance explained by PC2: 18.97%\n",
      "Total variance explained: 63.24%\n",
      "(The remaining variance is lost in the 2D projection)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DIMENSIONALITY REDUCTION: PCA FOR VISUALIZATION\n",
    "------------------------------------------------\n",
    "THE PROBLEM:\n",
    "------------\n",
    "- We have 30 features (30 dimensions)\n",
    "- Humans can only visualize 2D or 3D\n",
    "- We need to reduce dimensions to see clusters\n",
    "\n",
    "WHAT IS PCA?\n",
    "------------\n",
    "Principal Component Analysis (PCA):\n",
    "- Finds directions of maximum variance in the data\n",
    "- Projects data onto these \"principal components\"\n",
    "- First PC captures most variance, second PC captures second-most, etc.\n",
    "- We use first 2 PCs to visualize in 2D\n",
    "\n",
    "WHY PCA?\n",
    "--------\n",
    "- Fast and interpretable\n",
    "- Preserves global structure (good for seeing overall patterns)\n",
    "- Standard technique in medical data analysis\n",
    "\n",
    "NOTE:\n",
    "-----\n",
    "Unlike the old synthetic gene-expression example (2000 features), PCA in this dataset\n",
    "should explain a much larger fraction of variance in 2D.\n",
    "\"\"\"\n",
    "pca = PCA(n_components=2)  # Reduce from 30 dimensions to 2 dimensions\n",
    "X_pca = pca.fit_transform(X_scaled)  # Transform data to 2D space\n",
    "\n",
    "# Check how much variance is explained by these 2 components\n",
    "variance_explained = pca.explained_variance_ratio_\n",
    "print(f\"Variance explained by PC1: {variance_explained[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {variance_explained[1]:.2%}\")\n",
    "print(f\"Total variance explained: {variance_explained.sum():.2%}\")\n",
    "print(f\"(The remaining variance is lost in the 2D projection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fb29bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different values of k...\n",
      "============================================================\n",
      "k= 2: Inertia=11595.53, Silhouette Score=0.3434\n",
      "k= 3: Inertia=10061.80, Silhouette Score=0.3144\n",
      "k= 4: Inertia=9258.99, Silhouette Score=0.2833\n",
      "k= 5: Inertia=8558.66, Silhouette Score=0.1582\n",
      "k= 6: Inertia=7970.26, Silhouette Score=0.1604\n",
      "\n",
      "============================================================\n",
      "RESULTS:\n",
      "============================================================\n",
      "Best k by Silhouette Score: k=2 (score=0.3434)\n",
      "Ground truth k: 2\n",
      "Match: ✓ YES!\n",
      "============================================================\n",
      "\n",
      "Plots saved. Silhouette score is more objective than elbow method!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: K-MEANS CLUSTERING - FINDING OPTIMAL NUMBER OF CLUSTERS\n",
    "----------------------------------------------------------------\n",
    "WHAT IS K-MEANS?\n",
    "---------------\n",
    "- Partitions data into k clusters\n",
    "- Each cluster has a \"centroid\" (center point)\n",
    "- Assigns each sample to nearest centroid\n",
    "- Minimizes \"inertia\" (sum of squared distances to centroids)\n",
    "\n",
    "THE CHALLENGE: HOW MANY CLUSTERS (k)?\n",
    "-------------------------------------\n",
    "In real clustering, we DON'T know how many patient subtypes exist!\n",
    "We need to find the optimal k using MULTIPLE methods.\n",
    "\n",
    "METHOD 1: ELBOW METHOD\n",
    "----------------------\n",
    "- Try different values of k (2, 3, 4, ...)\n",
    "- Calculate \"inertia\" for each k\n",
    "  * Inertia = sum of squared distances from samples to their cluster centers\n",
    "  * Lower inertia = tighter clusters\n",
    "- Plot k vs inertia\n",
    "- Look for \"elbow\" - point where adding more clusters doesn't help much\n",
    "\n",
    "METHOD 2: SILHOUETTE SCORE (BETTER!)\n",
    "------------------------------------\n",
    "- Measures how similar samples are to their own cluster vs other clusters\n",
    "- Range: -1 to +1\n",
    "  * +1: Perfect clustering (sample is very similar to its cluster, very different from others)\n",
    "  * 0: Overlapping clusters\n",
    "  * -1: Wrong clustering (sample is more similar to other clusters)\n",
    "- Higher score = better clustering\n",
    "- Can identify optimal k more objectively than elbow method\n",
    "\n",
    "WHY USE BOTH?\n",
    "-------------\n",
    "- Elbow method: Visual, intuitive, but subjective\n",
    "- Silhouette score: Quantitative, objective, more reliable\n",
    "- Comparing both validates our choice of k\n",
    "\"\"\"\n",
    "# Step 1: K-Means Clustering\n",
    "# Find optimal k using BOTH elbow method AND silhouette score\n",
    "\n",
    "inertias = []  # Store inertia for each k\n",
    "silhouette_scores = []  # Store silhouette score for each k\n",
    "# Note: Breast Cancer dataset has 2 classes, so we test k from 2 to 6\n",
    "# (k=1 is not useful for clustering and silhouette score is undefined for k=1)\n",
    "k_range = range(2, 7)  # Try k from 2 to 6 clusters\n",
    "\n",
    "print(\"Testing different values of k...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different values of k\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)  # Fit on scaled data\n",
    "    inertias.append(kmeans.inertia_)  # Store inertia\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    labels = kmeans.predict(X_scaled)\n",
    "    sil_score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"k={k:2d}: Inertia={inertias[-1]:.2f}, Silhouette Score={sil_score:.4f}\")\n",
    "\n",
    "# Find optimal k from silhouette score (highest score)\n",
    "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "max_silhouette = max(silhouette_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best k by Silhouette Score: k={optimal_k_silhouette} (score={max_silhouette:.4f})\")\n",
    "print(f\"Ground truth k: {len(np.unique(y))}\")\n",
    "print(f\"Match: {'✓ YES!' if optimal_k_silhouette == len(np.unique(y)) else '✗ NO'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot both methods side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Elbow method\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=optimal_k_silhouette, color='r', linestyle='--', \n",
    "            label=f'Optimal k={optimal_k_silhouette} (from silhouette)')\n",
    "ax1.axvline(x=len(np.unique(y)), color='g', linestyle='--', \n",
    "            label=f'Ground truth k={len(np.unique(y))}')\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n",
    "ax1.set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Silhouette score\n",
    "ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=optimal_k_silhouette, color='r', linestyle='--', \n",
    "            label=f'Optimal k={optimal_k_silhouette}')\n",
    "ax2.axvline(x=len(np.unique(y)), color='g', linestyle='--', \n",
    "            label=f'Ground truth k={len(np.unique(y))}')\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Score Method\\n(Higher = Better)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('elbow_method.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nPlots saved. Silhouette score is more objective than elbow method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b69795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "K-MEANS CLUSTERING COMPLETE\n",
      "============================================================\n",
      "Chosen k: 2 (from silhouette)\n",
      "Final Silhouette Score: 0.3434\n",
      "Cluster distribution: [375 194]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "APPLY K-MEANS WITH OPTIMAL K\n",
    "-----------------------------\n",
    "We use the optimal k determined by silhouette score (most objective in this notebook).\n",
    "\n",
    "For the Breast Cancer Wisconsin dataset:\n",
    "- Ground truth has 2 diagnosis classes (benign vs malignant)\n",
    "- We expect the best k to be 2\n",
    "\n",
    "WHAT HAPPENS:\n",
    "-------------\n",
    "- K-Means assigns each patient to one of k clusters\n",
    "- Returns cluster labels (0, 1, 2, ...)\n",
    "- These labels represent discovered patient groups\n",
    "\"\"\"\n",
    "\n",
    "# Choose optimal k from silhouette analysis computed in the previous cell\n",
    "optimal_k = int(optimal_k_silhouette)\n",
    "\n",
    "# Apply K-Means clustering with chosen k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters_kmeans = kmeans.fit_predict(X_scaled)  # Cluster assignment for each patient\n",
    "\n",
    "# Report clustering quality\n",
    "final_silhouette = silhouette_score(X_scaled, clusters_kmeans)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"K-MEANS CLUSTERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Chosen k: {optimal_k} (from silhouette)\")\n",
    "print(f\"Final Silhouette Score: {final_silhouette:.4f}\")\n",
    "print(f\"Cluster distribution: {np.bincount(clusters_kmeans)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40e36713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal DBSCAN parameters...\n",
      "============================================================\n",
      "min_samples: 6\n",
      "Optimal eps (95th percentile of k-distances): 5.4948\n",
      "eps range: [1.3529, 14.2762]\n",
      "============================================================\n",
      "\n",
      "Applying DBSCAN with eps=5.4948, min_samples=6...\n",
      "\n",
      "============================================================\n",
      "DBSCAN RESULTS:\n",
      "============================================================\n",
      "Number of clusters found: 1\n",
      "Number of noise points: 15 (2.6%)\n",
      "Cluster labels: [-1, 0]\n",
      "Ground truth clusters: 2\n",
      "============================================================\n",
      "\n",
      "Note: -1 means 'noise' - samples that don't belong to any cluster\n",
      "K-distance plot saved as 'dbscan_parameter_selection.png'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: DBSCAN CLUSTERING\n",
    "-------------------------\n",
    "WHAT IS DBSCAN?\n",
    "--------------\n",
    "Density-Based Spatial Clustering of Applications with Noise:\n",
    "- Finds clusters based on DENSITY (not distance to center like K-Means)\n",
    "- Can find clusters of arbitrary shapes (not just spherical)\n",
    "- Automatically identifies NOISE/OUTLIERS (samples that don't fit any cluster)\n",
    "- Doesn't require you to specify number of clusters!\n",
    "\n",
    "KEY PARAMETERS - HOW TO CHOOSE THEM:\n",
    "------------------------------------\n",
    "1. eps (epsilon): Maximum distance between samples in same cluster\n",
    "   HOW TO FIND IT:\n",
    "   - Use k-distance graph (k-nearest neighbor distances)\n",
    "   - Find the \"knee\" or \"elbow\" in the sorted distances\n",
    "   - This is where distances jump (separating dense from sparse regions)\n",
    "   - Rule of thumb: eps = distance at the knee point\n",
    "   \n",
    "2. min_samples: Minimum samples needed to form a cluster\n",
    "   HOW TO CHOOSE:\n",
    "   - Typically: min_samples = 2 × number of dimensions (for high-dim data)\n",
    "   - For medical data with 30 features: min_samples = 5-10 is reasonable\n",
    "   - Higher = more strict, fewer clusters, more noise\n",
    "   - Lower = more lenient, more clusters, less noise\n",
    "   - Common values: 4, 5, 6, or 10\n",
    "\n",
    "METHOD: AUTOMATIC EPS SELECTION\n",
    "--------------------------------\n",
    "We'll use k-nearest neighbor distances to find optimal eps:\n",
    "1. Calculate distance to kth nearest neighbor for each point\n",
    "2. Sort these distances\n",
    "3. Find the \"knee\" where distances jump (this is optimal eps)\n",
    "4. This separates dense regions (clusters) from sparse regions (noise)\n",
    "\n",
    "ADVANTAGES:\n",
    "-----------\n",
    "- No need to guess number of clusters\n",
    "- Handles outliers naturally (labels them as -1)\n",
    "- Can find non-spherical clusters\n",
    "\n",
    "DISADVANTAGES:\n",
    "--------------\n",
    "- Sensitive to parameters (eps, min_samples)\n",
    "- Struggles with clusters of varying densities\n",
    "- Can be slow for large datasets\n",
    "\"\"\"\n",
    "# Step 2: DBSCAN Clustering with AUTOMATIC parameter selection\n",
    "\n",
    "print(\"Finding optimal DBSCAN parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# METHOD: Use k-nearest neighbor distances to find optimal eps\n",
    "# We'll use min_samples as k for the k-distance graph\n",
    "min_samples = 6  # For 30D data, 6 is reasonable (2 × dimensions = 60 would be too high)\n",
    "\n",
    "# Calculate distances to kth nearest neighbor for each point\n",
    "# This helps us find the \"knee\" in distance distribution\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
    "\n",
    "# Get distance to kth nearest neighbor (last column, since sorted)\n",
    "k_distances = distances[:, -1]  # Distance to kth (min_samples-th) nearest neighbor\n",
    "k_distances_sorted = np.sort(k_distances)[::-1]  # Sort descending\n",
    "\n",
    "# Find optimal eps using knee detection\n",
    "# The knee is where the curve bends (transition from dense to sparse)\n",
    "# We'll use a simple method: find point with maximum curvature\n",
    "# Or use the percentile method: eps = 95th percentile of k-distances\n",
    "optimal_eps = np.percentile(k_distances, 95)  # 95th percentile as eps\n",
    "\n",
    "print(f\"min_samples: {min_samples}\")\n",
    "print(f\"Optimal eps (95th percentile of k-distances): {optimal_eps:.4f}\")\n",
    "print(f\"eps range: [{k_distances.min():.4f}, {k_distances.max():.4f}]\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot k-distance graph to visualize optimal eps\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: k-distance graph\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(k_distances_sorted)), k_distances_sorted, 'b-', linewidth=2)\n",
    "plt.axhline(y=optimal_eps, color='r', linestyle='--', linewidth=2, \n",
    "            label=f'Optimal eps={optimal_eps:.4f}')\n",
    "plt.xlabel('Points (sorted by k-distance)', fontsize=11)\n",
    "plt.ylabel(f'Distance to {min_samples}th nearest neighbor', fontsize=11)\n",
    "plt.title('K-Distance Graph for DBSCAN\\n(Look for the \"knee\" - optimal eps)', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Histogram of k-distances\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(k_distances, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=optimal_eps, color='r', linestyle='--', linewidth=2, \n",
    "            label=f'Optimal eps={optimal_eps:.4f}')\n",
    "plt.xlabel(f'Distance to {min_samples}th nearest neighbor', fontsize=11)\n",
    "plt.ylabel('Frequency', fontsize=11)\n",
    "plt.title('Distribution of K-Distances', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dbscan_parameter_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Apply DBSCAN with optimal parameters\n",
    "print(f\"\\nApplying DBSCAN with eps={optimal_eps:.4f}, min_samples={min_samples}...\")\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
    "clusters_dbscan = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# DBSCAN labels: -1 = noise/outlier, 0+ = cluster ID\n",
    "n_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
    "n_noise = list(clusters_dbscan).count(-1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DBSCAN RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of clusters found: {n_clusters_dbscan}\")\n",
    "print(f\"Number of noise points: {n_noise} ({n_noise/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Cluster labels: {sorted(set(clusters_dbscan))}\")\n",
    "print(f\"Ground truth clusters: {len(np.unique(y))}\")\n",
    "if n_clusters_dbscan > 0:\n",
    "    # Calculate silhouette score (only if we have clusters)\n",
    "    # Note: Silhouette doesn't work well with noise points, so we'll skip them\n",
    "    non_noise_mask = clusters_dbscan != -1\n",
    "    if np.sum(non_noise_mask) > 1 and len(set(clusters_dbscan[non_noise_mask])) > 1:\n",
    "        dbscan_silhouette = silhouette_score(X_scaled[non_noise_mask], \n",
    "                                             clusters_dbscan[non_noise_mask])\n",
    "        print(f\"Silhouette Score (excluding noise): {dbscan_silhouette:.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: -1 means 'noise' - samples that don't belong to any cluster\")\n",
    "print(\"K-distance plot saved as 'dbscan_parameter_selection.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ad7ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical clustering complete!\n",
      "Cluster distribution: [184 385]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: HIERARCHICAL CLUSTERING (AGGLOMERATIVE)\n",
    "-----------------------------------------------\n",
    "WHAT IS HIERARCHICAL CLUSTERING?\n",
    "---------------------------------\n",
    "- Builds a tree (dendrogram) of clusters\n",
    "- Starts with each sample as its own cluster\n",
    "- Repeatedly merges closest clusters\n",
    "- Creates a hierarchy: can cut at any level to get different numbers of clusters\n",
    "\n",
    "TYPES:\n",
    "------\n",
    "- Agglomerative (bottom-up): Start with individual samples, merge up\n",
    "- Divisive (top-down): Start with all samples, split down\n",
    "We use Agglomerative here.\n",
    "\n",
    "ADVANTAGES:\n",
    "-----------\n",
    "- Creates interpretable hierarchy (dendrogram)\n",
    "- Can see relationships between clusters\n",
    "- No need to specify k in advance (but we do for comparison)\n",
    "\n",
    "DISADVANTAGES:\n",
    "--------------\n",
    "- Computationally expensive for large datasets\n",
    "- Sensitive to noise and outliers\n",
    "- Once a merge is made, it can't be undone\n",
    "\n",
    "WHY USE IT?\n",
    "-----------\n",
    "- Provides different perspective than K-Means\n",
    "- Can reveal hierarchical structure in patient subtypes\n",
    "- Useful for understanding relationships between groups\n",
    "\"\"\"\n",
    "# Step 3: Hierarchical Clustering\n",
    "# Agglomerative: Start with individual samples, merge closest pairs\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)  # Cut tree to get k clusters\n",
    "clusters_hierarchical = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"Hierarchical clustering complete!\")\n",
    "print(f\"Cluster distribution: {np.bincount(clusters_hierarchical)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0cda3701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP transformation complete!\n",
      "UMAP embedding shape: (569, 2)\n",
      "(This preserves local neighborhood structure better than PCA)\n",
      "Note: With this dataset, X_umap should be (569, 2).\n",
      "If you still see (500, 2), re-run the notebook from the data-loading cell so\n",
      "UMAP uses the breast cancer dataset (not the old synthetic one).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 4: UMAP FOR ADVANCED VISUALIZATION\n",
    "----------------------------------------\n",
    "WHAT IS UMAP?\n",
    "-------------\n",
    "Uniform Manifold Approximation and Projection:\n",
    "- Advanced dimensionality reduction technique\n",
    "- Preserves LOCAL structure (neighborhoods) better than PCA\n",
    "- Can reveal clusters that PCA might miss\n",
    "- Particularly good for high-dimensional biological data\n",
    "\n",
    "UMAP vs PCA:\n",
    "------------\n",
    "- PCA: Preserves GLOBAL structure (overall variance)\n",
    "- UMAP: Preserves LOCAL structure (neighborhood relationships)\n",
    "- UMAP often shows clearer cluster separation\n",
    "\n",
    "WHY USE BOTH?\n",
    "-------------\n",
    "- PCA: Fast, interpretable, shows global patterns\n",
    "- UMAP: More sophisticated, better for complex data, shows local patterns\n",
    "- Comparing both helps validate that clusters are real, not artifacts\n",
    "\n",
    "PARAMETERS:\n",
    "-----------\n",
    "- n_components=2: Reduce to 2D for visualization\n",
    "- random_state=42: For reproducibility\n",
    "\"\"\"\n",
    "# Step 4: UMAP for Visualization\n",
    "# UMAP is slower than PCA but often reveals better cluster structure\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)  # Transform to 2D UMAP space\n",
    "\n",
    "print(\"UMAP transformation complete!\")\n",
    "print(f\"UMAP embedding shape: {X_umap.shape}\")\n",
    "print(\"(This preserves local neighborhood structure better than PCA)\")\n",
    "print(\"Note: With this dataset, X_umap should be (569, 2).\")\n",
    "print(\"If you still see (500, 2), re-run the notebook from the data-loading cell so\")\n",
    "print(\"UMAP uses the breast cancer dataset (not the old synthetic one).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4dc47955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering comparison plot saved as 'clustering_comparison.png'\n",
      "Compare the plots to see which method gives clearest cluster separation!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nVISUALIZATION INTERPRETATION - BREAST CANCER DATA\\n--------------------------------------------------\\nWHAT DO THE PLOTS TELL US?\\n\\n1. K-MEANS ON PCA (Top-Left):\\n   - Should show 2 clusters (benign vs malignant)\\n   - With real medical data, clusters should be well-separated\\n   - PCA should capture much more variance (60-80%) than synthetic data\\n   - Clear separation indicates good clustering\\n\\n2. DBSCAN ON PCA (Top-Right):\\n   - Should find 2 clusters (benign and malignant)\\n   - May identify some noise points (outliers)\\n   - With proper parameters, should work better than synthetic data\\n   - DBSCAN can handle the 30-dimensional space better than 2000D\\n\\n3. HIERARCHICAL ON PCA (Bottom-Left):\\n   - Should show 2 clusters similar to K-Means\\n   - May have slightly different assignments than K-Means\\n   - Both methods should find the benign/malignant separation\\n   - This validates that the cluster structure is real\\n\\n4. K-MEANS ON UMAP (Bottom-Right):\\n   - Should show EXCELLENT separation between benign and malignant!\\n   - UMAP preserves LOCAL structure (neighborhoods)\\n   - This reveals the true cluster structure clearly\\n   - Two distinct groups should be visible\\n\\nKEY INSIGHT:\\n------------\\nWith real medical data (30 features, 2 classes), clustering should work\\nMUCH better than synthetic high-dimensional data. The benign and malignant\\ntumors should be clearly separated, validating the clustering approach.\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 5: VISUALIZE CLUSTERING RESULTS\n",
    "-------------------------------------\n",
    "WHY VISUALIZE?\n",
    "--------------\n",
    "- See if clusters make sense visually\n",
    "- Compare different clustering methods\n",
    "- Identify which method works best for our data\n",
    "- Validate that discovered clusters are meaningful\n",
    "\n",
    "WHAT TO LOOK FOR:\n",
    "-----------------\n",
    "- Clear separation between clusters (good!)\n",
    "- Tight, compact clusters (good!)\n",
    "- Overlapping clusters (might need different method or parameters)\n",
    "- Consistent results across methods (validates findings)\n",
    "\n",
    "THE PLOTS:\n",
    "----------\n",
    "1. K-Means on PCA: See how K-Means clusters look in PCA space\n",
    "2. DBSCAN on PCA: See density-based clusters (note: -1 = noise/outliers)\n",
    "3. Hierarchical on PCA: See hierarchical clustering results\n",
    "4. K-Means on UMAP: See if UMAP reveals better cluster structure\n",
    "\n",
    "COLOR CODING:\n",
    "-------------\n",
    "- Each color represents a different cluster\n",
    "- Same color = same cluster assignment\n",
    "- Different colors = different patient subtypes\n",
    "\"\"\"\n",
    "# Step 5: Visualize Clusters\n",
    "# Create 2x2 grid to compare all methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: K-Means clustering visualized in PCA space\n",
    "axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_kmeans, cmap='viridis', s=30, alpha=0.6)\n",
    "axes[0, 0].set_title('K-Means Clustering (PCA view)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('PC1')\n",
    "axes[0, 0].set_ylabel('PC2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DBSCAN clustering (note: -1 values are noise/outliers)\n",
    "axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_dbscan, cmap='viridis', s=30, alpha=0.6)\n",
    "axes[0, 1].set_title(f'DBSCAN Clustering ({n_clusters_dbscan} clusters, {n_noise} noise)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('PC1')\n",
    "axes[0, 1].set_ylabel('PC2')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Hierarchical clustering\n",
    "axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_hierarchical, cmap='viridis', s=30, alpha=0.6)\n",
    "axes[1, 0].set_title('Hierarchical Clustering (PCA view)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('PC1')\n",
    "axes[1, 0].set_ylabel('PC2')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: K-Means clusters visualized in UMAP space\n",
    "# UMAP often shows clearer separation than PCA\n",
    "axes[1, 1].scatter(X_umap[:, 0], X_umap[:, 1], c=clusters_kmeans, cmap='viridis', s=30, alpha=0.6)\n",
    "axes[1, 1].set_title('UMAP Visualization with K-Means Clusters', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('UMAP 1')\n",
    "axes[1, 1].set_ylabel('UMAP 2')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparison of Clustering Methods', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Clustering comparison plot saved as 'clustering_comparison.png'\")\n",
    "print(\"Compare the plots to see which method gives clearest cluster separation!\")\n",
    "\n",
    "\"\"\"\n",
    "VISUALIZATION INTERPRETATION - BREAST CANCER DATA\n",
    "--------------------------------------------------\n",
    "WHAT DO THE PLOTS TELL US?\n",
    "\n",
    "1. K-MEANS ON PCA (Top-Left):\n",
    "   - Should show 2 clusters (benign vs malignant)\n",
    "   - With real medical data, clusters should be well-separated\n",
    "   - PCA should capture much more variance (60-80%) than synthetic data\n",
    "   - Clear separation indicates good clustering\n",
    "\n",
    "2. DBSCAN ON PCA (Top-Right):\n",
    "   - Should find 2 clusters (benign and malignant)\n",
    "   - May identify some noise points (outliers)\n",
    "   - With proper parameters, should work better than synthetic data\n",
    "   - DBSCAN can handle the 30-dimensional space better than 2000D\n",
    "\n",
    "3. HIERARCHICAL ON PCA (Bottom-Left):\n",
    "   - Should show 2 clusters similar to K-Means\n",
    "   - May have slightly different assignments than K-Means\n",
    "   - Both methods should find the benign/malignant separation\n",
    "   - This validates that the cluster structure is real\n",
    "\n",
    "4. K-MEANS ON UMAP (Bottom-Right):\n",
    "   - Should show EXCELLENT separation between benign and malignant!\n",
    "   - UMAP preserves LOCAL structure (neighborhoods)\n",
    "   - This reveals the true cluster structure clearly\n",
    "   - Two distinct groups should be visible\n",
    "\n",
    "KEY INSIGHT:\n",
    "------------\n",
    "With real medical data (30 features, 2 classes), clustering should work\n",
    "MUCH better than synthetic high-dimensional data. The benign and malignant\n",
    "tumors should be clearly separated, validating the clustering approach.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "876249db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLUSTER ANALYSIS - BREAST CANCER PATIENTS\n",
      "======================================================================\n",
      "\n",
      "Cluster size and diagnosis comparison:\n",
      "        Disease_Status                    \n",
      "                 count      mean       std\n",
      "Cluster                                   \n",
      "0                  375  0.904000  0.294985\n",
      "1                  194  0.092784  0.290879\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DETAILED CLUSTER BREAKDOWN:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Cluster 0:\n",
      "  Total patients: 375\n",
      "  Malignant (0): 36 (9.6%)\n",
      "  Benign (1): 339 (90.4%)\n",
      "  Mean Disease_Status: 0.904\n",
      "  Cluster type: BENIGN (non-cancerous)\n",
      "\n",
      "Cluster 1:\n",
      "  Total patients: 194\n",
      "  Malignant (0): 176 (90.7%)\n",
      "  Benign (1): 18 (9.3%)\n",
      "  Mean Disease_Status: 0.093\n",
      "  Cluster type: MALIGNANT (cancerous)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION:\n",
      "======================================================================\n",
      "✓ Good clustering: Each cluster is predominantly one diagnosis type\n",
      "✗ Poor clustering: Clusters are mixed (similar proportions of both)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 6: ANALYZE DISCOVERED CLUSTERS\n",
    "------------------------------------\n",
    "WHY ANALYZE CLUSTERS?\n",
    "--------------------\n",
    "- Validate that clusters are meaningful\n",
    "- Understand characteristics of each patient group\n",
    "- Compare discovered clusters to ground truth diagnosis\n",
    "- Generate insights for medical interpretation\n",
    "\n",
    "WHAT WE'RE DOING:\n",
    "-----------------\n",
    "1. Add cluster labels to our DataFrame\n",
    "2. Group by cluster and calculate statistics\n",
    "3. Compare cluster characteristics\n",
    "4. Check if clusters align with ground truth labels (Disease_Status)\n",
    "   - Disease_Status = 0: Malignant (cancerous)\n",
    "   - Disease_Status = 1: Benign (non-cancerous)\n",
    "\n",
    "INTERPRETATION:\n",
    "--------------\n",
    "- Count: How many patients in each cluster\n",
    "- Mean Disease_Status: Average diagnosis (0 = benign, 1 = malignant)\n",
    "  * If clustering is good, each cluster should have similar Disease_Status values\n",
    "  * Cluster with mean ~0 = mostly benign patients\n",
    "  * Cluster with mean ~1 = mostly malignant patients\n",
    "  * This tells us if our clustering matches the true diagnosis\n",
    "\n",
    "MEDICAL INTERPRETATION:\n",
    "----------------------\n",
    "- Perfect clustering: One cluster = all benign, other cluster = all malignant\n",
    "- Good clustering: Each cluster is predominantly one type (>80% same type)\n",
    "- Poor clustering: Clusters are mixed (similar proportions of both types)\n",
    "\n",
    "NOTE: In real projects, we wouldn't have Disease_Status to compare against!\n",
    "\"\"\"\n",
    "# Step 6: Analyze Clusters\n",
    "# Add cluster labels to dataframe for analysis\n",
    "df['Cluster'] = clusters_kmeans\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "# Group by cluster and calculate summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER ANALYSIS - BREAST CANCER PATIENTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCluster size and diagnosis comparison:\")\n",
    "cluster_stats = df.groupby('Cluster').agg({\n",
    "    'Disease_Status': ['count', 'mean', 'std']\n",
    "})\n",
    "print(cluster_stats)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"DETAILED CLUSTER BREAKDOWN:\")\n",
    "print(\"-\"*70)\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df[df['Cluster'] == cluster_id]\n",
    "    n_malignant = np.sum(cluster_data['Disease_Status'] == 0)\n",
    "    n_benign = np.sum(cluster_data['Disease_Status'] == 1)\n",
    "    total = len(cluster_data)\n",
    "    mean_status = cluster_data['Disease_Status'].mean()\n",
    "    \n",
    "    # Determine cluster type\n",
    "    # Since 0=malignant and 1=benign:\n",
    "    # mean close to 0 => mostly malignant, mean close to 1 => mostly benign\n",
    "    if mean_status < 0.2:\n",
    "        cluster_type = \"MALIGNANT (cancerous)\"\n",
    "    elif mean_status > 0.8:\n",
    "        cluster_type = \"BENIGN (non-cancerous)\"\n",
    "    else:\n",
    "        cluster_type = \"MIXED\"\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Total patients: {total}\")\n",
    "    print(f\"  Malignant (0): {n_malignant} ({n_malignant/total*100:.1f}%)\")\n",
    "    print(f\"  Benign (1): {n_benign} ({n_benign/total*100:.1f}%)\")\n",
    "    print(f\"  Mean Disease_Status: {mean_status:.3f}\")\n",
    "    print(f\"  Cluster type: {cluster_type}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Good clustering: Each cluster is predominantly one diagnosis type\")\n",
    "print(\"✗ Poor clustering: Clusters are mixed (similar proportions of both)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b45e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MARKER FEATURE IDENTIFICATION - BREAST CANCER\n",
      "======================================================================\n",
      "\n",
      "Finding features that are characteristic of each cluster...\n",
      "(Features with largest difference between cluster and others)\n",
      "\n",
      "These features help distinguish between benign and malignant tumors.\n",
      "\n",
      "======================================================================\n",
      "TOP 10 MARKER FEATURES FOR CLUSTER 0\n",
      "======================================================================\n",
      "Cluster size: 375 patients\n",
      "Cluster type: BENIGN (based on majority diagnosis)\n",
      "\n",
      "Feature Name                        | Difference   | Direction\n",
      "----------------------------------------------------------------------\n",
      "worst area                          |    833.1534 | ↓ LOWER\n",
      "mean area                           |    493.0849 | ↓ LOWER\n",
      "area error                          |     53.5564 | ↓ LOWER\n",
      "worst perimeter                     |     52.4163 | ↓ LOWER\n",
      "mean perimeter                      |     35.6328 | ↓ LOWER\n",
      "worst radius                        |      7.3346 | ↓ LOWER\n",
      "mean radius                         |      4.9878 | ↓ LOWER\n",
      "worst texture                       |      4.5294 | ↓ LOWER\n",
      "mean texture                        |      3.0128 | ↓ LOWER\n",
      "perimeter error                     |      2.5534 | ↓ LOWER\n",
      "\n",
      "These features are most characteristic of BENIGN tumors\n",
      "\n",
      "======================================================================\n",
      "TOP 10 MARKER FEATURES FOR CLUSTER 1\n",
      "======================================================================\n",
      "Cluster size: 194 patients\n",
      "Cluster type: MALIGNANT (based on majority diagnosis)\n",
      "\n",
      "Feature Name                        | Difference   | Direction\n",
      "----------------------------------------------------------------------\n",
      "worst area                          |    833.1534 | ↑ HIGHER\n",
      "mean area                           |    493.0849 | ↑ HIGHER\n",
      "area error                          |     53.5564 | ↑ HIGHER\n",
      "worst perimeter                     |     52.4163 | ↑ HIGHER\n",
      "mean perimeter                      |     35.6328 | ↑ HIGHER\n",
      "worst radius                        |      7.3346 | ↑ HIGHER\n",
      "mean radius                         |      4.9878 | ↑ HIGHER\n",
      "worst texture                       |      4.5294 | ↑ HIGHER\n",
      "mean texture                        |      3.0128 | ↑ HIGHER\n",
      "perimeter error                     |      2.5534 | ↑ HIGHER\n",
      "\n",
      "These features are most characteristic of MALIGNANT tumors\n",
      "\n",
      "======================================================================\n",
      "MEDICAL INTERPRETATION:\n",
      "======================================================================\n",
      "Marker features help us understand:\n",
      "  - What distinguishes benign from malignant tumors\n",
      "  - Which cell nucleus measurements are diagnostic\n",
      "  - Clinical features that can be used for cancer screening\n",
      "  - Biological differences between tumor types\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 7: FIND MARKER FEATURES FOR EACH CLUSTER\n",
    "-----------------------------------------------\n",
    "WHAT ARE MARKER FEATURES?\n",
    "--------------------------\n",
    "Features (cell nucleus measurements) that are:\n",
    "- Significantly different in one cluster\n",
    "- Characteristic of that patient group\n",
    "- Can distinguish between benign and malignant tumors\n",
    "\n",
    "WHY FIND MARKER FEATURES?\n",
    "-------------------------\n",
    "- Medical interpretation: Understand what distinguishes tumor types\n",
    "- Diagnostic markers: Could be used to classify new patients\n",
    "- Clinical insights: Understand tumor characteristics\n",
    "- Research: Understand cancer mechanisms\n",
    "\n",
    "HOW WE FIND THEM:\n",
    "----------------\n",
    "1. For each cluster, calculate mean value of each feature\n",
    "2. Calculate mean value in all OTHER clusters\n",
    "3. Find features with largest difference (marker features)\n",
    "4. These features \"define\" that patient group\n",
    "\n",
    "REAL-WORLD APPLICATION:\n",
    "----------------------\n",
    "- Identify diagnostic markers for tumor classification\n",
    "- Develop cancer screening tests\n",
    "- Guide treatment decisions\n",
    "- Understand tumor biology\n",
    "\"\"\"\n",
    "# Find marker features for each cluster\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MARKER FEATURE IDENTIFICATION - BREAST CANCER\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFinding features that are characteristic of each cluster...\")\n",
    "print(\"(Features with largest difference between cluster and others)\\n\")\n",
    "print(\"These features help distinguish between benign and malignant tumors.\")\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    # Get samples in this cluster\n",
    "    cluster_samples = df[df['Cluster'] == cluster_id]\n",
    "    # Get samples in all other clusters\n",
    "    other_samples = df[df['Cluster'] != cluster_id]\n",
    "    \n",
    "    # Calculate mean value for each feature\n",
    "    # Drop non-feature columns (Disease_Status, Cluster)\n",
    "    cluster_mean = cluster_samples.drop(['Disease_Status', 'Cluster'], axis=1).mean()\n",
    "    other_mean = other_samples.drop(['Disease_Status', 'Cluster'], axis=1).mean()\n",
    "    \n",
    "    # Find features with highest absolute difference\n",
    "    # These are the \"marker features\" that define this cluster\n",
    "    diff = (cluster_mean - other_mean).abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TOP 10 MARKER FEATURES FOR CLUSTER {cluster_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Cluster size: {len(cluster_samples)} patients\")\n",
    "    \n",
    "    # Determine cluster diagnosis type\n",
    "    # Reminder: 0=malignant, 1=benign\n",
    "    cluster_diagnosis = \"MALIGNANT\" if cluster_samples['Disease_Status'].mean() < 0.5 else \"BENIGN\"\n",
    "    print(f\"Cluster type: {cluster_diagnosis} (based on majority diagnosis)\")\n",
    "    \n",
    "    print(f\"\\n{'Feature Name':<35s} | {'Difference':<12s} | {'Direction'}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    for i, (feature, diff_value) in enumerate(diff.head(10).items(), 1):\n",
    "        cluster_val = cluster_mean[feature]\n",
    "        other_val = other_mean[feature]\n",
    "        direction = \"↑ HIGHER\" if cluster_val > other_val else \"↓ LOWER\"\n",
    "        # Truncate long feature names for display\n",
    "        feature_display = feature[:34] if len(feature) <= 34 else feature[:31] + \"...\"\n",
    "        print(f\"{feature_display:<35s} | {diff_value:>11.4f} | {direction}\")\n",
    "    \n",
    "    print(f\"\\nThese features are most characteristic of {cluster_diagnosis} tumors\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEDICAL INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Marker features help us understand:\")\n",
    "print(\"  - What distinguishes benign from malignant tumors\")\n",
    "print(\"  - Which cell nucleus measurements are diagnostic\")\n",
    "print(\"  - Clinical features that can be used for cancer screening\")\n",
    "print(\"  - Biological differences between tumor types\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "afcf07fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== External cluster quality vs true labels ===\n",
      "K-Means:       ARI = 0.654, NMI = 0.532\n",
      "Hierarchical:  ARI = 0.575, NMI = 0.457\n",
      "DBSCAN:        Not meaningful (only one cluster + noise)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# 0) True labels (remember: 0 = malignant, 1 = benign)\n",
    "y_true = y\n",
    "\n",
    "# 1) K-Means\n",
    "ari_kmeans = adjusted_rand_score(y_true, clusters_kmeans)\n",
    "nmi_kmeans = normalized_mutual_info_score(y_true, clusters_kmeans)\n",
    "\n",
    "# 2) Hierarchical\n",
    "ari_hier = adjusted_rand_score(y_true, clusters_hierarchical)\n",
    "nmi_hier = normalized_mutual_info_score(y_true, clusters_hierarchical)\n",
    "\n",
    "# 3) DBSCAN (only if it found >1 cluster)\n",
    "if n_clusters_dbscan > 1:\n",
    "    ari_dbscan = adjusted_rand_score(y_true, clusters_dbscan)\n",
    "    nmi_dbscan = normalized_mutual_info_score(y_true, clusters_dbscan)\n",
    "else:\n",
    "    ari_dbscan = nmi_dbscan = None\n",
    "\n",
    "print(\"=== External cluster quality vs true labels ===\")\n",
    "print(f\"K-Means:       ARI = {ari_kmeans:.3f}, NMI = {nmi_kmeans:.3f}\")\n",
    "print(f\"Hierarchical:  ARI = {ari_hier:.3f}, NMI = {nmi_hier:.3f}\")\n",
    "if ari_dbscan is not None:\n",
    "    print(f\"DBSCAN:        ARI = {ari_dbscan:.3f}, NMI = {nmi_dbscan:.3f}\")\n",
    "else:\n",
    "    print(\"DBSCAN:        Not meaningful (only one cluster + noise)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
